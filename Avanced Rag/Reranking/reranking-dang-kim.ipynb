{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4cqa0s0XvjP"
      },
      "source": [
        "# Reranking Methods in RAG Systems\n",
        "\n",
        "## Overview\n",
        "Reranking is a crucial step in Retrieval-Augmented Generation (RAG) systems that aims to improve the relevance and quality of retrieved documents. It involves reassessing and reordering initially retrieved documents to ensure that the most pertinent information is prioritized for subsequent processing or presentation.\n",
        "\n",
        "## Motivation\n",
        "The primary motivation for reranking in RAG systems is to overcome limitations of initial retrieval methods, which often rely on simpler similarity metrics. Reranking allows for more sophisticated relevance assessment, taking into account nuanced relationships between queries and documents that might be missed by traditional retrieval techniques. This process aims to enhance the overall performance of RAG systems by ensuring that the most relevant information is used in the generation phase.\n",
        "\n",
        "## Key Components\n",
        "Reranking systems typically include the following components:\n",
        "\n",
        "1. Initial Retriever: Often a vector store using embedding-based similarity search.\n",
        "2. Reranking Model: This can be either:\n",
        "   - A Large Language Model (LLM) for scoring relevance\n",
        "   - A Cross-Encoder model specifically trained for relevance assessment\n",
        "3. Scoring Mechanism: A method to assign relevance scores to documents\n",
        "4. Sorting and Selection Logic: To reorder documents based on new scores\n",
        "\n",
        "## Method Details\n",
        "The reranking process generally follows these steps:\n",
        "\n",
        "1. Initial Retrieval: Fetch an initial set of potentially relevant documents.\n",
        "2. Pair Creation: Form query-document pairs for each retrieved document.\n",
        "3. Scoring:\n",
        "   - LLM Method: Use prompts to ask the LLM to rate document relevance.\n",
        "   - Cross-Encoder Method: Feed query-document pairs directly into the model.\n",
        "4. Score Interpretation: Parse and normalize the relevance scores.\n",
        "5. Reordering: Sort documents based on their new relevance scores.\n",
        "6. Selection: Choose the top K documents from the reordered list.\n",
        "\n",
        "## Benefits of this Approach\n",
        "Reranking offers several advantages:\n",
        "\n",
        "1. Improved Relevance: By using more sophisticated models, reranking can capture subtle relevance factors.\n",
        "2. Flexibility: Different reranking methods can be applied based on specific needs and resources.\n",
        "3. Enhanced Context Quality: Providing more relevant documents to the RAG system improves the quality of generated responses.\n",
        "4. Reduced Noise: Reranking helps filter out less relevant information, focusing on the most pertinent content.\n",
        "\n",
        "## Conclusion\n",
        "Reranking is a powerful technique in RAG systems that significantly enhances the quality of retrieved information. Whether using LLM-based scoring or specialized Cross-Encoder models, reranking allows for more nuanced and accurate assessment of document relevance. This improved relevance translates directly to better performance in downstream tasks, making reranking an essential component in advanced RAG implementations.\n",
        "\n",
        "The choice between LLM-based and Cross-Encoder reranking methods depends on factors such as required accuracy, available computational resources, and specific application needs. Both approaches offer substantial improvements over basic retrieval methods and contribute to the overall effectiveness of RAG systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fA7yiMAXvjR"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/reranking-visualization.svg\" alt=\"rerank llm\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HG3RsmhlXvjR"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "\n",
        "<img src=\"../images/reranking_comparison.svg\" alt=\"rerank llm\" style=\"width:100%; height:auto;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QO_FPraXvjS"
      },
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY-TJYqGXvjS",
        "outputId": "25097bb0-7c2d-4fd6-f4af-72b2e5db2474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.4)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.7)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.7)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.15.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.6.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.6)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.2->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.1->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install langchain langchain-openai python-dotenv sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X95nFoDXvjT",
        "outputId": "0131a7c4-fc14-4c7c-b4d7-4e03525829d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'RAG_TECHNIQUES' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository to access helper functions and evaluation modules\n",
        "!git clone https://github.com/NirDiamant/RAG_TECHNIQUES.git\n",
        "import sys\n",
        "sys.path.append('RAG_TECHNIQUES')\n",
        "# If you need to run with the latest data\n",
        "# !cp -r RAG_TECHNIQUES/data ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "oT6JXvDNb990"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7s9PZ-IpXvjT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.documents import Document\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from langchain_openai import ChatOpenAI\n",
        "# from langchain_opena import RetrievalQA\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "\n",
        "# Original path append replaced for Colab compatibility\n",
        "# from helper_functions import *\n",
        "# from evaluation.evalute_rag import *\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set the OpenAI API key environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
        "os.environ[\"BASE_URL\"] = os.getenv('BASE_URL')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itCS-mC9XvjT"
      },
      "source": [
        "### Define the document's path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUcFh8QpXvjU",
        "outputId": "ef2b1be4-324d-4b87-da64-d8dbc4f11925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-26 13:14:15--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206372 (202K) [application/octet-stream]\n",
            "Saving to: ‘data/Understanding_Climate_Change.pdf’\n",
            "\n",
            "\r          data/Unde   0%[                    ]       0  --.-KB/s               \rdata/Understanding_ 100%[===================>] 201.54K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2026-01-26 13:14:15 (12.4 MB/s) - ‘data/Understanding_Climate_Change.pdf’ saved [206372/206372]\n",
            "\n",
            "--2026-01-26 13:14:15--  https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 206372 (202K) [application/octet-stream]\n",
            "Saving to: ‘data/Understanding_Climate_Change.pdf’\n",
            "\n",
            "data/Understanding_ 100%[===================>] 201.54K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2026-01-26 13:14:15 (12.1 MB/s) - ‘data/Understanding_Climate_Change.pdf’ saved [206372/206372]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download required data files\n",
        "import os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Download the PDF document used in this notebook\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n",
        "!wget -O data/Understanding_Climate_Change.pdf https://raw.githubusercontent.com/NirDiamant/RAG_TECHNIQUES/main/data/Understanding_Climate_Change.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ifsrt1YNXvjU"
      },
      "outputs": [],
      "source": [
        "path = \"data/Understanding_Climate_Change.pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmDyCmMOXvjU"
      },
      "source": [
        "### Create a vector store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community langchain_openai langchain_classic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI0a-Iw7bKSM",
        "outputId": "cca27acd-35d4-450a-8513-0f995c8a303a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.12/dist-packages (1.1.7)\n",
            "Requirement already satisfied: langchain_classic in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (1.2.7)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.15.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_classic) (1.1.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_classic) (2.12.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (0.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_classic) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2026.1.4)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7n9OXx_btok",
        "outputId": "66917b36-1dc0-42fb-d238-2eaac2012ffe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-6.6.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pypdf-6.6.2-py3-none-any.whl (329 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/329.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-6.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX5B6B_hdbTj",
        "outputId": "deb13351-dd78-421d-b65f-9e34d69d8802"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/23.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/23.8 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/23.8 MB\u001b[0m \u001b[31m131.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m17.6/23.8 MB\u001b[0m \u001b[31m244.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m258.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m258.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Oq8vKwNFXvjV"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Load PDF\n",
        "loader = PyPDFLoader(path)\n",
        "documents = loader.load()\n",
        "\n",
        "# Split documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# Create embeddings\n",
        "# embeddings = OpenAIEmbeddings(\n",
        "#     api_key=os.getenv('OPENAI_API_KEY'),\n",
        "#     model=\"text-embedding-ada-002\",\n",
        "#     chunk_size=1,\n",
        "#     base_url=os.getenv('BASE_URL')\n",
        "# )\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Create vectorstore\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCBLVCjPXvjV"
      },
      "source": [
        "## Method 1: LLM based function to rerank the retrieved documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQT5l8r2XvjV"
      },
      "source": [
        "### Create a custom reranking function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "R6UXIOSoXvjV"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "# from langchain_core.chains import LLMChain\n",
        "\n",
        "\n",
        "class RatingScore(BaseModel):\n",
        "    relevance_score: float = Field(..., description=\"The relevance score of a document to a query.\")\n",
        "\n",
        "def rerank_documents(query: str, docs: List[Document], top_n: int = 3) -> List[Document]:\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"query\", \"doc\"],\n",
        "        template=\"\"\"On a scale of 1-10, rate the relevance of the following document to the query. Consider the specific context and intent of the query, not just keyword matches.\n",
        "        Query: {query}\n",
        "        Document: {doc}\n",
        "        Relevance Score:\"\"\"\n",
        "    )\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0,\n",
        "                     model_name=\"gpt-4.1\",\n",
        "                     max_tokens=4000,\n",
        "                     api_key=os.getenv('OPENAI_API_KEY'),\n",
        "                     base_url=os.getenv('BASE_URL')\n",
        "    )\n",
        "    llm_chain = prompt_template | llm.with_structured_output(RatingScore)\n",
        "\n",
        "    scored_docs = []\n",
        "    for doc in docs:\n",
        "        input_data = {\"query\": query, \"doc\": doc.page_content}\n",
        "        score = llm_chain.invoke(input_data).relevance_score\n",
        "        try:\n",
        "            score = float(score)\n",
        "        except ValueError:\n",
        "            score = 0  # Default score if parsing fails\n",
        "        scored_docs.append((doc, score))\n",
        "\n",
        "    reranked_docs = sorted(scored_docs, key=lambda x: x[1], reverse=True)\n",
        "    return [doc for doc, _ in reranked_docs[:top_n]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYZ4ul1sXvjV"
      },
      "source": [
        "### Example usage of the reranking function with a sample query relevant to the document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6IU8ekLXvjV",
        "outputId": "80a20cfa-a5e4-4db2-fdda-3063428c28d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top initial documents:\n",
            "\n",
            "Document 1:\n",
            "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
            "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
            "experiencing shi...\n",
            "\n",
            "Document 2:\n",
            "protection, and habitat creation. \n",
            "Climate-Resilient Conservation \n",
            "Conservation strategies must account for climate change impacts to be effective. This \n",
            "includes identifying climate refugia, areas le...\n",
            "\n",
            "Document 3:\n",
            "The economic costs of climate change include damage to infrastructure, reduced agricultural \n",
            "productivity, health care costs, and lost labor productivity. Extreme weather events, such as \n",
            "hurricanes a...\n",
            "Query: What are the impacts of climate change on biodiversity?\n",
            "\n",
            "Top reranked documents:\n",
            "\n",
            "Document 1:\n",
            "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
            "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
            "experiencing shi...\n",
            "\n",
            "Document 2:\n",
            "Coral reefs are highly sensitive to changes in temperature and acidity. Ocean acidification \n",
            "and warming waters contribute to coral bleaching and mortality, threatening biodiversity and \n",
            "fisheries. Pr...\n",
            "\n",
            "Document 3:\n",
            "Freshwater Ecosystems \n",
            "Freshwater ecosystems, including rivers, lakes, and wetlands, are affected by changes in \n",
            "precipitation patterns, temperature, and water flow. These changes can lead to altered ...\n"
          ]
        }
      ],
      "source": [
        "query = \"What are the impacts of climate change on biodiversity?\"\n",
        "initial_docs = vectorstore.similarity_search(query, k=15)\n",
        "reranked_docs = rerank_documents(query, initial_docs)\n",
        "\n",
        "# print first 3 initial documents\n",
        "print(\"Top initial documents:\")\n",
        "for i, doc in enumerate(initial_docs[:3]):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(f\"Query: {query}\\n\")\n",
        "print(\"Top reranked documents:\")\n",
        "for i, doc in enumerate(reranked_docs):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmicSbhiXvjW"
      },
      "source": [
        "### Create a custom retriever based on our reranker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q70grvsmXvjW",
        "outputId": "f09b5417-6f12-4ef6-a313-b0d84db74366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1478752784.py:17: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  class CustomRetriever(BaseRetriever, BaseModel):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1 contains information about climate change, including:\n",
            "\n",
            "- The definition of climate change as significant, long-term changes in global climate patterns such as temperature, precipitation, and wind.\n",
            "- The role of human activities, especially burning fossil fuels and deforestation, in contributing to climate change over the past century.\n",
            "- The historical context of climate change, noting that Earth's climate has changed throughout history due to small variations in Earth's orbit, with the modern climate era beginning about 11,700 years ago after the last ice age.\n",
            "- International collaboration efforts to address climate change, including the United Nations Framework Convention on Climate Change (UNFCCC), which provides a framework for global cooperation and agreements like the Kyoto Protocol and the Paris Agreement.\n",
            "- The Paris Agreement’s goal to limit global warming to well below 2 degrees Celsius above pre-industrial levels, with efforts to limit it to 1.5 degrees, and the use of nationally determined contributions (NDCs) by countries.\n",
            "- National strategies such as carbon pricing mechanisms (carbon taxes and cap-and-trade systems) that incentivize emission reductions by assigning a cost to carbon emissions.\n",
            "- The impacts of climate change on terrestrial ecosystems, including shifts in habitat ranges, species distributions, and ecosystem functions, leading to biodiversity loss and ecological disruption.\n",
            "- The vulnerability of marine ecosystems to climate change, including rising sea temperatures, ocean acidification, changing currents, and their effects on marine biodiversity, species migration, reproductive cycles, and food webs.\n"
          ]
        }
      ],
      "source": [
        "from typing import Any, List\n",
        "import os\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_classic.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Custom Retriever (rerank)\n",
        "# -------------------------\n",
        "\n",
        "class CustomRetriever(BaseRetriever, BaseModel):\n",
        "    vectorstore: Any = Field()\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        docs = self.vectorstore.similarity_search(query, k=30)\n",
        "        return rerank_documents(query, docs, top_n=3)\n",
        "\n",
        "\n",
        "retriever = CustomRetriever(vectorstore=vectorstore)\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4.1\",\n",
        "    temperature=0,\n",
        "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    base_url=os.getenv(\"BASE_URL\"),\n",
        ")\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Use ONLY the following context to answer.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{input}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(\n",
        "    retriever=retriever,\n",
        "    combine_docs_chain=document_chain,\n",
        ")\n",
        "\n",
        "response = retrieval_chain.invoke({\"input\": \"What is in document 1?\"})\n",
        "\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys0SDK-HXvjW"
      },
      "source": [
        "### Example query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzTRKSzZXvjW",
        "outputId": "21946a7e-2339-476e-d8da-41fe173cba4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: what is the capital of france?\n",
            "Answer: The capital of France is not mentioned in the provided context.\n",
            "\n",
            "Relevant source documents:\n",
            "\n",
            "Document 1:\n",
            "negative emissions. The captured CO2 can be stored or used in various applications. Scaling \n",
            "DAC technology and reducing costs are critical for its widespread adoption. \n",
            "Chapter 16: Global Cooperation...\n",
            "\n",
            "Document 2:\n",
            "Chapter 6: Global and Local Climate Action \n",
            "International Collaboration \n",
            "United Nations Framework Convention on Climate Change (UNFCCC) \n",
            "The UNFCCC is an international treaty aimed at addressing clima...\n",
            "\n",
            "Document 3:\n",
            "a long time. These projects can help sequester carbon and provide new habitats for wildlife. \n",
            "Strategic planning and ecological considerations are essential for maximizing benefits. \n",
            "Climate Policy \n",
            "E...\n"
          ]
        }
      ],
      "source": [
        "result = retrieval_chain.invoke({\"input\": query})\n",
        "\n",
        "print(f\"\\nQuestion: {query}\")\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(\"\\nRelevant source documents:\")\n",
        "for i, doc in enumerate(result[\"context\"]):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NO4QVH5EXvjW"
      },
      "source": [
        "### Example that demonstrates why we should use reranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMvy2RCMXvjW",
        "outputId": "3e85236e-cf36-4399-ac61-08daf30ff77b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of Retrieval Techniques\n",
            "==================================\n",
            "Query: what is the capital of france?\n",
            "\n",
            "Baseline Retrieval Result:\n",
            "\n",
            "Document 1:\n",
            "The capital of France is huge.\n",
            "\n",
            "Document 2:\n",
            "The capital of France is great.\n",
            "\n",
            "Advanced Retrieval Result:\n",
            "\n",
            "Document 1:\n",
            "Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower. \n",
            "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\n",
            "\n",
            "Document 2:\n",
            "The capital of France is great.\n",
            "\n",
            "Document 3:\n",
            "The capital of France is beautiful.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "chunks = [\n",
        "    \"The capital of France is great.\",\n",
        "    \"The capital of France is huge.\",\n",
        "    \"The capital of France is beautiful.\",\n",
        "    \"\"\"Have you ever visited Paris? It is a beautiful city where you can eat delicious food and see the Eiffel Tower.\n",
        "    I really enjoyed all the cities in france, but its capital with the Eiffel Tower is my favorite city.\"\"\",\n",
        "    \"I really enjoyed my trip to Paris, France. The city is beautiful and the food is delicious. I would love to visit again. Such a great capital city.\"\n",
        "]\n",
        "docs = [Document(page_content=sentence) for sentence in chunks]\n",
        "\n",
        "\n",
        "def compare_rag_techniques(query: str, docs: List[Document] = docs) -> None:\n",
        "    embeddings = HuggingFaceEmbeddings(\n",
        "      model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "    print(\"Comparison of Retrieval Techniques\")\n",
        "    print(\"==================================\")\n",
        "    print(f\"Query: {query}\\n\")\n",
        "\n",
        "    print(\"Baseline Retrieval Result:\")\n",
        "    baseline_docs = vectorstore.similarity_search(query, k=2)\n",
        "    for i, doc in enumerate(baseline_docs):\n",
        "        print(f\"\\nDocument {i+1}:\")\n",
        "        print(doc.page_content)\n",
        "\n",
        "    print(\"\\nAdvanced Retrieval Result:\")\n",
        "    custom_retriever = CustomRetriever(vectorstore=vectorstore)\n",
        "    advanced_docs = custom_retriever._get_relevant_documents(query)\n",
        "    for i, doc in enumerate(advanced_docs):\n",
        "        print(f\"\\nDocument {i+1}:\")\n",
        "        print(doc.page_content)\n",
        "\n",
        "\n",
        "query = \"what is the capital of france?\"\n",
        "compare_rag_techniques(query, docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek9qkBy6XvjW"
      },
      "source": [
        "## Method 2: Cross Encoder models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6FWh_IVXvjW"
      },
      "source": [
        "### Define the cross encoder class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRKHgPy8XvjW",
        "outputId": "f68abc17-00ca-49ea-ab8c-824eba41beb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-192462221.py:3: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
            "  class CrossEncoderRetriever(BaseRetriever, BaseModel):\n"
          ]
        }
      ],
      "source": [
        "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "class CrossEncoderRetriever(BaseRetriever, BaseModel):\n",
        "    vectorstore: Any = Field(description=\"Vector store for initial retrieval\")\n",
        "    cross_encoder: Any = Field(description=\"Cross-encoder model for reranking\")\n",
        "    k: int = Field(default=5, description=\"Number of documents to retrieve initially\")\n",
        "    rerank_top_k: int = Field(default=3, description=\"Number of documents to return after reranking\")\n",
        "\n",
        "    class Config:\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        # Initial retrieval\n",
        "        initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
        "\n",
        "        # Prepare pairs for cross-encoder\n",
        "        pairs = [[query, doc.page_content] for doc in initial_docs]\n",
        "\n",
        "        # Get cross-encoder scores\n",
        "        scores = self.cross_encoder.predict(pairs)\n",
        "\n",
        "        # Sort documents by score\n",
        "        scored_docs = sorted(zip(initial_docs, scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Return top reranked documents\n",
        "        return [doc for doc, _ in scored_docs[:self.rerank_top_k]]\n",
        "\n",
        "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
        "        raise NotImplementedError(\"Async retrieval not implemented\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg9QkT0LXvjX"
      },
      "source": [
        "### Create an instance and showcase over an example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u40yyp6DXvjX",
        "outputId": "00c44961-a3cd-41f0-e4d5-99064bbb848d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What are the impacts of climate change on biodiversity?\n",
            "Answer: Climate change impacts biodiversity by shifting habitat ranges, changing species distributions, and affecting ecosystem functions. In terrestrial ecosystems like forests, grasslands, and deserts, these changes can lead to a loss of biodiversity and disrupt ecological balance. In marine ecosystems, rising sea temperatures, ocean acidification, and changing currents affect marine biodiversity, causing species migration and changes in reproductive cycles that can disrupt marine food webs and fisheries. Overall, these impacts threaten biodiversity and the stability of ecosystems.\n",
            "\n",
            "Relevant source documents:\n",
            "\n",
            "Document 1:\n",
            "Climate change is altering terrestrial ecosystems by shifting habitat ranges, changing species \n",
            "distributions, and impacting ecosystem functions. Forests, grasslands, and deserts are \n",
            "experiencing shi...\n",
            "\n",
            "Document 2:\n",
            "protection, and habitat creation. \n",
            "Climate-Resilient Conservation \n",
            "Conservation strategies must account for climate change impacts to be effective. This \n",
            "includes identifying climate refugia, areas le...\n",
            "\n",
            "Document 3:\n",
            "goals. Policies should promote synergies between biodiversity conservation and climate \n",
            "action. \n",
            "Chapter 10: Climate Change and Human Health \n",
            "Health Impacts \n",
            "Heat-Related Illnesses \n",
            "Rising temperature...\n",
            "\n",
            "Document 4:\n",
            "Local communities are often on the front lines of climate impacts and can be powerful agents \n",
            "of change. Community-based conservation projects involve residents in protecting and \n",
            "restoring natural re...\n",
            "\n",
            "Document 5:\n",
            "cultural perceptions. \n",
            "Youth Engagement \n",
            "Youth are vital stakeholders in climate action. Empowering young people through education, \n",
            "activism, and leadership opportunities can drive transformative cha...\n"
          ]
        }
      ],
      "source": [
        "# Create the cross-encoder retriever\n",
        "cross_encoder_retriever = CrossEncoderRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    cross_encoder=cross_encoder,\n",
        "    k=10,  # Retrieve 10 documents initially\n",
        "    rerank_top_k=5  # Return top 5 after reranking\n",
        ")\n",
        "\n",
        "# Set up the LLM\n",
        "llm = ChatOpenAI(temperature=0.1,\n",
        "                 model_name=\"gpt-4.1\",\n",
        "                 api_key=os.getenv('OPENAI_API_KEY'),\n",
        "                 base_url=os.getenv('BASE_URL')\n",
        "                )\n",
        "\n",
        "# # Create the RetrievalQA chain with the cross-encoder retriever\n",
        "# qa_chain = RetrievalQA.from_chain_type(\n",
        "#     llm=llm,\n",
        "#     chain_type=\"stuff\",\n",
        "#     retriever=cross_encoder_retriever,\n",
        "#     return_source_documents=True\n",
        "# )\n",
        "\n",
        "qa_chain = create_retrieval_chain(\n",
        "    retriever=cross_encoder_retriever,\n",
        "    combine_docs_chain=document_chain,\n",
        ")\n",
        "\n",
        "\n",
        "# Example query\n",
        "query = \"What are the impacts of climate change on biodiversity?\"\n",
        "result = qa_chain.invoke({\"input\": query})\n",
        "\n",
        "print(f\"\\nQuestion: {query}\")\n",
        "print(f\"Answer: {result['answer']}\")\n",
        "print(\"\\nRelevant source documents:\")\n",
        "for i, doc in enumerate(result[\"context\"]):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(doc.page_content[:200] + \"...\")  # Print first 200 characters of each document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5uu-1qvXvjX"
      },
      "source": [
        "![](https://europe-west1-rag-techniques-views-tracker.cloudfunctions.net/rag-techniques-tracker?notebook=all-rag-techniques--reranking)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}