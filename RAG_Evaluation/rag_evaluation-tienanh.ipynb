{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9568e5cabff641ff885fab303ccb5ed0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_37d9fec4906f477fa8278f4711f4ec2a"
          }
        },
        "d3c59b9261e64797ae3a30afe10166cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79f43b2640fd48e6a6871b55c691df2d",
            "placeholder": "​",
            "style": "IPY_MODEL_646602fffafb4814906792fde5b8f4b7",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "0169d38eaacd4799ae98f05fbabbbb7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_50cae662316a4e54b82716983bb2994d",
            "placeholder": "​",
            "style": "IPY_MODEL_ecfff0cbae9e4193abb36a31a36bc268",
            "value": ""
          }
        },
        "c7cc598e9ec448c6b16333d340c6cdaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_1cc24dfffe5e47cf984d35e74e207efb",
            "style": "IPY_MODEL_d80ce670327d486b90c02251b0be9ec1",
            "value": true
          }
        },
        "826188ff14f34544833afc794151d344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_06420374a8554fdaa354e3c2dbb5d520",
            "style": "IPY_MODEL_f1fd37301016445c98f287f7f0852de2",
            "tooltip": ""
          }
        },
        "11027f2e96e742c690b8f795039a4348": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55bce8e8ad4048d9bc73f92886432591",
            "placeholder": "​",
            "style": "IPY_MODEL_6c32c9696d98457fbf4e00905a450123",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "37d9fec4906f477fa8278f4711f4ec2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "79f43b2640fd48e6a6871b55c691df2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "646602fffafb4814906792fde5b8f4b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50cae662316a4e54b82716983bb2994d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecfff0cbae9e4193abb36a31a36bc268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cc24dfffe5e47cf984d35e74e207efb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d80ce670327d486b90c02251b0be9ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06420374a8554fdaa354e3c2dbb5d520": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1fd37301016445c98f287f7f0852de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "55bce8e8ad4048d9bc73f92886432591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c32c9696d98457fbf4e00905a450123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94f45986758843cf97f6aed27a6275cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_559eeb34e39b4451b26d4b3e539654de",
            "placeholder": "​",
            "style": "IPY_MODEL_8d6016bcac54450f8596ec7cb0ef0f6c",
            "value": "Connecting..."
          }
        },
        "559eeb34e39b4451b26d4b3e539654de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d6016bcac54450f8596ec7cb0ef0f6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84fb8c35511044a59726489ea0fca0c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac333cd4c3864c298e8b1e249f7de3c1",
              "IPY_MODEL_9666a45a4a6146839af03153c29c84cd",
              "IPY_MODEL_704c8e4bcb93480abeffec11c6213e7a"
            ],
            "layout": "IPY_MODEL_dca012abf9454c07b17065aa8df24198"
          }
        },
        "ac333cd4c3864c298e8b1e249f7de3c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9219e036a9fd4e339667a7d92a6e2d00",
            "placeholder": "​",
            "style": "IPY_MODEL_de93478cc3fb48f0a3e82a0eb7e22cff",
            "value": "README.md: 100%"
          }
        },
        "9666a45a4a6146839af03153c29c84cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_892376ceb3c649d294c6e0164174dd1b",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_830be910cad94114b464546513de74c7",
            "value": 21
          }
        },
        "704c8e4bcb93480abeffec11c6213e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d35c278f1e5492295ef3bc35cc4e34a",
            "placeholder": "​",
            "style": "IPY_MODEL_c8cf123ee2d640229f6914e784e9fd1b",
            "value": " 21.0/21.0 [00:00&lt;00:00, 511B/s]"
          }
        },
        "dca012abf9454c07b17065aa8df24198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9219e036a9fd4e339667a7d92a6e2d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de93478cc3fb48f0a3e82a0eb7e22cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "892376ceb3c649d294c6e0164174dd1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "830be910cad94114b464546513de74c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d35c278f1e5492295ef3bc35cc4e34a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8cf123ee2d640229f6914e784e9fd1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b50544503754c2ebe27e41be904f849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca91619d9f65465ba537cb7f0fa20f2e",
              "IPY_MODEL_45d33b3eff9248849d1998bc556fa553",
              "IPY_MODEL_3874e2e444584ee3a964770d00bed329"
            ],
            "layout": "IPY_MODEL_93c96bdfe7c34332b3b9b93bae2f1802"
          }
        },
        "ca91619d9f65465ba537cb7f0fa20f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab98971cf0204647b6241a1ffcd7ec6b",
            "placeholder": "​",
            "style": "IPY_MODEL_eeb6f61cba984c069a864a05f93b90da",
            "value": "huggingface_doc.csv: 100%"
          }
        },
        "45d33b3eff9248849d1998bc556fa553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7226b7c0c434a838579f5cfec8a4a23",
            "max": 21954601,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0590571785e47c48a0f092b90be6e85",
            "value": 21954601
          }
        },
        "3874e2e444584ee3a964770d00bed329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84851052caab42d0bb4b5b6f44e001d9",
            "placeholder": "​",
            "style": "IPY_MODEL_c37a2dceab9d4eb39879f1a9704bddd1",
            "value": " 22.0M/22.0M [00:01&lt;00:00, 14.8MB/s]"
          }
        },
        "93c96bdfe7c34332b3b9b93bae2f1802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab98971cf0204647b6241a1ffcd7ec6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eeb6f61cba984c069a864a05f93b90da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7226b7c0c434a838579f5cfec8a4a23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0590571785e47c48a0f092b90be6e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84851052caab42d0bb4b5b6f44e001d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c37a2dceab9d4eb39879f1a9704bddd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc8ec5359e9b46bfb770fc534ee200bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae9cb759340c4c2699c33dda30c66b99",
              "IPY_MODEL_615b84ac5b624144bde507996a296a7e",
              "IPY_MODEL_a43c281742d04db2ad8c5bb1f8197dac"
            ],
            "layout": "IPY_MODEL_c27c2ddb94784a40aa370df978b2dc6b"
          }
        },
        "ae9cb759340c4c2699c33dda30c66b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dcaa0b724764c10b64a3abcfa039459",
            "placeholder": "​",
            "style": "IPY_MODEL_477e05e23a1a4513952b5f3091df7280",
            "value": "Generating train split: 100%"
          }
        },
        "615b84ac5b624144bde507996a296a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f58f07951be4418989e5bb2b5e7ddf93",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8358f8ae15f34d65bf10d400e7c8b791",
            "value": 2647
          }
        },
        "a43c281742d04db2ad8c5bb1f8197dac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa51fd3f815b4b8e84ebc946671866cc",
            "placeholder": "​",
            "style": "IPY_MODEL_1f66c7d43808479db97b5db21eb2af42",
            "value": " 2647/2647 [00:00&lt;00:00, 6549.99 examples/s]"
          }
        },
        "c27c2ddb94784a40aa370df978b2dc6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dcaa0b724764c10b64a3abcfa039459": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "477e05e23a1a4513952b5f3091df7280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f58f07951be4418989e5bb2b5e7ddf93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8358f8ae15f34d65bf10d400e7c8b791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fa51fd3f815b4b8e84ebc946671866cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f66c7d43808479db97b5db21eb2af42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9b3a9c585e041118073dad0e052ee35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5eb3842be12b4e7f9fbf4d2a8a89de0f",
              "IPY_MODEL_806ab45b0cf34060b9518d44ade3c3d0",
              "IPY_MODEL_257a91020fe64336acbe50e52abc1af5"
            ],
            "layout": "IPY_MODEL_c570c71811b340eba9ae3965eec217ad"
          }
        },
        "5eb3842be12b4e7f9fbf4d2a8a89de0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6561e26f3c0a4cb9a6badeb051a05b91",
            "placeholder": "​",
            "style": "IPY_MODEL_7956e5d87fed4d26a20fa865cd97fa31",
            "value": "100%"
          }
        },
        "806ab45b0cf34060b9518d44ade3c3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ddeda9fba4f43fda8b7d92609e5da30",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c1b7cedeef243edbf9493d4d76ecae9",
            "value": 2647
          }
        },
        "257a91020fe64336acbe50e52abc1af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acb6324a9e274dbeacd8eedcb7ec8d92",
            "placeholder": "​",
            "style": "IPY_MODEL_982fea8a7c124e5884732e293d834f83",
            "value": " 2647/2647 [00:00&lt;00:00, 20153.63it/s]"
          }
        },
        "c570c71811b340eba9ae3965eec217ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6561e26f3c0a4cb9a6badeb051a05b91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7956e5d87fed4d26a20fa865cd97fa31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ddeda9fba4f43fda8b7d92609e5da30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c1b7cedeef243edbf9493d4d76ecae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acb6324a9e274dbeacd8eedcb7ec8d92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "982fea8a7c124e5884732e293d834f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a68fce5fa89a4cb39122d7d2f8e6011c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3395fa836cb74511b8ccd977f369c06d",
              "IPY_MODEL_090f887346f94e3b8da283cdfe64a450",
              "IPY_MODEL_300ba0511ff44e0cadb51c844babaff0"
            ],
            "layout": "IPY_MODEL_aa1634e33ce74ef482bc88e582d95098"
          }
        },
        "3395fa836cb74511b8ccd977f369c06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f08b205a45f4ca2995e0334d8564999",
            "placeholder": "​",
            "style": "IPY_MODEL_fcc7721449164f76a7ce8c0ecfbc1ef9",
            "value": "100%"
          }
        },
        "090f887346f94e3b8da283cdfe64a450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e7f74732fea45cbaa609ec08a46c42f",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c8360adf7b324fa5a40e65afd8677d76",
            "value": 10
          }
        },
        "300ba0511ff44e0cadb51c844babaff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13b3df04efe0417990271f68af06227e",
            "placeholder": "​",
            "style": "IPY_MODEL_ce80fba32e6542d7910dc5fd62c70097",
            "value": " 10/10 [00:32&lt;00:00,  3.33s/it]"
          }
        },
        "aa1634e33ce74ef482bc88e582d95098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f08b205a45f4ca2995e0334d8564999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc7721449164f76a7ce8c0ecfbc1ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e7f74732fea45cbaa609ec08a46c42f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8360adf7b324fa5a40e65afd8677d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13b3df04efe0417990271f68af06227e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce80fba32e6542d7910dc5fd62c70097": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67ccfb78e01845b2a309027c84c5d3bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5263138439cf4b8282abbd7faa34dabb",
              "IPY_MODEL_0b5e114d4c0c481a8646a36a4d3af68a",
              "IPY_MODEL_61d404d2090e455c9b75df3713dbad67"
            ],
            "layout": "IPY_MODEL_98ea1e2f06da4af8a299c5411c778aff"
          }
        },
        "5263138439cf4b8282abbd7faa34dabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_268485be65a14f979a69e0f3e668faa1",
            "placeholder": "​",
            "style": "IPY_MODEL_4b4167ac59ce495ea7c439f281d4d9d4",
            "value": "100%"
          }
        },
        "0b5e114d4c0c481a8646a36a4d3af68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eeabcef351c243459be8aa7d4e4544b5",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1677be7041244985a03990f2d60c8b67",
            "value": 10
          }
        },
        "61d404d2090e455c9b75df3713dbad67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16e194846c4e49538403f8669ffbf878",
            "placeholder": "​",
            "style": "IPY_MODEL_294a478356554300a74bb8f277cd7ff1",
            "value": " 10/10 [02:18&lt;00:00, 12.77s/it]"
          }
        },
        "98ea1e2f06da4af8a299c5411c778aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "268485be65a14f979a69e0f3e668faa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b4167ac59ce495ea7c439f281d4d9d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eeabcef351c243459be8aa7d4e4544b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1677be7041244985a03990f2d60c8b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16e194846c4e49538403f8669ffbf878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "294a478356554300a74bb8f277cd7ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a5d5a0f99474c64a2addfa9a7de11fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8cf32bda73743a9bc76951fee1a4b22",
              "IPY_MODEL_0fe1e5eb10644f1e86240a801d6883fb",
              "IPY_MODEL_019db5c2da354eb196574bc14635f61c"
            ],
            "layout": "IPY_MODEL_c73e3b63834747acafaf570fc26fdcb0"
          }
        },
        "e8cf32bda73743a9bc76951fee1a4b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9814ac7438f4a61ab93867896ac210b",
            "placeholder": "​",
            "style": "IPY_MODEL_a85b28b16ed641e886afc8a6824677e9",
            "value": "README.md: 100%"
          }
        },
        "0fe1e5eb10644f1e86240a801d6883fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d10b2070eb54da29b94b65baae1bdc4",
            "max": 893,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6bf12790c1a4dc8b3b4e2da18330d33",
            "value": 893
          }
        },
        "019db5c2da354eb196574bc14635f61c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2433e06600104b24bae5530e4468ae5e",
            "placeholder": "​",
            "style": "IPY_MODEL_11a0497c4c824339b5b514230eb41a04",
            "value": " 893/893 [00:00&lt;00:00, 87.8kB/s]"
          }
        },
        "c73e3b63834747acafaf570fc26fdcb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9814ac7438f4a61ab93867896ac210b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a85b28b16ed641e886afc8a6824677e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d10b2070eb54da29b94b65baae1bdc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6bf12790c1a4dc8b3b4e2da18330d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2433e06600104b24bae5530e4468ae5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11a0497c4c824339b5b514230eb41a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c88c802b5884b7a87dd6c3508476886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_edf9552b9cea4d59923473a734ed873e",
              "IPY_MODEL_badd665411d6439da8552e7603c20a23",
              "IPY_MODEL_229bf92377d7402990b67fb7aecb4a11"
            ],
            "layout": "IPY_MODEL_49a3e6dbab6c45b399c4b0d2b11de87c"
          }
        },
        "edf9552b9cea4d59923473a734ed873e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_392b421fd88e4d7a9db9bcada8932077",
            "placeholder": "​",
            "style": "IPY_MODEL_702506e4198b49e786e43fd8b4dd2202",
            "value": "data/train-00000-of-00001.parquet: 100%"
          }
        },
        "badd665411d6439da8552e7603c20a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56f5c3b35518482d9f097608cb122a51",
            "max": 289016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32d70e9c2577459d99fea09717744b2f",
            "value": 289016
          }
        },
        "229bf92377d7402990b67fb7aecb4a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6ad1d842cf44d27b4dc4233fcaff2c0",
            "placeholder": "​",
            "style": "IPY_MODEL_cb1eed43000445c3b8b2229c02495647",
            "value": " 289k/289k [00:01&lt;00:00, 234kB/s]"
          }
        },
        "49a3e6dbab6c45b399c4b0d2b11de87c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "392b421fd88e4d7a9db9bcada8932077": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "702506e4198b49e786e43fd8b4dd2202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56f5c3b35518482d9f097608cb122a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32d70e9c2577459d99fea09717744b2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6ad1d842cf44d27b4dc4233fcaff2c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb1eed43000445c3b8b2229c02495647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3919bb913f140ec94f96a08122306a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d9f36dd918e4c6c865dcc6a3e781f14",
              "IPY_MODEL_7ccc6f7cec7b4186bc49831dea01aac4",
              "IPY_MODEL_11e8f67155fc414187b411162f61cb53"
            ],
            "layout": "IPY_MODEL_c9d44e6aca5d46d19353a9d7e36219e7"
          }
        },
        "6d9f36dd918e4c6c865dcc6a3e781f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adf77b9ba4f449ec92eaeba464bc0cae",
            "placeholder": "​",
            "style": "IPY_MODEL_88bc9f06ffbe49ac8814000b6b38b76e",
            "value": "Generating train split: 100%"
          }
        },
        "7ccc6f7cec7b4186bc49831dea01aac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ddc0839006a4f219a942d1ed4c3f915",
            "max": 65,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8fa764a964047559ed35c0c112cff2d",
            "value": 65
          }
        },
        "11e8f67155fc414187b411162f61cb53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6189b0a76604e199cf2d89b844f9606",
            "placeholder": "​",
            "style": "IPY_MODEL_04b45c84fcc64bc9a51d4cdd32e396a7",
            "value": " 65/65 [00:00&lt;00:00, 1085.40 examples/s]"
          }
        },
        "c9d44e6aca5d46d19353a9d7e36219e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adf77b9ba4f449ec92eaeba464bc0cae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88bc9f06ffbe49ac8814000b6b38b76e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ddc0839006a4f219a942d1ed4c3f915": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8fa764a964047559ed35c0c112cff2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6189b0a76604e199cf2d89b844f9606": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04b45c84fcc64bc9a51d4cdd32e396a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3689a1d53aed49269d4a8f1ee64ac301": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aec569879cfe405b9bcce7622a0f957d",
              "IPY_MODEL_ab6e699079474cadbea9ce0482983897",
              "IPY_MODEL_af511b2328ee4e3896019023ef072219"
            ],
            "layout": "IPY_MODEL_6695ed417878432fa4c55a266d9bceab"
          }
        },
        "aec569879cfe405b9bcce7622a0f957d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41396c21bce74c5cb86d962d7c0dd2d0",
            "placeholder": "​",
            "style": "IPY_MODEL_415bf991e48d432983a877727cc01c1d",
            "value": "100%"
          }
        },
        "ab6e699079474cadbea9ce0482983897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9ae5f90e46545489800fcb2eb872613",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b0a96895dcf489da1e7fd58f8e84f40",
            "value": 2647
          }
        },
        "af511b2328ee4e3896019023ef072219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_535a9c3bc22748129d55f1e28e20c511",
            "placeholder": "​",
            "style": "IPY_MODEL_2ff070b046ef4cb2841e4c55c96aa094",
            "value": " 2647/2647 [00:00&lt;00:00, 15508.91it/s]"
          }
        },
        "6695ed417878432fa4c55a266d9bceab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41396c21bce74c5cb86d962d7c0dd2d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "415bf991e48d432983a877727cc01c1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9ae5f90e46545489800fcb2eb872613": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b0a96895dcf489da1e7fd58f8e84f40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "535a9c3bc22748129d55f1e28e20c511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ff070b046ef4cb2841e4c55c96aa094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4ce26d493404446bf8ef1d006c63b73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aca878a367f24e5eaeaf48376331cf16",
              "IPY_MODEL_c2d90b5568a3431c803c5ef206713648",
              "IPY_MODEL_d492d1f9ac9c45ccb3fecb64d18bdb7a"
            ],
            "layout": "IPY_MODEL_47cdf2a1496346a3a5cd568a50c4dd77"
          }
        },
        "aca878a367f24e5eaeaf48376331cf16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e16cf22f5e7f4381ac961914c3d7cebe",
            "placeholder": "​",
            "style": "IPY_MODEL_3388d99f080a43d298bf1609e9f41b00",
            "value": "100%"
          }
        },
        "c2d90b5568a3431c803c5ef206713648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f018a8b0e2e34371b9efaefc057293b2",
            "max": 65,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c674b4aeb854a88bd59b690cbf2aa7f",
            "value": 65
          }
        },
        "d492d1f9ac9c45ccb3fecb64d18bdb7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8afa32fa01b4420c9a5dd6fb783c469b",
            "placeholder": "​",
            "style": "IPY_MODEL_e656da250f954b76bad047c380c68546",
            "value": " 65/65 [10:07&lt;00:00, 19.81s/it]"
          }
        },
        "47cdf2a1496346a3a5cd568a50c4dd77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e16cf22f5e7f4381ac961914c3d7cebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3388d99f080a43d298bf1609e9f41b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f018a8b0e2e34371b9efaefc057293b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c674b4aeb854a88bd59b690cbf2aa7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8afa32fa01b4420c9a5dd6fb783c469b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e656da250f954b76bad047c380c68546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "496d3ea96f1a4730bbecb164d269b2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac93b6953e4544eaafe7c74a47f09c20",
              "IPY_MODEL_6e132bfeafc040dba1eec37fac8d46d0",
              "IPY_MODEL_9331f900c19a4b689f139db0a88a7d0d"
            ],
            "layout": "IPY_MODEL_53edc179da214a46b860ca7a9db117f8"
          }
        },
        "ac93b6953e4544eaafe7c74a47f09c20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc289aee2b974ad1be0474672ae5700b",
            "placeholder": "​",
            "style": "IPY_MODEL_1bf8ea6489e74199af14ac827934f92a",
            "value": "100%"
          }
        },
        "6e132bfeafc040dba1eec37fac8d46d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acd0fab29e3a457e8b78e5c82e1e8f7c",
            "max": 65,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39a75f8ad2d64e78b9d247274a21398a",
            "value": 65
          }
        },
        "9331f900c19a4b689f139db0a88a7d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25fd85c5861845308a412117734a507d",
            "placeholder": "​",
            "style": "IPY_MODEL_fb16e35f50bc4333aa8ad7bea29807c1",
            "value": " 65/65 [22:25&lt;00:00, 18.77s/it]"
          }
        },
        "53edc179da214a46b860ca7a9db117f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc289aee2b974ad1be0474672ae5700b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bf8ea6489e74199af14ac827934f92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acd0fab29e3a457e8b78e5c82e1e8f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39a75f8ad2d64e78b9d247274a21398a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25fd85c5861845308a412117734a507d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb16e35f50bc4333aa8ad7bea29807c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Evaluation\n",
        "First, we install the required model dependancies."
      ],
      "metadata": {
        "id": "FT_DJUM5_QDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets langchain-community langchain-openai langchain-core\n",
        "!pip install ragatouille==0.0.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q9Tk1uWI5BT7",
        "outputId": "3af04816-ecf1-4260-bbdf-65d45348b596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ragatouille==0.0.9 in /usr/local/lib/python3.12/dist-packages (0.0.9)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (0.14.12)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (1.13.2)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (1.2.6)\n",
            "Requirement already satisfied: colbert-ai>=0.2.19 in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (0.2.22)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (1.2.3)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (1.20.1)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (2.5.2)\n",
            "Requirement already satisfied: voyager in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (2.9.0+cu126)\n",
            "Requirement already satisfied: fast-pytorch-kmeans in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (0.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from ragatouille==0.0.9) (5.2.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9) (3.8.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9) (4.0.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9) (3.1.2)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9) (3.1.46)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9) (1.2.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9) (1.13.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9) (4.57.3)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille==0.0.9) (5.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille==0.0.9) (3.5.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu->ragatouille==0.0.9) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu->ragatouille==0.0.9) (25.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain->ragatouille==0.0.9) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain->ragatouille==0.0.9) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille==0.0.9) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille==0.0.9) (0.6.1)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille==0.0.9) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille==0.0.9) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille==0.0.9) (0.13.0)\n",
            "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille==0.0.9) (0.5.3)\n",
            "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.12 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille==0.0.9) (0.14.12)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille==0.0.9) (0.5.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille==0.0.9) (0.9.4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille==0.0.9) (0.6.13)\n",
            "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille==0.0.9) (0.5.6)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille==0.0.9) (0.5.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille==0.0.9) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->ragatouille==0.0.9) (5.29.5)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx->ragatouille==0.0.9) (0.5.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->ragatouille==0.0.9) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->ragatouille==0.0.9) (0.36.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from srsly->ragatouille==0.0.9) (2.0.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->ragatouille==0.0.9) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->ragatouille==0.0.9) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core->ragatouille==0.0.9) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragatouille==0.0.9) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragatouille==0.0.9) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragatouille==0.0.9) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragatouille==0.0.9) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille==0.0.9) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille==0.0.9) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille==0.0.9) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille==0.0.9) (0.25.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (3.13.3)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (0.22.1)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (1.2.0)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (2.12.2)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (1.6.0)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (4.5.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (2.0.45)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (0.12.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (1.17.3)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (2.14.0)\n",
            "Requirement already satisfied: llama-cloud==0.1.35 in /usr/local/lib/python3.12/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille==0.0.9) (0.1.35)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille==0.0.9) (2026.1.4)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (0.7.1)\n",
            "Requirement already satisfied: pandas<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (2.2.2)\n",
            "Requirement already satisfied: pypdf<7,>=6.1.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (6.6.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille==0.0.9) (0.6.54)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index->ragatouille==0.0.9) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille==0.0.9) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille==0.0.9) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille==0.0.9) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->ragatouille==0.0.9) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille==0.0.9) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille==0.0.9) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9) (0.3.8)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille==0.0.9) (0.70.16)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille==0.0.9) (3.1.5)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from GitPython->colbert-ai>=0.2.19->ragatouille==0.0.9) (4.0.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->ragatouille==0.0.9) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (1.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (2.8.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->GitPython->colbert-ai>=0.2.19->ragatouille==0.0.9) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille==0.0.9) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille==0.0.9) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille==0.0.9) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille==0.0.9) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain->ragatouille==0.0.9) (1.12.1)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (0.4.2)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.54 in /usr/local/lib/python3.12/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille==0.0.9) (0.6.54)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers->ragatouille==0.0.9) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers->ragatouille==0.0.9) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (3.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (3.26.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille==0.0.9) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille==0.0.9) (0.4.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %reload_ext autoreload\n",
        "# %autoreload 2"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cTwMpY8OnfWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple, Any\n",
        "import json\n",
        "import datasets\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "metadata": {
        "id": "xnBmJWABQ7yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "9568e5cabff641ff885fab303ccb5ed0",
            "d3c59b9261e64797ae3a30afe10166cf",
            "0169d38eaacd4799ae98f05fbabbbb7a",
            "c7cc598e9ec448c6b16333d340c6cdaa",
            "826188ff14f34544833afc794151d344",
            "11027f2e96e742c690b8f795039a4348",
            "37d9fec4906f477fa8278f4711f4ec2a",
            "79f43b2640fd48e6a6871b55c691df2d",
            "646602fffafb4814906792fde5b8f4b7",
            "50cae662316a4e54b82716983bb2994d",
            "ecfff0cbae9e4193abb36a31a36bc268",
            "1cc24dfffe5e47cf984d35e74e207efb",
            "d80ce670327d486b90c02251b0be9ec1",
            "06420374a8554fdaa354e3c2dbb5d520",
            "f1fd37301016445c98f287f7f0852de2",
            "55bce8e8ad4048d9bc73f92886432591",
            "6c32c9696d98457fbf4e00905a450123",
            "94f45986758843cf97f6aed27a6275cc",
            "559eeb34e39b4451b26d4b3e539654de",
            "8d6016bcac54450f8596ec7cb0ef0f6c"
          ]
        },
        "id": "EqEg65bMRW0f",
        "outputId": "f1fe1b6c-0c16-4c27-c592-a1df802768fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9568e5cabff641ff885fab303ccb5ed0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load your knowledge base"
      ],
      "metadata": {
        "id": "kcwUbMA-_rPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "84fb8c35511044a59726489ea0fca0c9",
            "ac333cd4c3864c298e8b1e249f7de3c1",
            "9666a45a4a6146839af03153c29c84cd",
            "704c8e4bcb93480abeffec11c6213e7a",
            "dca012abf9454c07b17065aa8df24198",
            "9219e036a9fd4e339667a7d92a6e2d00",
            "de93478cc3fb48f0a3e82a0eb7e22cff",
            "892376ceb3c649d294c6e0164174dd1b",
            "830be910cad94114b464546513de74c7",
            "4d35c278f1e5492295ef3bc35cc4e34a",
            "c8cf123ee2d640229f6914e784e9fd1b",
            "9b50544503754c2ebe27e41be904f849",
            "ca91619d9f65465ba537cb7f0fa20f2e",
            "45d33b3eff9248849d1998bc556fa553",
            "3874e2e444584ee3a964770d00bed329",
            "93c96bdfe7c34332b3b9b93bae2f1802",
            "ab98971cf0204647b6241a1ffcd7ec6b",
            "eeb6f61cba984c069a864a05f93b90da",
            "d7226b7c0c434a838579f5cfec8a4a23",
            "d0590571785e47c48a0f092b90be6e85",
            "84851052caab42d0bb4b5b6f44e001d9",
            "c37a2dceab9d4eb39879f1a9704bddd1",
            "cc8ec5359e9b46bfb770fc534ee200bd",
            "ae9cb759340c4c2699c33dda30c66b99",
            "615b84ac5b624144bde507996a296a7e",
            "a43c281742d04db2ad8c5bb1f8197dac",
            "c27c2ddb94784a40aa370df978b2dc6b",
            "2dcaa0b724764c10b64a3abcfa039459",
            "477e05e23a1a4513952b5f3091df7280",
            "f58f07951be4418989e5bb2b5e7ddf93",
            "8358f8ae15f34d65bf10d400e7c8b791",
            "fa51fd3f815b4b8e84ebc946671866cc",
            "1f66c7d43808479db97b5db21eb2af42"
          ]
        },
        "id": "3ZBOid8ARjaF",
        "outputId": "fa30e389-a0e1-48d9-e8b2-2d92eafb1163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84fb8c35511044a59726489ea0fca0c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "huggingface_doc.csv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b50544503754c2ebe27e41be904f849"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc8ec5359e9b46bfb770fc534ee200bd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Build a synthetic dataset for evaluation\n",
        "We first build a synthetic dataset of questions and associated contexts. The method is to get elements from our knowledge base, and ask an LLM to generate questions based on these documents.\n",
        "\n",
        "Then we setup other LLM agents to act as quality filters for the generated QA couples: each of them will act as the filter for a specific flaw.\n",
        "\n",
        "### 1.1. Prepare source documents"
      ],
      "metadata": {
        "id": "pnbzQ16A_3OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document as LangchainDocument\n",
        "\n",
        "langchain_docs = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm(ds)\n",
        "]\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "docs_processed = []\n",
        "for doc in langchain_docs:\n",
        "    docs_processed += text_splitter.split_documents([doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c9b3a9c585e041118073dad0e052ee35",
            "5eb3842be12b4e7f9fbf4d2a8a89de0f",
            "806ab45b0cf34060b9518d44ade3c3d0",
            "257a91020fe64336acbe50e52abc1af5",
            "c570c71811b340eba9ae3965eec217ad",
            "6561e26f3c0a4cb9a6badeb051a05b91",
            "7956e5d87fed4d26a20fa865cd97fa31",
            "8ddeda9fba4f43fda8b7d92609e5da30",
            "4c1b7cedeef243edbf9493d4d76ecae9",
            "acb6324a9e274dbeacd8eedcb7ec8d92",
            "982fea8a7c124e5884732e293d834f83"
          ]
        },
        "id": "HEADoj_sIFl4",
        "outputId": "905dfe10-82ad-4981-c1e3-1cd423653242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2647 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9b3a9c585e041118073dad0e052ee35"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Setup agents for question generation\n",
        "\n",
        "We use [Mixtral](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2) (\"mistralai/Mistral-7B-Instruct-v0.2\") for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)."
      ],
      "metadata": {
        "id": "JqJfV_K1__Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get(\"key_hf\")\n",
        "\n",
        "repo_model = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "\n",
        "llm_client = InferenceClient(\n",
        "    model = repo_model,\n",
        "    token = hf_token,\n",
        "    timeout = 120\n",
        ")\n",
        "\n",
        "def call_llm(inference_client: InferenceClient, prompt: str):\n",
        "    response = inference_client.chat.completions.create(\n",
        "        messages=\n",
        "          [{\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt\n",
        "          },],\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "call_llm(llm_client, \"This is a test context\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "y3K2WAKKR-2L",
        "outputId": "7a0d8b53-186b-427b-face-b7a3ad071572",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I see. In programming, a test context is an instance of a test runner or testing framework that is used to execute tests. It provides access to various resources and services that are needed for the tests to run and report their results.\\n\\nFor example, a test context in a unit testing framework might give you access to a mocking library for creating test doubles, a database connection for testing database interactions, or a logging service for reporting test results.\\n\\nIf you have a specific testing framework or testing scenario in mind, I'd be happy to help you with any questions you have about creating and using a test context. Just let me know what you need!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QA_generation_prompt = \"\"\"\n",
        "Your task is to write a factoid question and an answer given a context.\n",
        "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
        "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
        "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Output:::\n",
        "Factoid question: (your factoid question)\n",
        "Answer: (your answer to the factoid question)\n",
        "\n",
        "Now here is the context.\n",
        "\n",
        "Context: {context}\\n\n",
        "Output:::\"\"\""
      ],
      "metadata": {
        "id": "GQbmg_LSV_ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's generate our QA couples.\n",
        "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
        "\n",
        "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
      ],
      "metadata": {
        "id": "hhQTOcHXASK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "N_GENERATIONS = 10  # We intentionally generate only 10 QA couples here for cost and time considerations\n",
        "\n",
        "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
        "\n",
        "outputs = []\n",
        "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
        "    # Generate QA couple\n",
        "    output_QA_couple = call_llm(\n",
        "        llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
        "    )\n",
        "    try:\n",
        "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
        "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
        "        assert len(answer) < 300, \"Answer is too long\"\n",
        "        outputs.append(\n",
        "            {\n",
        "                \"context\": sampled_context.page_content,\n",
        "                \"question\": question,\n",
        "                \"answer\": answer,\n",
        "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
        "            }\n",
        "        )\n",
        "    except:\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "a68fce5fa89a4cb39122d7d2f8e6011c",
            "3395fa836cb74511b8ccd977f369c06d",
            "090f887346f94e3b8da283cdfe64a450",
            "300ba0511ff44e0cadb51c844babaff0",
            "aa1634e33ce74ef482bc88e582d95098",
            "2f08b205a45f4ca2995e0334d8564999",
            "fcc7721449164f76a7ce8c0ecfbc1ef9",
            "1e7f74732fea45cbaa609ec08a46c42f",
            "c8360adf7b324fa5a40e65afd8677d76",
            "13b3df04efe0417990271f68af06227e",
            "ce80fba32e6542d7910dc5fd62c70097"
          ]
        },
        "id": "Xcnlb9XIVzQc",
        "outputId": "618e1117-9822-4c32-c931-f76e1124200e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 10 QA couples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a68fce5fa89a4cb39122d7d2f8e6011c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(outputs))\n",
        "display(pd.DataFrame(outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BGfExgwPWIep",
        "outputId": "55743f16-7bcf-4cd4-eff9-73348caed2aa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            context  \\\n",
              "0  - Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\\nmoment.\\n- Order: 'C' or row-major. This seems to have won. We can add that information later if needed.\\n- Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format.\\n\\n### Benefits\\n\\nSince we can invent a new format we can propose additional benefits:\\n\\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\\non the size of the header of 100MB to prevent parsing extremely large JSON.\\n Also when reading the file, there's a guarantee that addresses in the file\\n do not overlap in any way, meaning when you're loading a file you should never\\n exceed the size of the file in memory\\n\\n- Faster load: PyTorch seems to be the fastest file to load out in the major\\nML formats. However, it does seem to have an extra copy on CPU, which we\\ncan bypass in this lib by using `torch.UntypedStorage.from_file`.\\nCurrently, CPU loading times are extremely fast with this lib compared to pickle.\\nGPU loading times are as fast or faster than PyTorch equivalent.\\nLoading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\\nto be faster too somehow (similar behavior in torch pickle)\\n\\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\\nload only part of the tensors on the various models. For\\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s.\\nThis really speeds up feedbacks loops when developing on the model. For instance\\nyou don't have to have separate copies of the weights when changing the distribution\\nstrategy (for instance Pipeline Parallelism vs Tensor Parallelism).\\n\\nLicense: Apache-2.0   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |      |      |[camembert/camembert-large](https://huggingface.co/camembert/camembert-large)                                                                      |3660        |6       |                         |                                                                                   |[LICENSE](https://huggingface.co/camembert/camembert-large/blob/main/LICENSE)                                           |                                                                                                    |             |\\n|      |      |[stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)                          |3553        |80      |                         |                                                                                   |[LICENSE](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b/blob/main/LICENSE)                     |                                                                                                    |             |\\n|      |      |[TheBloke/llama-2-70b-Guanaco-QLoRA-fp16](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16)                                          |3537        |52      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16/blob/main/LICENSE.txt)                     |                                                                                                    |             |   \n",
              "2                                                                                                                                                                                                                                                                                            Deep Layer Aggregation\\n\\nExtending  “shallow” skip connections, **Dense Layer Aggregation (DLA)** incorporates more depth and sharing. The authors introduce two structures for deep layer aggregation (DLA): iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA). These structures are expressed through an architectural framework, independent of the choice of backbone, for compatibility with current and future networks. \\n\\nIDA focuses on fusing resolutions and scales while HDA focuses on merging features from all modules and channels. IDA follows the base hierarchy to refine resolution and aggregate scale stage-bystage. HDA assembles its own hierarchy of tree-structured connections that cross and merge stages to aggregate different levels of representation. \\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model('dla102', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert('RGB')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])\\n```   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Objective: Move the agent so the box is within the agents its field of view\\n\\nActors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera\\n\\nObservation space: \\n- An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format.\\n  \\nAction space:\\n- A discrete action space with 3 possible actions\\n- Turn left 10 degrees\\n- Turn right 10 degrees\\n- Move forward\\n\\nReward function:\\n- A sparse reward for moving the box within a 60 degree fov cone in front of the agent.\\n- A timeout penaly of -1 if the agent does not reach the object in 100 time-steps\\n\\nParallel: 4 independent instances of the same environment configuration.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|\\n| [How to quantize a question answering model with NNCF](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb) | Show how to apply post-training quantization on a question answering model using [NNCF](https://github.com/openvinotoolkit/nncf) and to accelerate inference with OpenVINO| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)|   \n",
              "5                                                                         Simple call on one item:\\n\\n```python\\n>>> pipe = pipeline(\"text-classification\")\\n>>> pipe(\"This restaurant is awesome\")\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\\n```\\n\\nIf you want to use a specific model from the [hub](https://huggingface.co) you can ignore the task if the model on\\nthe hub already defines it:\\n\\n```python\\n>>> pipe = pipeline(model=\"roberta-large-mnli\")\\n>>> pipe(\"This restaurant is awesome\")\\n[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]\\n```\\n\\nTo call a pipeline on many items, you can call it with a *list*.\\n\\n```python\\n>>> pipe = pipeline(\"text-classification\")\\n>>> pipe([\"This restaurant is awesome\", \"This restaurant is awful\"])\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135},\\n {'label': 'NEGATIVE', 'score': 0.9996669292449951}]\\n```\\n\\nTo iterate over full datasets it is recommended to use a `dataset` directly. This means you don't need to allocate\\nthe whole dataset at once, nor do you need to do batching yourself. This should work just as fast as custom loops on\\nGPU. If it doesn't don't hesitate to create an issue.\\n\\n```python\\nimport datasets\\nfrom transformers import pipeline\\nfrom transformers.pipelines.pt_utils import KeyDataset\\nfrom tqdm.auto import tqdm\\n\\npipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\\ndataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\\n\\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\\nfor out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\\n    print(out)\\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\\n    # {\"text\": ....}\\n    # ....\\n```\\n\\nFor ease of use, a generator is also possible:\\n\\n\\n```python\\nfrom transformers import pipeline\\n\\npipe = pipeline(\"text-classification\")   \n",
              "6                                                                                                   - [#5840](https://github.com/gradio-app/gradio/pull/5840) [`4e62b8493`](https://github.com/gradio-app/gradio/commit/4e62b8493dfce50bafafe49f1a5deb929d822103) - Ensure websocket polyfill doesn't load if there is already a `global.Webocket` property set.  Thanks [@Jay2theWhy](https://github.com/Jay2theWhy)!\\n- [#5839](https://github.com/gradio-app/gradio/pull/5839) [`b83064da0`](https://github.com/gradio-app/gradio/commit/b83064da0005ca055fc15ee478cf064bf91702a4) - Fix error when scrolling dropdown with scrollbar.  Thanks [@Kit-p](https://github.com/Kit-p)!\\n- [#5822](https://github.com/gradio-app/gradio/pull/5822) [`7b63db271`](https://github.com/gradio-app/gradio/commit/7b63db27161ab538f20cf8523fc04c9c3b604a98) - Convert async methods in the Examples class into normal sync methods.  Thanks [@whitphx](https://github.com/whitphx)!\\n- [#5904](https://github.com/gradio-app/gradio/pull/5904) [`891d42e9b`](https://github.com/gradio-app/gradio/commit/891d42e9baa7ab85ede2a5eadb56c274b0ed2785) - Define Font.__repr__() to be printed in the doc in a readable format.  Thanks [@whitphx](https://github.com/whitphx)!\\n- [#5811](https://github.com/gradio-app/gradio/pull/5811) [`1d5b15a2d`](https://github.com/gradio-app/gradio/commit/1d5b15a2d24387154f2cfb40a36de25b331471d3) - Assert refactor in external.py.  Thanks [@harry-urek](https://github.com/harry-urek)!\\n- [#5827](https://github.com/gradio-app/gradio/pull/5827) [`48e09ee88`](https://github.com/gradio-app/gradio/commit/48e09ee88799efa38a5cc9b1b61e462f72ec6093) - Quick fix: Chatbot change event.  Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\\n- [#5890](https://github.com/gradio-app/gradio/pull/5890) [`c4ba832b3`](https://github.com/gradio-app/gradio/commit/c4ba832b318dad5e8bf565cfa0daf93ca188498f) - Remove deprecation warning from `gr.update` and clean up associated code.  Thanks [@abidlabs](https://github.com/abidlabs)!   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     !--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Multitask Prompt Tuning\\n\\n[Multitask Prompt Tuning](https://huggingface.co/papers/2303.02861)  decomposes the soft prompts of each task into a single learned transferable prompt instead of a separate prompt for each task. The single learned prompt can be adapted for each task by multiplicative low rank updates.\\n\\nThe abstract from the paper is:   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              <p align=\"center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/distill_sd/Picture6.png\" width=500>\\n</p>\\n\\n## Conclusion\\n\\nWe invite the open-source community to help us improve and achieve wider adoption of these distilled SD models. Users can join our [Discord](https://discord.gg/s6E6eHJk) server, where we will be announcing the latest updates to these models, releasing more checkpoints and some exciting new LoRAs. And if you like our work, please give us a star on our [Github](https://github.com/segmind/distill-sd).   \n",
              "9                                                                | Task | Example datasets | Trainer support | 🤗 Accelerate | 🤗 Datasets | Colab\\n|---|---|:---:|:---:|:---:|:---:|\\n| [**`language-modeling`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) | [WikiText-2](https://huggingface.co/datasets/wikitext) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\\n| [**`multiple-choice`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) | [SWAG](https://huggingface.co/datasets/swag) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)\\n| [**`question-answering`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) | [SQuAD](https://huggingface.co/datasets/squad) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)\\n| [**`summarization`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) |  [XSum](https://huggingface.co/datasets/xsum) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)\\n| [**`text-classification`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) | [GLUE](https://huggingface.co/datasets/glue) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)   \n",
              "\n",
              "                                                                                                                                                              question  \\\n",
              "0                                                                                     What is the average loading time for CPU with this library compared to pickle?\\n   \n",
              "1                                                                                 What is the size (number of parameters) of \"llama-2-70b-Guanaco-QLoRA-fp16\" model?\\n   \n",
              "2                                                                What are the two structures introduced in Dense Layer Aggregation (DLA) for deep layer aggregation?\\n   \n",
              "3                                                                                                      What is the shape of the RGB camera in the observation space?\\n   \n",
              "4                                                        Which OpenVINO notebook from Hugging Face provides instructions on exporting a model and running inference?\\n   \n",
              "5                                                                              What is the label of the first text in the pipe(\"This restaurant is awesome\") output?\\n   \n",
              "6  In how many pull requests were font-related changes made between commit hashes \"4e62b8493dfce50bafafe49f1a5deb929d822103\" and \"baa7ab85ede2a5eadb56c274b0ed2785\"?\\n   \n",
              "7                                                                                     In which license is the HuggingFace Multitask Prompt Tuning paper distributed?\\n   \n",
              "8                                                                           In which Discord server can users find the latest updates about the distilled SD models?\\n   \n",
              "9                                                                                  In which GitHub repository are the examples for question-answering tasks located?\\n   \n",
              "\n",
              "                                                                                                                                                                                                    answer  \\\n",
              "0                                                                                                                                   The CPU loading times are faster with this library compared to pickle.   \n",
              "1                                                                                                                                                                                                     3537   \n",
              "2                                                                                                                                 Iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA)   \n",
              "3                                                                                                                                      The RGB camera in the observation space has a shape of (3, 40, 40).   \n",
              "4                                                                                                                                         The notebook named \"optimum_openvino_inference.ipynb\" does this.   \n",
              "5                                                                                                                                                               The label of the first text is 'POSITIVE'.   \n",
              "6                                                                                                                                                 One pull request (#5904) contained font-related changes.   \n",
              "7                                                                                                      The HuggingFace Multitask Prompt Tuning paper is distributed under the Apache License, Version 2.0.   \n",
              "8                                                            Users can find the latest updates about the distilled SD models on the Discord server with the invitation link <https://discord.gg/s6E6eHJk>.   \n",
              "9  The examples for question-answering tasks are located in the GitHub repository: `huggingface/transformers`. Specifically, they can be found in the subdirectory: `examples/pytorch/question-answering`.   \n",
              "\n",
              "                                                                            source_doc  \n",
              "0                                          huggingface/safetensors/blob/main/README.md  \n",
              "1          huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md  \n",
              "2                        huggingface/pytorch-image-models/blob/main/docs/models/dla.md  \n",
              "3                              huggingface/simulate/blob/main/docs/source/howto/rl.mdx  \n",
              "4                                    huggingface/optimum/blob/main/notebooks/README.md  \n",
              "5          huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md  \n",
              "6                                             gradio-app/gradio/blob/main/CHANGELOG.md  \n",
              "7  huggingface/peft/blob/main/docs/source/package_reference/multitask_prompt_tuning.md  \n",
              "8                                        huggingface/blog/blob/main/sd_distillation.md  \n",
              "9                        huggingface/transformers/blob/main/examples/pytorch/README.md  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b99820c4-5289-4dbf-b07d-b56b3706bb4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>- Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\\nmoment.\\n- Order: 'C' or row-major. This seems to have won. We can add that information later if needed.\\n- Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format.\\n\\n### Benefits\\n\\nSince we can invent a new format we can propose additional benefits:\\n\\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\\non the size of the header of 100MB to prevent parsing extremely large JSON.\\n Also when reading the file, there's a guarantee that addresses in the file\\n do not overlap in any way, meaning when you're loading a file you should never\\n exceed the size of the file in memory\\n\\n- Faster load: PyTorch seems to be the fastest file to load out in the major\\nML formats. However, it does seem to have an extra copy on CPU, which we\\ncan bypass in this lib by using `torch.UntypedStorage.from_file`.\\nCurrently, CPU loading times are extremely fast with this lib compared to pickle.\\nGPU loading times are as fast or faster than PyTorch equivalent.\\nLoading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\\nto be faster too somehow (similar behavior in torch pickle)\\n\\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\\nload only part of the tensors on the various models. For\\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s.\\nThis really speeds up feedbacks loops when developing on the model. For instance\\nyou don't have to have separate copies of the weights when changing the distribution\\nstrategy (for instance Pipeline Parallelism vs Tensor Parallelism).\\n\\nLicense: Apache-2.0</td>\n",
              "      <td>What is the average loading time for CPU with this library compared to pickle?\\n</td>\n",
              "      <td>The CPU loading times are faster with this library compared to pickle.</td>\n",
              "      <td>huggingface/safetensors/blob/main/README.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>|      |      |[camembert/camembert-large](https://huggingface.co/camembert/camembert-large)                                                                      |3660        |6       |                         |                                                                                   |[LICENSE](https://huggingface.co/camembert/camembert-large/blob/main/LICENSE)                                           |                                                                                                    |             |\\n|      |      |[stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)                          |3553        |80      |                         |                                                                                   |[LICENSE](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b/blob/main/LICENSE)                     |                                                                                                    |             |\\n|      |      |[TheBloke/llama-2-70b-Guanaco-QLoRA-fp16](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16)                                          |3537        |52      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16/blob/main/LICENSE.txt)                     |                                                                                                    |             |</td>\n",
              "      <td>What is the size (number of parameters) of \"llama-2-70b-Guanaco-QLoRA-fp16\" model?\\n</td>\n",
              "      <td>3537</td>\n",
              "      <td>huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Deep Layer Aggregation\\n\\nExtending  “shallow” skip connections, **Dense Layer Aggregation (DLA)** incorporates more depth and sharing. The authors introduce two structures for deep layer aggregation (DLA): iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA). These structures are expressed through an architectural framework, independent of the choice of backbone, for compatibility with current and future networks. \\n\\nIDA focuses on fusing resolutions and scales while HDA focuses on merging features from all modules and channels. IDA follows the base hierarchy to refine resolution and aggregate scale stage-bystage. HDA assembles its own hierarchy of tree-structured connections that cross and merge stages to aggregate different levels of representation. \\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model('dla102', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert('RGB')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])\\n```</td>\n",
              "      <td>What are the two structures introduced in Dense Layer Aggregation (DLA) for deep layer aggregation?\\n</td>\n",
              "      <td>Iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA)</td>\n",
              "      <td>huggingface/pytorch-image-models/blob/main/docs/models/dla.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Objective: Move the agent so the box is within the agents its field of view\\n\\nActors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera\\n\\nObservation space: \\n- An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format.\\n  \\nAction space:\\n- A discrete action space with 3 possible actions\\n- Turn left 10 degrees\\n- Turn right 10 degrees\\n- Move forward\\n\\nReward function:\\n- A sparse reward for moving the box within a 60 degree fov cone in front of the agent.\\n- A timeout penaly of -1 if the agent does not reach the object in 100 time-steps\\n\\nParallel: 4 independent instances of the same environment configuration.</td>\n",
              "      <td>What is the shape of the RGB camera in the observation space?\\n</td>\n",
              "      <td>The RGB camera in the observation space has a shape of (3, 40, 40).</td>\n",
              "      <td>huggingface/simulate/blob/main/docs/source/howto/rl.mdx</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|\\n| [How to quantize a question answering model with NNCF](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb) | Show how to apply post-training quantization on a question answering model using [NNCF](https://github.com/openvinotoolkit/nncf) and to accelerate inference with OpenVINO| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)|</td>\n",
              "      <td>Which OpenVINO notebook from Hugging Face provides instructions on exporting a model and running inference?\\n</td>\n",
              "      <td>The notebook named \"optimum_openvino_inference.ipynb\" does this.</td>\n",
              "      <td>huggingface/optimum/blob/main/notebooks/README.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Simple call on one item:\\n\\n```python\\n&gt;&gt;&gt; pipe = pipeline(\"text-classification\")\\n&gt;&gt;&gt; pipe(\"This restaurant is awesome\")\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\\n```\\n\\nIf you want to use a specific model from the [hub](https://huggingface.co) you can ignore the task if the model on\\nthe hub already defines it:\\n\\n```python\\n&gt;&gt;&gt; pipe = pipeline(model=\"roberta-large-mnli\")\\n&gt;&gt;&gt; pipe(\"This restaurant is awesome\")\\n[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]\\n```\\n\\nTo call a pipeline on many items, you can call it with a *list*.\\n\\n```python\\n&gt;&gt;&gt; pipe = pipeline(\"text-classification\")\\n&gt;&gt;&gt; pipe([\"This restaurant is awesome\", \"This restaurant is awful\"])\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135},\\n {'label': 'NEGATIVE', 'score': 0.9996669292449951}]\\n```\\n\\nTo iterate over full datasets it is recommended to use a `dataset` directly. This means you don't need to allocate\\nthe whole dataset at once, nor do you need to do batching yourself. This should work just as fast as custom loops on\\nGPU. If it doesn't don't hesitate to create an issue.\\n\\n```python\\nimport datasets\\nfrom transformers import pipeline\\nfrom transformers.pipelines.pt_utils import KeyDataset\\nfrom tqdm.auto import tqdm\\n\\npipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\\ndataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\\n\\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\\nfor out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\\n    print(out)\\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\\n    # {\"text\": ....}\\n    # ....\\n```\\n\\nFor ease of use, a generator is also possible:\\n\\n\\n```python\\nfrom transformers import pipeline\\n\\npipe = pipeline(\"text-classification\")</td>\n",
              "      <td>What is the label of the first text in the pipe(\"This restaurant is awesome\") output?\\n</td>\n",
              "      <td>The label of the first text is 'POSITIVE'.</td>\n",
              "      <td>huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>- [#5840](https://github.com/gradio-app/gradio/pull/5840) [`4e62b8493`](https://github.com/gradio-app/gradio/commit/4e62b8493dfce50bafafe49f1a5deb929d822103) - Ensure websocket polyfill doesn't load if there is already a `global.Webocket` property set.  Thanks [@Jay2theWhy](https://github.com/Jay2theWhy)!\\n- [#5839](https://github.com/gradio-app/gradio/pull/5839) [`b83064da0`](https://github.com/gradio-app/gradio/commit/b83064da0005ca055fc15ee478cf064bf91702a4) - Fix error when scrolling dropdown with scrollbar.  Thanks [@Kit-p](https://github.com/Kit-p)!\\n- [#5822](https://github.com/gradio-app/gradio/pull/5822) [`7b63db271`](https://github.com/gradio-app/gradio/commit/7b63db27161ab538f20cf8523fc04c9c3b604a98) - Convert async methods in the Examples class into normal sync methods.  Thanks [@whitphx](https://github.com/whitphx)!\\n- [#5904](https://github.com/gradio-app/gradio/pull/5904) [`891d42e9b`](https://github.com/gradio-app/gradio/commit/891d42e9baa7ab85ede2a5eadb56c274b0ed2785) - Define Font.__repr__() to be printed in the doc in a readable format.  Thanks [@whitphx](https://github.com/whitphx)!\\n- [#5811](https://github.com/gradio-app/gradio/pull/5811) [`1d5b15a2d`](https://github.com/gradio-app/gradio/commit/1d5b15a2d24387154f2cfb40a36de25b331471d3) - Assert refactor in external.py.  Thanks [@harry-urek](https://github.com/harry-urek)!\\n- [#5827](https://github.com/gradio-app/gradio/pull/5827) [`48e09ee88`](https://github.com/gradio-app/gradio/commit/48e09ee88799efa38a5cc9b1b61e462f72ec6093) - Quick fix: Chatbot change event.  Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\\n- [#5890](https://github.com/gradio-app/gradio/pull/5890) [`c4ba832b3`](https://github.com/gradio-app/gradio/commit/c4ba832b318dad5e8bf565cfa0daf93ca188498f) - Remove deprecation warning from `gr.update` and clean up associated code.  Thanks [@abidlabs](https://github.com/abidlabs)!</td>\n",
              "      <td>In how many pull requests were font-related changes made between commit hashes \"4e62b8493dfce50bafafe49f1a5deb929d822103\" and \"baa7ab85ede2a5eadb56c274b0ed2785\"?\\n</td>\n",
              "      <td>One pull request (#5904) contained font-related changes.</td>\n",
              "      <td>gradio-app/gradio/blob/main/CHANGELOG.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n--&gt;\\n\\n# Multitask Prompt Tuning\\n\\n[Multitask Prompt Tuning](https://huggingface.co/papers/2303.02861)  decomposes the soft prompts of each task into a single learned transferable prompt instead of a separate prompt for each task. The single learned prompt can be adapted for each task by multiplicative low rank updates.\\n\\nThe abstract from the paper is:</td>\n",
              "      <td>In which license is the HuggingFace Multitask Prompt Tuning paper distributed?\\n</td>\n",
              "      <td>The HuggingFace Multitask Prompt Tuning paper is distributed under the Apache License, Version 2.0.</td>\n",
              "      <td>huggingface/peft/blob/main/docs/source/package_reference/multitask_prompt_tuning.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>&lt;p align=\"center\"&gt;\\n    &lt;img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/distill_sd/Picture6.png\" width=500&gt;\\n&lt;/p&gt;\\n\\n## Conclusion\\n\\nWe invite the open-source community to help us improve and achieve wider adoption of these distilled SD models. Users can join our [Discord](https://discord.gg/s6E6eHJk) server, where we will be announcing the latest updates to these models, releasing more checkpoints and some exciting new LoRAs. And if you like our work, please give us a star on our [Github](https://github.com/segmind/distill-sd).</td>\n",
              "      <td>In which Discord server can users find the latest updates about the distilled SD models?\\n</td>\n",
              "      <td>Users can find the latest updates about the distilled SD models on the Discord server with the invitation link &lt;https://discord.gg/s6E6eHJk&gt;.</td>\n",
              "      <td>huggingface/blog/blob/main/sd_distillation.md</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>| Task | Example datasets | Trainer support | 🤗 Accelerate | 🤗 Datasets | Colab\\n|---|---|:---:|:---:|:---:|:---:|\\n| [**`language-modeling`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) | [WikiText-2](https://huggingface.co/datasets/wikitext) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\\n| [**`multiple-choice`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) | [SWAG](https://huggingface.co/datasets/swag) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)\\n| [**`question-answering`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) | [SQuAD](https://huggingface.co/datasets/squad) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)\\n| [**`summarization`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) |  [XSum](https://huggingface.co/datasets/xsum) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)\\n| [**`text-classification`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) | [GLUE](https://huggingface.co/datasets/glue) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)</td>\n",
              "      <td>In which GitHub repository are the examples for question-answering tasks located?\\n</td>\n",
              "      <td>The examples for question-answering tasks are located in the GitHub repository: `huggingface/transformers`. Specifically, they can be found in the subdirectory: `examples/pytorch/question-answering`.</td>\n",
              "      <td>huggingface/transformers/blob/main/examples/pytorch/README.md</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b99820c4-5289-4dbf-b07d-b56b3706bb4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b99820c4-5289-4dbf-b07d-b56b3706bb4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b99820c4-5289-4dbf-b07d-b56b3706bb4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(pd\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"<p align=\\\"center\\\">\\n    <img src=\\\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/distill_sd/Picture6.png\\\" width=500>\\n</p>\\n\\n## Conclusion\\n\\nWe invite the open-source community to help us improve and achieve wider adoption of these distilled SD models. Users can join our [Discord](https://discord.gg/s6E6eHJk) server, where we will be announcing the latest updates to these models, releasing more checkpoints and some exciting new LoRAs. And if you like our work, please give us a star on our [Github](https://github.com/segmind/distill-sd).\",\n          \"|      |      |[camembert/camembert-large](https://huggingface.co/camembert/camembert-large)                                                                      |3660        |6       |                         |                                                                                   |[LICENSE](https://huggingface.co/camembert/camembert-large/blob/main/LICENSE)                                           |                                                                                                    |             |\\n|      |      |[stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)                          |3553        |80      |                         |                                                                                   |[LICENSE](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b/blob/main/LICENSE)                     |                                                                                                    |             |\\n|      |      |[TheBloke/llama-2-70b-Guanaco-QLoRA-fp16](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16)                                          |3537        |52      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16/blob/main/LICENSE.txt)                     |                                                                                                    |             |\",\n          \"Simple call on one item:\\n\\n```python\\n>>> pipe = pipeline(\\\"text-classification\\\")\\n>>> pipe(\\\"This restaurant is awesome\\\")\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\\n```\\n\\nIf you want to use a specific model from the [hub](https://huggingface.co) you can ignore the task if the model on\\nthe hub already defines it:\\n\\n```python\\n>>> pipe = pipeline(model=\\\"roberta-large-mnli\\\")\\n>>> pipe(\\\"This restaurant is awesome\\\")\\n[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]\\n```\\n\\nTo call a pipeline on many items, you can call it with a *list*.\\n\\n```python\\n>>> pipe = pipeline(\\\"text-classification\\\")\\n>>> pipe([\\\"This restaurant is awesome\\\", \\\"This restaurant is awful\\\"])\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135},\\n {'label': 'NEGATIVE', 'score': 0.9996669292449951}]\\n```\\n\\nTo iterate over full datasets it is recommended to use a `dataset` directly. This means you don't need to allocate\\nthe whole dataset at once, nor do you need to do batching yourself. This should work just as fast as custom loops on\\nGPU. If it doesn't don't hesitate to create an issue.\\n\\n```python\\nimport datasets\\nfrom transformers import pipeline\\nfrom transformers.pipelines.pt_utils import KeyDataset\\nfrom tqdm.auto import tqdm\\n\\npipe = pipeline(\\\"automatic-speech-recognition\\\", model=\\\"facebook/wav2vec2-base-960h\\\", device=0)\\ndataset = datasets.load_dataset(\\\"superb\\\", name=\\\"asr\\\", split=\\\"test\\\")\\n\\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\\nfor out in tqdm(pipe(KeyDataset(dataset, \\\"file\\\"))):\\n    print(out)\\n    # {\\\"text\\\": \\\"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\\\"}\\n    # {\\\"text\\\": ....}\\n    # ....\\n```\\n\\nFor ease of use, a generator is also possible:\\n\\n\\n```python\\nfrom transformers import pipeline\\n\\npipe = pipeline(\\\"text-classification\\\")\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"In which Discord server can users find the latest updates about the distilled SD models?\\n\",\n          \"What is the size (number of parameters) of \\\"llama-2-70b-Guanaco-QLoRA-fp16\\\" model?\\n\",\n          \"What is the label of the first text in the pipe(\\\"This restaurant is awesome\\\") output?\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Users can find the latest updates about the distilled SD models on the Discord server with the invitation link <https://discord.gg/s6E6eHJk>.\",\n          \"3537\",\n          \"The label of the first text is 'POSITIVE'.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"huggingface/blog/blob/main/sd_distillation.md\",\n          \"huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md\",\n          \"huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. Setup critique agents\n",
        "\n",
        "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
        "\n",
        "We thus build critique agents that will rate each question on several criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
        "- **Groundedness:** can the question be answered from the given context?\n",
        "- **Relevance:** is the question relevant to users? For instance, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practitioners.\n",
        "\n",
        "One last failure case we've noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`.\n",
        "We also build a critique agent for this criteria:\n",
        "- **Stand-alone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `What is the function used in this article?` for a question generated from a specific blog article.\n",
        "\n",
        "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
        "\n",
        "💡 ___When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.___\n",
        "\n",
        "We now build and run these critique agents."
      ],
      "metadata": {
        "id": "MS2OSOybAcIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ],
      "metadata": {
        "id": "5M1P7-DBWUqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "print(\"Generating critique for each QA couple...\")\n",
        "\n",
        "for output in tqdm(outputs):\n",
        "    critique_inputs = {\n",
        "        \"groundedness\": question_groundedness_critique_prompt.format(\n",
        "            context=output[\"context\"], question=output[\"question\"]\n",
        "        ),\n",
        "        \"relevance\": question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
        "        \"standalone\": question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "    }\n",
        "\n",
        "    for criterion, prompt in critique_inputs.items():\n",
        "        try:\n",
        "            evaluation = call_llm(llm_client, prompt)\n",
        "\n",
        "            score_match = re.search(r\"Total rating:\\s*(\\d+)\", evaluation, re.IGNORECASE)\n",
        "\n",
        "            eval_match = re.search(r\"Evaluation:\\s*(.+?)(?=\\nTotal rating:|$)\", evaluation, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "            if score_match:\n",
        "                score = int(score_match.group(1))\n",
        "            else:\n",
        "                score = None\n",
        "\n",
        "            if eval_match:\n",
        "                explanation = eval_match.group(1).strip()\n",
        "            else:\n",
        "                explanation = evaluation\n",
        "\n",
        "            output.update({\n",
        "                f\"{criterion}_score\": score,\n",
        "                f\"{criterion}_eval\": explanation,\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error critiquing {criterion}: {e}\")\n",
        "            output.update({\n",
        "                f\"{criterion}_score\": None,\n",
        "                f\"{criterion}_eval\": \"Error\",\n",
        "            })\n",
        "if outputs:\n",
        "    print(\"Keys in first item:\", outputs[0].keys())"
      ],
      "metadata": {
        "id": "CQ9Z1_K8UKYC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "67ccfb78e01845b2a309027c84c5d3bc",
            "5263138439cf4b8282abbd7faa34dabb",
            "0b5e114d4c0c481a8646a36a4d3af68a",
            "61d404d2090e455c9b75df3713dbad67",
            "98ea1e2f06da4af8a299c5411c778aff",
            "268485be65a14f979a69e0f3e668faa1",
            "4b4167ac59ce495ea7c439f281d4d9d4",
            "eeabcef351c243459be8aa7d4e4544b5",
            "1677be7041244985a03990f2d60c8b67",
            "16e194846c4e49538403f8669ffbf878",
            "294a478356554300a74bb8f277cd7ff1"
          ]
        },
        "outputId": "e8cc452c-57b3-473e-8534-90152f00cd8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating critique for each QA couple...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67ccfb78e01845b2a309027c84c5d3bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in first item: dict_keys(['context', 'question', 'answer', 'source_doc', 'groundedness_score', 'groundedness_eval', 'relevance_score', 'relevance_eval', 'standalone_score', 'standalone_eval'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(pd.DataFrame(outputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5nqXs0WpFFHJ",
        "outputId": "3a0587a9-ecfe-458b-e7e2-9fa7a56095a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            context  \\\n",
              "0  - Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\\nmoment.\\n- Order: 'C' or row-major. This seems to have won. We can add that information later if needed.\\n- Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format.\\n\\n### Benefits\\n\\nSince we can invent a new format we can propose additional benefits:\\n\\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\\non the size of the header of 100MB to prevent parsing extremely large JSON.\\n Also when reading the file, there's a guarantee that addresses in the file\\n do not overlap in any way, meaning when you're loading a file you should never\\n exceed the size of the file in memory\\n\\n- Faster load: PyTorch seems to be the fastest file to load out in the major\\nML formats. However, it does seem to have an extra copy on CPU, which we\\ncan bypass in this lib by using `torch.UntypedStorage.from_file`.\\nCurrently, CPU loading times are extremely fast with this lib compared to pickle.\\nGPU loading times are as fast or faster than PyTorch equivalent.\\nLoading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\\nto be faster too somehow (similar behavior in torch pickle)\\n\\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\\nload only part of the tensors on the various models. For\\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s.\\nThis really speeds up feedbacks loops when developing on the model. For instance\\nyou don't have to have separate copies of the weights when changing the distribution\\nstrategy (for instance Pipeline Parallelism vs Tensor Parallelism).\\n\\nLicense: Apache-2.0   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |      |      |[camembert/camembert-large](https://huggingface.co/camembert/camembert-large)                                                                      |3660        |6       |                         |                                                                                   |[LICENSE](https://huggingface.co/camembert/camembert-large/blob/main/LICENSE)                                           |                                                                                                    |             |\\n|      |      |[stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)                          |3553        |80      |                         |                                                                                   |[LICENSE](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b/blob/main/LICENSE)                     |                                                                                                    |             |\\n|      |      |[TheBloke/llama-2-70b-Guanaco-QLoRA-fp16](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16)                                          |3537        |52      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16/blob/main/LICENSE.txt)                     |                                                                                                    |             |   \n",
              "2                                                                                                                                                                                                                                                                                            Deep Layer Aggregation\\n\\nExtending  “shallow” skip connections, **Dense Layer Aggregation (DLA)** incorporates more depth and sharing. The authors introduce two structures for deep layer aggregation (DLA): iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA). These structures are expressed through an architectural framework, independent of the choice of backbone, for compatibility with current and future networks. \\n\\nIDA focuses on fusing resolutions and scales while HDA focuses on merging features from all modules and channels. IDA follows the base hierarchy to refine resolution and aggregate scale stage-bystage. HDA assembles its own hierarchy of tree-structured connections that cross and merge stages to aggregate different levels of representation. \\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model('dla102', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert('RGB')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])\\n```   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Objective: Move the agent so the box is within the agents its field of view\\n\\nActors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera\\n\\nObservation space: \\n- An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format.\\n  \\nAction space:\\n- A discrete action space with 3 possible actions\\n- Turn left 10 degrees\\n- Turn right 10 degrees\\n- Move forward\\n\\nReward function:\\n- A sparse reward for moving the box within a 60 degree fov cone in front of the agent.\\n- A timeout penaly of -1 if the agent does not reach the object in 100 time-steps\\n\\nParallel: 4 independent instances of the same environment configuration.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|\\n| [How to quantize a question answering model with NNCF](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb) | Show how to apply post-training quantization on a question answering model using [NNCF](https://github.com/openvinotoolkit/nncf) and to accelerate inference with OpenVINO| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)|   \n",
              "5                                                                         Simple call on one item:\\n\\n```python\\n>>> pipe = pipeline(\"text-classification\")\\n>>> pipe(\"This restaurant is awesome\")\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\\n```\\n\\nIf you want to use a specific model from the [hub](https://huggingface.co) you can ignore the task if the model on\\nthe hub already defines it:\\n\\n```python\\n>>> pipe = pipeline(model=\"roberta-large-mnli\")\\n>>> pipe(\"This restaurant is awesome\")\\n[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]\\n```\\n\\nTo call a pipeline on many items, you can call it with a *list*.\\n\\n```python\\n>>> pipe = pipeline(\"text-classification\")\\n>>> pipe([\"This restaurant is awesome\", \"This restaurant is awful\"])\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135},\\n {'label': 'NEGATIVE', 'score': 0.9996669292449951}]\\n```\\n\\nTo iterate over full datasets it is recommended to use a `dataset` directly. This means you don't need to allocate\\nthe whole dataset at once, nor do you need to do batching yourself. This should work just as fast as custom loops on\\nGPU. If it doesn't don't hesitate to create an issue.\\n\\n```python\\nimport datasets\\nfrom transformers import pipeline\\nfrom transformers.pipelines.pt_utils import KeyDataset\\nfrom tqdm.auto import tqdm\\n\\npipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\\ndataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\\n\\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\\nfor out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\\n    print(out)\\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\\n    # {\"text\": ....}\\n    # ....\\n```\\n\\nFor ease of use, a generator is also possible:\\n\\n\\n```python\\nfrom transformers import pipeline\\n\\npipe = pipeline(\"text-classification\")   \n",
              "6                                                                                                   - [#5840](https://github.com/gradio-app/gradio/pull/5840) [`4e62b8493`](https://github.com/gradio-app/gradio/commit/4e62b8493dfce50bafafe49f1a5deb929d822103) - Ensure websocket polyfill doesn't load if there is already a `global.Webocket` property set.  Thanks [@Jay2theWhy](https://github.com/Jay2theWhy)!\\n- [#5839](https://github.com/gradio-app/gradio/pull/5839) [`b83064da0`](https://github.com/gradio-app/gradio/commit/b83064da0005ca055fc15ee478cf064bf91702a4) - Fix error when scrolling dropdown with scrollbar.  Thanks [@Kit-p](https://github.com/Kit-p)!\\n- [#5822](https://github.com/gradio-app/gradio/pull/5822) [`7b63db271`](https://github.com/gradio-app/gradio/commit/7b63db27161ab538f20cf8523fc04c9c3b604a98) - Convert async methods in the Examples class into normal sync methods.  Thanks [@whitphx](https://github.com/whitphx)!\\n- [#5904](https://github.com/gradio-app/gradio/pull/5904) [`891d42e9b`](https://github.com/gradio-app/gradio/commit/891d42e9baa7ab85ede2a5eadb56c274b0ed2785) - Define Font.__repr__() to be printed in the doc in a readable format.  Thanks [@whitphx](https://github.com/whitphx)!\\n- [#5811](https://github.com/gradio-app/gradio/pull/5811) [`1d5b15a2d`](https://github.com/gradio-app/gradio/commit/1d5b15a2d24387154f2cfb40a36de25b331471d3) - Assert refactor in external.py.  Thanks [@harry-urek](https://github.com/harry-urek)!\\n- [#5827](https://github.com/gradio-app/gradio/pull/5827) [`48e09ee88`](https://github.com/gradio-app/gradio/commit/48e09ee88799efa38a5cc9b1b61e462f72ec6093) - Quick fix: Chatbot change event.  Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\\n- [#5890](https://github.com/gradio-app/gradio/pull/5890) [`c4ba832b3`](https://github.com/gradio-app/gradio/commit/c4ba832b318dad5e8bf565cfa0daf93ca188498f) - Remove deprecation warning from `gr.update` and clean up associated code.  Thanks [@abidlabs](https://github.com/abidlabs)!   \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     !--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n-->\\n\\n# Multitask Prompt Tuning\\n\\n[Multitask Prompt Tuning](https://huggingface.co/papers/2303.02861)  decomposes the soft prompts of each task into a single learned transferable prompt instead of a separate prompt for each task. The single learned prompt can be adapted for each task by multiplicative low rank updates.\\n\\nThe abstract from the paper is:   \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              <p align=\"center\">\\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/distill_sd/Picture6.png\" width=500>\\n</p>\\n\\n## Conclusion\\n\\nWe invite the open-source community to help us improve and achieve wider adoption of these distilled SD models. Users can join our [Discord](https://discord.gg/s6E6eHJk) server, where we will be announcing the latest updates to these models, releasing more checkpoints and some exciting new LoRAs. And if you like our work, please give us a star on our [Github](https://github.com/segmind/distill-sd).   \n",
              "9                                                                | Task | Example datasets | Trainer support | 🤗 Accelerate | 🤗 Datasets | Colab\\n|---|---|:---:|:---:|:---:|:---:|\\n| [**`language-modeling`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) | [WikiText-2](https://huggingface.co/datasets/wikitext) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\\n| [**`multiple-choice`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) | [SWAG](https://huggingface.co/datasets/swag) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)\\n| [**`question-answering`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) | [SQuAD](https://huggingface.co/datasets/squad) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)\\n| [**`summarization`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) |  [XSum](https://huggingface.co/datasets/xsum) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)\\n| [**`text-classification`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) | [GLUE](https://huggingface.co/datasets/glue) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)   \n",
              "\n",
              "                                                                                                                                                              question  \\\n",
              "0                                                                                     What is the average loading time for CPU with this library compared to pickle?\\n   \n",
              "1                                                                                 What is the size (number of parameters) of \"llama-2-70b-Guanaco-QLoRA-fp16\" model?\\n   \n",
              "2                                                                What are the two structures introduced in Dense Layer Aggregation (DLA) for deep layer aggregation?\\n   \n",
              "3                                                                                                      What is the shape of the RGB camera in the observation space?\\n   \n",
              "4                                                        Which OpenVINO notebook from Hugging Face provides instructions on exporting a model and running inference?\\n   \n",
              "5                                                                              What is the label of the first text in the pipe(\"This restaurant is awesome\") output?\\n   \n",
              "6  In how many pull requests were font-related changes made between commit hashes \"4e62b8493dfce50bafafe49f1a5deb929d822103\" and \"baa7ab85ede2a5eadb56c274b0ed2785\"?\\n   \n",
              "7                                                                                     In which license is the HuggingFace Multitask Prompt Tuning paper distributed?\\n   \n",
              "8                                                                           In which Discord server can users find the latest updates about the distilled SD models?\\n   \n",
              "9                                                                                  In which GitHub repository are the examples for question-answering tasks located?\\n   \n",
              "\n",
              "                                                                                                                                                                                                    answer  \\\n",
              "0                                                                                                                                   The CPU loading times are faster with this library compared to pickle.   \n",
              "1                                                                                                                                                                                                     3537   \n",
              "2                                                                                                                                 Iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA)   \n",
              "3                                                                                                                                      The RGB camera in the observation space has a shape of (3, 40, 40).   \n",
              "4                                                                                                                                         The notebook named \"optimum_openvino_inference.ipynb\" does this.   \n",
              "5                                                                                                                                                               The label of the first text is 'POSITIVE'.   \n",
              "6                                                                                                                                                 One pull request (#5904) contained font-related changes.   \n",
              "7                                                                                                      The HuggingFace Multitask Prompt Tuning paper is distributed under the Apache License, Version 2.0.   \n",
              "8                                                            Users can find the latest updates about the distilled SD models on the Discord server with the invitation link <https://discord.gg/s6E6eHJk>.   \n",
              "9  The examples for question-answering tasks are located in the GitHub repository: `huggingface/transformers`. Specifically, they can be found in the subdirectory: `examples/pytorch/question-answering`.   \n",
              "\n",
              "                                                                            source_doc  \\\n",
              "0                                          huggingface/safetensors/blob/main/README.md   \n",
              "1          huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md   \n",
              "2                        huggingface/pytorch-image-models/blob/main/docs/models/dla.md   \n",
              "3                              huggingface/simulate/blob/main/docs/source/howto/rl.mdx   \n",
              "4                                    huggingface/optimum/blob/main/notebooks/README.md   \n",
              "5          huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md   \n",
              "6                                             gradio-app/gradio/blob/main/CHANGELOG.md   \n",
              "7  huggingface/peft/blob/main/docs/source/package_reference/multitask_prompt_tuning.md   \n",
              "8                                        huggingface/blog/blob/main/sd_distillation.md   \n",
              "9                        huggingface/transformers/blob/main/examples/pytorch/README.md   \n",
              "\n",
              "   groundedness_score  \\\n",
              "0                   1   \n",
              "1                   5   \n",
              "2                   5   \n",
              "3                   1   \n",
              "4                   5   \n",
              "5                   5   \n",
              "6                   1   \n",
              "7                   1   \n",
              "8                   5   \n",
              "9                   5   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                        groundedness_eval  \\\n",
              "0                                                                            The context mentions the benefits of the new library in terms of preventing DOS attacks, faster load times on CPU compared to pickle, and lazy loading. However, it does not provide any specific numerical data or detailed comparison between the average loading times of the library and pickle for CPU.   \n",
              "1  The context provides the number of parameters for the given model under the \"Size (MB)\" column, but no clear indication of the total number of parameters. However, we can find the number of parameters by multiplying the size (in MB) by 1024 and then dividing by 1,000,000. This calculation can be performed as follows: (3537 / 1024) * 1024 * 1024 = 3,587,251,840 parameters.   \n",
              "2                                                                                                                                                 The context clearly states that the two structures introduced in DLA for deep layer aggregation are iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA. The context also explains the functions of each structure.   \n",
              "3                         The context does not provide any information about the shape of the RGB camera in the observation space from the perspective of the observation space itself. It only describes the shape of the observation space as an RGB image of size (3, 40, 40), but it doesn't provide any information about the physical shape of the camera in the observation space.   \n",
              "4                                                                                                                           The context provides two OpenVINO notebooks from Hugging Face. The first notebook, \"optimum\\_openvino\\_inference.ipynb,\" explains how to export a model to OpenVINO and run inference. Therefore, the question can be answered by referring to this notebook.   \n",
              "5                                                                                                  The context provides the output of a text classification pipeline, which includes the label 'POSITIVE' and a high score for the given input \"This restaurant is awesome\". There is no ambiguity in the context that the label for the first text in the pipeline output is 'POSITIVE'.   \n",
              "6                                                                                                                                                                                                                The context does not provide sufficient information to answer the question as it does not mention any specific changes related to fonts between the given commit hashes.   \n",
              "7                                                                                                                          The context does not explicitly mention the license of the paper \"Multitask Prompt Tuning\" itself, only the license of the context file where the paper is hosted. Therefore, it is not possible to unambiguously determine the answer from the context alone.   \n",
              "8                                                                                                                                                                                                                                         The context explicitly states that users can find the latest updates about the distilled SD models on the Discord server mentioned in the link.   \n",
              "9                                                                                              Given the context, the question \"In which GitHub repository are the examples for question-answering tasks located?\" can be answered unambiguously as the answer is provided in the context as \"https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering\".   \n",
              "\n",
              "   relevance_score  \\\n",
              "0                1   \n",
              "1                5   \n",
              "2                4   \n",
              "3                1   \n",
              "4                5   \n",
              "5                1   \n",
              "6                1   \n",
              "7                5   \n",
              "8                1   \n",
              "9                4   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                              relevance_eval  \\\n",
              "0  This question is not directly related to NLP applications or the Hugging Face ecosystem as it compares loading times of different serialization libraries (Transformers library vs pickle) for general Python data, rather than specifically focusing on NLP tasks or Hugging Face components. Furthermore, loading times for CPU and other factors like model size can highly influence the results, making it difficult to provide a definitive answer without knowing these specifics.   \n",
              "1                                                                                                                                                                                                                             This question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem as they often need to know the size of their chosen models to make informed decisions regarding system requirements and resource allocation.   \n",
              "2                                            DLA is a deep neural architecture proposed for object detection, which aggregates deep feature representations in a hierarchical and densely connected manner. Understanding the structures employed in DLA for deep layer aggregation can be crucial for machine learning developers working on NLP applications using Hugging Face's ecosystem, as NLP models may benefit from similar hierarchical and dense feature representation methods.   \n",
              "3                                                                                                                        This question is not useful for machine learning developers building NLP applications with the Hugging Face ecosystem as it is not related to natural language processing or the Hugging Face library specifically. It appears to be asking about the shape of an RGB camera in a computer vision context, rather than anything to do with machine learning or NLP.   \n",
              "4                                                                                         The question is relevant to machine learning developers building NLP applications with the Hugging Face ecosystem as it asks about a specific task: exporting a model and running inference using Hugging Face's OpenVINO toolkit. OpenVINO is a popular framework for running inference on mobile and embedded devices, and exporting models is an essential step in the ML development workflow.   \n",
              "5                                                                                                                                                                                                                         This question assumes knowledge of a specific output format from Hugging Face models, which is not provided in the context. It also does not specify which model or dataset is being used. These details are important for determining the label of a text output.   \n",
              "6                                                                                                                                                                                                                                                       This question is not directly related to machine learning or NLP applications using the Hugging Face ecosystem. It pertains to Git history and font changes, which is an unrelated concern for developers building NLP applications.   \n",
              "7                                                                       This question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem as it provides important information about the licensing of the research paper that discusses a key feature of the Hugging Face library. Understanding the licensing terms can help developers determine how they can use, modify, and distribute the information and code discussed in the paper.   \n",
              "8                                                                                                                                                                                                                                         This question is not directly related to building NLP applications using the Hugging Face ecosystem. It asks about a specific Discord server, which does not provide any relevant information for developers working on machine learning projects.   \n",
              "9                                                                                                                                                               This question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem as it provides information about the location of specific examples related to question-answering tasks. Having this information can help developers quickly access and learn from the available resources.   \n",
              "\n",
              "   standalone_score  \\\n",
              "0                 5   \n",
              "1                 5   \n",
              "2                 3   \n",
              "3                 5   \n",
              "4                 5   \n",
              "5                 1   \n",
              "6                 5   \n",
              "7                 5   \n",
              "8                 1   \n",
              "9                 5   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                standalone_eval  \n",
              "0                                                                                                                                                                This question can be understood without additional context, but it relies on the assumption that the reader is familiar with the concepts of \"loading time\" and \"comparison between library X and pickle\". However, these concepts are commonly used in data processing and machine learning fields, and they do not depend on a specific context or document.  \n",
              "1                                                                                        The question refers to a specific model named \"llama-2-70b-Guanaco-QLoRA-fp16\". The model name itself provides sufficient context to understand what is being asked, as long as the reader is familiar with the naming conventions used in the model community. The size or number of parameters of a model is a common piece of information that is often provided in the model documentation, making this a self-contained question.  \n",
              "2  The question refers to Dense Layer Aggregation (DLA), which is a specific deep learning architecture used for feature aggregation in deep neural networks. The question asks about two specific structures introduced in DLA for deep layer aggregation. To understand the question, one needs to have a basic understanding of DLA, but the explicit context of the question is sufficient to comprehend it. Therefore, the question is more context-dependent than independent, but still clear enough to be rated as a 3.  \n",
              "3                                                                                                                                                                                                                                                                        This question refers to the shape of an RGB camera in the observation space, which is a common concept in computer vision and robotics. However, the question does not include any context or specific reference to a particular RGB camera or system.  \n",
              "4                                                                                                                                                                                                    This question refers to specific OpenVINO notebooks available on Hugging Face, asking for one that provides instructions on exporting a model and running inference. The question assumes that the reader has some familiarity with OpenVINO and Hugging Face, but it does not require any additional context beyond that.  \n",
              "5                                                                                                                                                               The question refers to the label of the first text in the output of the pipe function, but it does not specify which pipe function or model is being used. In order to fully understand the question and provide an accurate answer, one would need to have additional context or information about which specific pipe function or model is being referred to.  \n",
              "6                                                                                                                                                                                                                                                                   This question refers to specific commit hashes and asks for information that can be obtained by examining the git history. However, it does not depend on any context outside of that, assuming that the operator has access to the repository in question.  \n",
              "7                                                                                                                                                                                                                                     This question refers to a specific document, namely the HuggingFace Multitask Prompt Tuning paper. However, it does not require any context-specific information beyond the title of the paper and the organization, Hugging Face, which is well-known in the machine learning community.  \n",
              "8                                                                                                                                                                                                                                                                                                               This question assumes the existence of Discord servers related to the distilled SD models. Therefore, to fully understand the question, some context, such as the name or access to these servers, is required.  \n",
              "9                                                                                                                                                                                                                                                                                                     This question is not dependent on any specific context and can be understood independently. The question refers to a GitHub repository, which is a common term used in software development and version control projects.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-badb0082-0300-48c5-b55a-cc56d8a3d63a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source_doc</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>groundedness_eval</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>relevance_eval</th>\n",
              "      <th>standalone_score</th>\n",
              "      <th>standalone_eval</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>- Endianness: Little-endian. This can be modified later, but it feels really unnecessary at the\\nmoment.\\n- Order: 'C' or row-major. This seems to have won. We can add that information later if needed.\\n- Stride: No striding, all tensors need to be packed before being serialized. I have yet to see a case where it seems useful to have a strided tensor stored in serialized format.\\n\\n### Benefits\\n\\nSince we can invent a new format we can propose additional benefits:\\n\\n- Prevent DOS attacks: We can craft the format in such a way that it's almost\\nimpossible to use malicious files to DOS attack a user. Currently, there's a limit\\non the size of the header of 100MB to prevent parsing extremely large JSON.\\n Also when reading the file, there's a guarantee that addresses in the file\\n do not overlap in any way, meaning when you're loading a file you should never\\n exceed the size of the file in memory\\n\\n- Faster load: PyTorch seems to be the fastest file to load out in the major\\nML formats. However, it does seem to have an extra copy on CPU, which we\\ncan bypass in this lib by using `torch.UntypedStorage.from_file`.\\nCurrently, CPU loading times are extremely fast with this lib compared to pickle.\\nGPU loading times are as fast or faster than PyTorch equivalent.\\nLoading first on CPU with memmapping with torch, and then moving all tensors to GPU seems\\nto be faster too somehow (similar behavior in torch pickle)\\n\\n- Lazy loading: in distributed (multi-node or multi-gpu) settings, it's nice to be able to\\nload only part of the tensors on the various models. For\\n[BLOOM](https://huggingface.co/bigscience/bloom) using this format enabled\\nto load the model on 8 GPUs from 10mn with regular PyTorch weights down to 45s.\\nThis really speeds up feedbacks loops when developing on the model. For instance\\nyou don't have to have separate copies of the weights when changing the distribution\\nstrategy (for instance Pipeline Parallelism vs Tensor Parallelism).\\n\\nLicense: Apache-2.0</td>\n",
              "      <td>What is the average loading time for CPU with this library compared to pickle?\\n</td>\n",
              "      <td>The CPU loading times are faster with this library compared to pickle.</td>\n",
              "      <td>huggingface/safetensors/blob/main/README.md</td>\n",
              "      <td>1</td>\n",
              "      <td>The context mentions the benefits of the new library in terms of preventing DOS attacks, faster load times on CPU compared to pickle, and lazy loading. However, it does not provide any specific numerical data or detailed comparison between the average loading times of the library and pickle for CPU.</td>\n",
              "      <td>1</td>\n",
              "      <td>This question is not directly related to NLP applications or the Hugging Face ecosystem as it compares loading times of different serialization libraries (Transformers library vs pickle) for general Python data, rather than specifically focusing on NLP tasks or Hugging Face components. Furthermore, loading times for CPU and other factors like model size can highly influence the results, making it difficult to provide a definitive answer without knowing these specifics.</td>\n",
              "      <td>5</td>\n",
              "      <td>This question can be understood without additional context, but it relies on the assumption that the reader is familiar with the concepts of \"loading time\" and \"comparison between library X and pickle\". However, these concepts are commonly used in data processing and machine learning fields, and they do not depend on a specific context or document.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>|      |      |[camembert/camembert-large](https://huggingface.co/camembert/camembert-large)                                                                      |3660        |6       |                         |                                                                                   |[LICENSE](https://huggingface.co/camembert/camembert-large/blob/main/LICENSE)                                           |                                                                                                    |             |\\n|      |      |[stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)                          |3553        |80      |                         |                                                                                   |[LICENSE](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b/blob/main/LICENSE)                     |                                                                                                    |             |\\n|      |      |[TheBloke/llama-2-70b-Guanaco-QLoRA-fp16](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16)                                          |3537        |52      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16/blob/main/LICENSE.txt)                     |                                                                                                    |             |</td>\n",
              "      <td>What is the size (number of parameters) of \"llama-2-70b-Guanaco-QLoRA-fp16\" model?\\n</td>\n",
              "      <td>3537</td>\n",
              "      <td>huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md</td>\n",
              "      <td>5</td>\n",
              "      <td>The context provides the number of parameters for the given model under the \"Size (MB)\" column, but no clear indication of the total number of parameters. However, we can find the number of parameters by multiplying the size (in MB) by 1024 and then dividing by 1,000,000. This calculation can be performed as follows: (3537 / 1024) * 1024 * 1024 = 3,587,251,840 parameters.</td>\n",
              "      <td>5</td>\n",
              "      <td>This question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem as they often need to know the size of their chosen models to make informed decisions regarding system requirements and resource allocation.</td>\n",
              "      <td>5</td>\n",
              "      <td>The question refers to a specific model named \"llama-2-70b-Guanaco-QLoRA-fp16\". The model name itself provides sufficient context to understand what is being asked, as long as the reader is familiar with the naming conventions used in the model community. The size or number of parameters of a model is a common piece of information that is often provided in the model documentation, making this a self-contained question.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Deep Layer Aggregation\\n\\nExtending  “shallow” skip connections, **Dense Layer Aggregation (DLA)** incorporates more depth and sharing. The authors introduce two structures for deep layer aggregation (DLA): iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA). These structures are expressed through an architectural framework, independent of the choice of backbone, for compatibility with current and future networks. \\n\\nIDA focuses on fusing resolutions and scales while HDA focuses on merging features from all modules and channels. IDA follows the base hierarchy to refine resolution and aggregate scale stage-bystage. HDA assembles its own hierarchy of tree-structured connections that cross and merge stages to aggregate different levels of representation. \\n\\n## How do I use this model on an image?\\nTo load a pretrained model:\\n\\n```python\\nimport timm\\nmodel = timm.create_model('dla102', pretrained=True)\\nmodel.eval()\\n```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data import resolve_data_config\\nfrom timm.data.transforms_factory import create_transform\\n\\nconfig = resolve_data_config({}, model=model)\\ntransform = create_transform(**config)\\n\\nurl, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\\nurllib.request.urlretrieve(url, filename)\\nimg = Image.open(filename).convert('RGB')\\ntensor = transform(img).unsqueeze(0) # transform and add batch dimension\\n```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tensor)\\nprobabilities = torch.nn.functional.softmax(out[0], dim=0)\\nprint(probabilities.shape)\\n# prints: torch.Size([1000])\\n```</td>\n",
              "      <td>What are the two structures introduced in Dense Layer Aggregation (DLA) for deep layer aggregation?\\n</td>\n",
              "      <td>Iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA)</td>\n",
              "      <td>huggingface/pytorch-image-models/blob/main/docs/models/dla.md</td>\n",
              "      <td>5</td>\n",
              "      <td>The context clearly states that the two structures introduced in DLA for deep layer aggregation are iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA. The context also explains the functions of each structure.</td>\n",
              "      <td>4</td>\n",
              "      <td>DLA is a deep neural architecture proposed for object detection, which aggregates deep feature representations in a hierarchical and densely connected manner. Understanding the structures employed in DLA for deep layer aggregation can be crucial for machine learning developers working on NLP applications using Hugging Face's ecosystem, as NLP models may benefit from similar hierarchical and dense feature representation methods.</td>\n",
              "      <td>3</td>\n",
              "      <td>The question refers to Dense Layer Aggregation (DLA), which is a specific deep learning architecture used for feature aggregation in deep neural networks. The question asks about two specific structures introduced in DLA for deep layer aggregation. To understand the question, one needs to have a basic understanding of DLA, but the explicit context of the question is sufficient to comprehend it. Therefore, the question is more context-dependent than independent, but still clear enough to be rated as a 3.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Objective: Move the agent so the box is within the agents its field of view\\n\\nActors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera\\n\\nObservation space: \\n- An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format.\\n  \\nAction space:\\n- A discrete action space with 3 possible actions\\n- Turn left 10 degrees\\n- Turn right 10 degrees\\n- Move forward\\n\\nReward function:\\n- A sparse reward for moving the box within a 60 degree fov cone in front of the agent.\\n- A timeout penaly of -1 if the agent does not reach the object in 100 time-steps\\n\\nParallel: 4 independent instances of the same environment configuration.</td>\n",
              "      <td>What is the shape of the RGB camera in the observation space?\\n</td>\n",
              "      <td>The RGB camera in the observation space has a shape of (3, 40, 40).</td>\n",
              "      <td>huggingface/simulate/blob/main/docs/source/howto/rl.mdx</td>\n",
              "      <td>1</td>\n",
              "      <td>The context does not provide any information about the shape of the RGB camera in the observation space from the perspective of the observation space itself. It only describes the shape of the observation space as an RGB image of size (3, 40, 40), but it doesn't provide any information about the physical shape of the camera in the observation space.</td>\n",
              "      <td>1</td>\n",
              "      <td>This question is not useful for machine learning developers building NLP applications with the Hugging Face ecosystem as it is not related to natural language processing or the Hugging Face library specifically. It appears to be asking about the shape of an RGB camera in a computer vision context, rather than anything to do with machine learning or NLP.</td>\n",
              "      <td>5</td>\n",
              "      <td>This question refers to the shape of an RGB camera in the observation space, which is a common concept in computer vision and robotics. However, the question does not include any context or specific reference to a particular RGB camera or system.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>| [How to run inference with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb) | Explains how to export your model to OpenVINO and run inference with OpenVINO Runtime on various tasks| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/optimum_openvino_inference.ipynb)|\\n| [How to quantize a question answering model with NNCF](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb) | Show how to apply post-training quantization on a question answering model using [NNCF](https://github.com/openvinotoolkit/nncf) and to accelerate inference with OpenVINO| [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)| [![Open in AWS Studio](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)|</td>\n",
              "      <td>Which OpenVINO notebook from Hugging Face provides instructions on exporting a model and running inference?\\n</td>\n",
              "      <td>The notebook named \"optimum_openvino_inference.ipynb\" does this.</td>\n",
              "      <td>huggingface/optimum/blob/main/notebooks/README.md</td>\n",
              "      <td>5</td>\n",
              "      <td>The context provides two OpenVINO notebooks from Hugging Face. The first notebook, \"optimum\\_openvino\\_inference.ipynb,\" explains how to export a model to OpenVINO and run inference. Therefore, the question can be answered by referring to this notebook.</td>\n",
              "      <td>5</td>\n",
              "      <td>The question is relevant to machine learning developers building NLP applications with the Hugging Face ecosystem as it asks about a specific task: exporting a model and running inference using Hugging Face's OpenVINO toolkit. OpenVINO is a popular framework for running inference on mobile and embedded devices, and exporting models is an essential step in the ML development workflow.</td>\n",
              "      <td>5</td>\n",
              "      <td>This question refers to specific OpenVINO notebooks available on Hugging Face, asking for one that provides instructions on exporting a model and running inference. The question assumes that the reader has some familiarity with OpenVINO and Hugging Face, but it does not require any additional context beyond that.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Simple call on one item:\\n\\n```python\\n&gt;&gt;&gt; pipe = pipeline(\"text-classification\")\\n&gt;&gt;&gt; pipe(\"This restaurant is awesome\")\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\\n```\\n\\nIf you want to use a specific model from the [hub](https://huggingface.co) you can ignore the task if the model on\\nthe hub already defines it:\\n\\n```python\\n&gt;&gt;&gt; pipe = pipeline(model=\"roberta-large-mnli\")\\n&gt;&gt;&gt; pipe(\"This restaurant is awesome\")\\n[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]\\n```\\n\\nTo call a pipeline on many items, you can call it with a *list*.\\n\\n```python\\n&gt;&gt;&gt; pipe = pipeline(\"text-classification\")\\n&gt;&gt;&gt; pipe([\"This restaurant is awesome\", \"This restaurant is awful\"])\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135},\\n {'label': 'NEGATIVE', 'score': 0.9996669292449951}]\\n```\\n\\nTo iterate over full datasets it is recommended to use a `dataset` directly. This means you don't need to allocate\\nthe whole dataset at once, nor do you need to do batching yourself. This should work just as fast as custom loops on\\nGPU. If it doesn't don't hesitate to create an issue.\\n\\n```python\\nimport datasets\\nfrom transformers import pipeline\\nfrom transformers.pipelines.pt_utils import KeyDataset\\nfrom tqdm.auto import tqdm\\n\\npipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\\ndataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\\n\\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\\nfor out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\\n    print(out)\\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\\n    # {\"text\": ....}\\n    # ....\\n```\\n\\nFor ease of use, a generator is also possible:\\n\\n\\n```python\\nfrom transformers import pipeline\\n\\npipe = pipeline(\"text-classification\")</td>\n",
              "      <td>What is the label of the first text in the pipe(\"This restaurant is awesome\") output?\\n</td>\n",
              "      <td>The label of the first text is 'POSITIVE'.</td>\n",
              "      <td>huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md</td>\n",
              "      <td>5</td>\n",
              "      <td>The context provides the output of a text classification pipeline, which includes the label 'POSITIVE' and a high score for the given input \"This restaurant is awesome\". There is no ambiguity in the context that the label for the first text in the pipeline output is 'POSITIVE'.</td>\n",
              "      <td>1</td>\n",
              "      <td>This question assumes knowledge of a specific output format from Hugging Face models, which is not provided in the context. It also does not specify which model or dataset is being used. These details are important for determining the label of a text output.</td>\n",
              "      <td>1</td>\n",
              "      <td>The question refers to the label of the first text in the output of the pipe function, but it does not specify which pipe function or model is being used. In order to fully understand the question and provide an accurate answer, one would need to have additional context or information about which specific pipe function or model is being referred to.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>- [#5840](https://github.com/gradio-app/gradio/pull/5840) [`4e62b8493`](https://github.com/gradio-app/gradio/commit/4e62b8493dfce50bafafe49f1a5deb929d822103) - Ensure websocket polyfill doesn't load if there is already a `global.Webocket` property set.  Thanks [@Jay2theWhy](https://github.com/Jay2theWhy)!\\n- [#5839](https://github.com/gradio-app/gradio/pull/5839) [`b83064da0`](https://github.com/gradio-app/gradio/commit/b83064da0005ca055fc15ee478cf064bf91702a4) - Fix error when scrolling dropdown with scrollbar.  Thanks [@Kit-p](https://github.com/Kit-p)!\\n- [#5822](https://github.com/gradio-app/gradio/pull/5822) [`7b63db271`](https://github.com/gradio-app/gradio/commit/7b63db27161ab538f20cf8523fc04c9c3b604a98) - Convert async methods in the Examples class into normal sync methods.  Thanks [@whitphx](https://github.com/whitphx)!\\n- [#5904](https://github.com/gradio-app/gradio/pull/5904) [`891d42e9b`](https://github.com/gradio-app/gradio/commit/891d42e9baa7ab85ede2a5eadb56c274b0ed2785) - Define Font.__repr__() to be printed in the doc in a readable format.  Thanks [@whitphx](https://github.com/whitphx)!\\n- [#5811](https://github.com/gradio-app/gradio/pull/5811) [`1d5b15a2d`](https://github.com/gradio-app/gradio/commit/1d5b15a2d24387154f2cfb40a36de25b331471d3) - Assert refactor in external.py.  Thanks [@harry-urek](https://github.com/harry-urek)!\\n- [#5827](https://github.com/gradio-app/gradio/pull/5827) [`48e09ee88`](https://github.com/gradio-app/gradio/commit/48e09ee88799efa38a5cc9b1b61e462f72ec6093) - Quick fix: Chatbot change event.  Thanks [@dawoodkhan82](https://github.com/dawoodkhan82)!\\n- [#5890](https://github.com/gradio-app/gradio/pull/5890) [`c4ba832b3`](https://github.com/gradio-app/gradio/commit/c4ba832b318dad5e8bf565cfa0daf93ca188498f) - Remove deprecation warning from `gr.update` and clean up associated code.  Thanks [@abidlabs](https://github.com/abidlabs)!</td>\n",
              "      <td>In how many pull requests were font-related changes made between commit hashes \"4e62b8493dfce50bafafe49f1a5deb929d822103\" and \"baa7ab85ede2a5eadb56c274b0ed2785\"?\\n</td>\n",
              "      <td>One pull request (#5904) contained font-related changes.</td>\n",
              "      <td>gradio-app/gradio/blob/main/CHANGELOG.md</td>\n",
              "      <td>1</td>\n",
              "      <td>The context does not provide sufficient information to answer the question as it does not mention any specific changes related to fonts between the given commit hashes.</td>\n",
              "      <td>1</td>\n",
              "      <td>This question is not directly related to machine learning or NLP applications using the Hugging Face ecosystem. It pertains to Git history and font changes, which is an unrelated concern for developers building NLP applications.</td>\n",
              "      <td>5</td>\n",
              "      <td>This question refers to specific commit hashes and asks for information that can be obtained by examining the git history. However, it does not depend on any context outside of that, assuming that the operator has access to the repository in question.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\\nthe License. You may obtain a copy of the License at\\n\\nhttp://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\\nspecific language governing permissions and limitations under the License.\\n\\n⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n\\n--&gt;\\n\\n# Multitask Prompt Tuning\\n\\n[Multitask Prompt Tuning](https://huggingface.co/papers/2303.02861)  decomposes the soft prompts of each task into a single learned transferable prompt instead of a separate prompt for each task. The single learned prompt can be adapted for each task by multiplicative low rank updates.\\n\\nThe abstract from the paper is:</td>\n",
              "      <td>In which license is the HuggingFace Multitask Prompt Tuning paper distributed?\\n</td>\n",
              "      <td>The HuggingFace Multitask Prompt Tuning paper is distributed under the Apache License, Version 2.0.</td>\n",
              "      <td>huggingface/peft/blob/main/docs/source/package_reference/multitask_prompt_tuning.md</td>\n",
              "      <td>1</td>\n",
              "      <td>The context does not explicitly mention the license of the paper \"Multitask Prompt Tuning\" itself, only the license of the context file where the paper is hosted. Therefore, it is not possible to unambiguously determine the answer from the context alone.</td>\n",
              "      <td>5</td>\n",
              "      <td>This question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem as it provides important information about the licensing of the research paper that discusses a key feature of the Hugging Face library. Understanding the licensing terms can help developers determine how they can use, modify, and distribute the information and code discussed in the paper.</td>\n",
              "      <td>5</td>\n",
              "      <td>This question refers to a specific document, namely the HuggingFace Multitask Prompt Tuning paper. However, it does not require any context-specific information beyond the title of the paper and the organization, Hugging Face, which is well-known in the machine learning community.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>&lt;p align=\"center\"&gt;\\n    &lt;img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/distill_sd/Picture6.png\" width=500&gt;\\n&lt;/p&gt;\\n\\n## Conclusion\\n\\nWe invite the open-source community to help us improve and achieve wider adoption of these distilled SD models. Users can join our [Discord](https://discord.gg/s6E6eHJk) server, where we will be announcing the latest updates to these models, releasing more checkpoints and some exciting new LoRAs. And if you like our work, please give us a star on our [Github](https://github.com/segmind/distill-sd).</td>\n",
              "      <td>In which Discord server can users find the latest updates about the distilled SD models?\\n</td>\n",
              "      <td>Users can find the latest updates about the distilled SD models on the Discord server with the invitation link &lt;https://discord.gg/s6E6eHJk&gt;.</td>\n",
              "      <td>huggingface/blog/blob/main/sd_distillation.md</td>\n",
              "      <td>5</td>\n",
              "      <td>The context explicitly states that users can find the latest updates about the distilled SD models on the Discord server mentioned in the link.</td>\n",
              "      <td>1</td>\n",
              "      <td>This question is not directly related to building NLP applications using the Hugging Face ecosystem. It asks about a specific Discord server, which does not provide any relevant information for developers working on machine learning projects.</td>\n",
              "      <td>1</td>\n",
              "      <td>This question assumes the existence of Discord servers related to the distilled SD models. Therefore, to fully understand the question, some context, such as the name or access to these servers, is required.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>| Task | Example datasets | Trainer support | 🤗 Accelerate | 🤗 Datasets | Colab\\n|---|---|:---:|:---:|:---:|:---:|\\n| [**`language-modeling`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling) | [WikiText-2](https://huggingface.co/datasets/wikitext) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\\n| [**`multiple-choice`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) | [SWAG](https://huggingface.co/datasets/swag) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb)\\n| [**`question-answering`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) | [SQuAD](https://huggingface.co/datasets/squad) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)\\n| [**`summarization`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization) |  [XSum](https://huggingface.co/datasets/xsum) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization.ipynb)\\n| [**`text-classification`**](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) | [GLUE](https://huggingface.co/datasets/glue) | ✅ | ✅ | ✅ | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)</td>\n",
              "      <td>In which GitHub repository are the examples for question-answering tasks located?\\n</td>\n",
              "      <td>The examples for question-answering tasks are located in the GitHub repository: `huggingface/transformers`. Specifically, they can be found in the subdirectory: `examples/pytorch/question-answering`.</td>\n",
              "      <td>huggingface/transformers/blob/main/examples/pytorch/README.md</td>\n",
              "      <td>5</td>\n",
              "      <td>Given the context, the question \"In which GitHub repository are the examples for question-answering tasks located?\" can be answered unambiguously as the answer is provided in the context as \"https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering\".</td>\n",
              "      <td>4</td>\n",
              "      <td>This question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem as it provides information about the location of specific examples related to question-answering tasks. Having this information can help developers quickly access and learn from the available resources.</td>\n",
              "      <td>5</td>\n",
              "      <td>This question is not dependent on any specific context and can be understood independently. The question refers to a GitHub repository, which is a common term used in software development and version control projects.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-badb0082-0300-48c5-b55a-cc56d8a3d63a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-badb0082-0300-48c5-b55a-cc56d8a3d63a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-badb0082-0300-48c5-b55a-cc56d8a3d63a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(pd\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"<p align=\\\"center\\\">\\n    <img src=\\\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/distill_sd/Picture6.png\\\" width=500>\\n</p>\\n\\n## Conclusion\\n\\nWe invite the open-source community to help us improve and achieve wider adoption of these distilled SD models. Users can join our [Discord](https://discord.gg/s6E6eHJk) server, where we will be announcing the latest updates to these models, releasing more checkpoints and some exciting new LoRAs. And if you like our work, please give us a star on our [Github](https://github.com/segmind/distill-sd).\",\n          \"|      |      |[camembert/camembert-large](https://huggingface.co/camembert/camembert-large)                                                                      |3660        |6       |                         |                                                                                   |[LICENSE](https://huggingface.co/camembert/camembert-large/blob/main/LICENSE)                                           |                                                                                                    |             |\\n|      |      |[stabilityai/japanese-stablelm-instruct-alpha-7b](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b)                          |3553        |80      |                         |                                                                                   |[LICENSE](https://huggingface.co/stabilityai/japanese-stablelm-instruct-alpha-7b/blob/main/LICENSE)                     |                                                                                                    |             |\\n|      |      |[TheBloke/llama-2-70b-Guanaco-QLoRA-fp16](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16)                                          |3537        |52      | llama2 |                                                 |[LICENSE.txt](https://huggingface.co/TheBloke/llama-2-70b-Guanaco-QLoRA-fp16/blob/main/LICENSE.txt)                     |                                                                                                    |             |\",\n          \"Simple call on one item:\\n\\n```python\\n>>> pipe = pipeline(\\\"text-classification\\\")\\n>>> pipe(\\\"This restaurant is awesome\\\")\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\\n```\\n\\nIf you want to use a specific model from the [hub](https://huggingface.co) you can ignore the task if the model on\\nthe hub already defines it:\\n\\n```python\\n>>> pipe = pipeline(model=\\\"roberta-large-mnli\\\")\\n>>> pipe(\\\"This restaurant is awesome\\\")\\n[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]\\n```\\n\\nTo call a pipeline on many items, you can call it with a *list*.\\n\\n```python\\n>>> pipe = pipeline(\\\"text-classification\\\")\\n>>> pipe([\\\"This restaurant is awesome\\\", \\\"This restaurant is awful\\\"])\\n[{'label': 'POSITIVE', 'score': 0.9998743534088135},\\n {'label': 'NEGATIVE', 'score': 0.9996669292449951}]\\n```\\n\\nTo iterate over full datasets it is recommended to use a `dataset` directly. This means you don't need to allocate\\nthe whole dataset at once, nor do you need to do batching yourself. This should work just as fast as custom loops on\\nGPU. If it doesn't don't hesitate to create an issue.\\n\\n```python\\nimport datasets\\nfrom transformers import pipeline\\nfrom transformers.pipelines.pt_utils import KeyDataset\\nfrom tqdm.auto import tqdm\\n\\npipe = pipeline(\\\"automatic-speech-recognition\\\", model=\\\"facebook/wav2vec2-base-960h\\\", device=0)\\ndataset = datasets.load_dataset(\\\"superb\\\", name=\\\"asr\\\", split=\\\"test\\\")\\n\\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\\nfor out in tqdm(pipe(KeyDataset(dataset, \\\"file\\\"))):\\n    print(out)\\n    # {\\\"text\\\": \\\"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\\\"}\\n    # {\\\"text\\\": ....}\\n    # ....\\n```\\n\\nFor ease of use, a generator is also possible:\\n\\n\\n```python\\nfrom transformers import pipeline\\n\\npipe = pipeline(\\\"text-classification\\\")\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"In which Discord server can users find the latest updates about the distilled SD models?\\n\",\n          \"What is the size (number of parameters) of \\\"llama-2-70b-Guanaco-QLoRA-fp16\\\" model?\\n\",\n          \"What is the label of the first text in the pipe(\\\"This restaurant is awesome\\\") output?\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Users can find the latest updates about the distilled SD models on the Discord server with the invitation link <https://discord.gg/s6E6eHJk>.\",\n          \"3537\",\n          \"The label of the first text is 'POSITIVE'.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"huggingface/blog/blob/main/sd_distillation.md\",\n          \"huggingface/hub-docs/blob/main/hacktoberfest_challenges/model_no_license.md\",\n          \"huggingface/transformers/blob/main/docs/source/en/main_classes/pipelines.md\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"groundedness_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"groundedness_eval\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The context explicitly states that users can find the latest updates about the distilled SD models on the Discord server mentioned in the link.\",\n          \"The context provides the number of parameters for the given model under the \\\"Size (MB)\\\" column, but no clear indication of the total number of parameters. However, we can find the number of parameters by multiplying the size (in MB) by 1024 and then dividing by 1,000,000. This calculation can be performed as follows: (3537 / 1024) * 1024 * 1024 = 3,587,251,840 parameters.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance_eval\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"This question is not directly related to building NLP applications using the Hugging Face ecosystem. It asks about a specific Discord server, which does not provide any relevant information for developers working on machine learning projects.\",\n          \"This question is useful for machine learning developers building NLP applications with the Hugging Face ecosystem as they often need to know the size of their chosen models to make informed decisions regarding system requirements and resource allocation.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"standalone_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"standalone_eval\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"This question assumes the existence of Discord servers related to the distilled SD models. Therefore, to fully understand the question, some context, such as the name or access to these servers, is required.\",\n          \"The question refers to a specific model named \\\"llama-2-70b-Guanaco-QLoRA-fp16\\\". The model name itself provides sufficient context to understand what is being asked, as long as the reader is familiar with the naming conventions used in the model community. The size or number of parameters of a model is a common piece of information that is often provided in the model documentation, making this a self-contained question.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us filter out bad questions based on our critique agent scores:"
      ],
      "metadata": {
        "id": "4TCsF85vAkX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "generated_questions = generated_questions.loc[\n",
        "    (generated_questions[\"groundedness_score\"] >= 4)\n",
        "    & (generated_questions[\"relevance_score\"] >= 4)\n",
        "    & (generated_questions[\"standalone_score\"] >= 4)\n",
        "]\n",
        "print(\"============================================\")\n",
        "print(\"Final evaluation dataset:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "eval_dataset = datasets.Dataset.from_pandas(\n",
        "    generated_questions, split=\"train\", preserve_index=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TYzPXTHkaeKB",
        "outputId": "40af2ac3-e2d5-49f4-ec6b-838162d3be84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation dataset before filtering:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                                                                                                                              question  \\\n",
              "0                                                                                     What is the average loading time for CPU with this library compared to pickle?\\n   \n",
              "1                                                                                 What is the size (number of parameters) of \"llama-2-70b-Guanaco-QLoRA-fp16\" model?\\n   \n",
              "2                                                                What are the two structures introduced in Dense Layer Aggregation (DLA) for deep layer aggregation?\\n   \n",
              "3                                                                                                      What is the shape of the RGB camera in the observation space?\\n   \n",
              "4                                                        Which OpenVINO notebook from Hugging Face provides instructions on exporting a model and running inference?\\n   \n",
              "5                                                                              What is the label of the first text in the pipe(\"This restaurant is awesome\") output?\\n   \n",
              "6  In how many pull requests were font-related changes made between commit hashes \"4e62b8493dfce50bafafe49f1a5deb929d822103\" and \"baa7ab85ede2a5eadb56c274b0ed2785\"?\\n   \n",
              "7                                                                                     In which license is the HuggingFace Multitask Prompt Tuning paper distributed?\\n   \n",
              "8                                                                           In which Discord server can users find the latest updates about the distilled SD models?\\n   \n",
              "9                                                                                  In which GitHub repository are the examples for question-answering tasks located?\\n   \n",
              "\n",
              "                                                                                                                                                                                                    answer  \\\n",
              "0                                                                                                                                   The CPU loading times are faster with this library compared to pickle.   \n",
              "1                                                                                                                                                                                                     3537   \n",
              "2                                                                                                                                 Iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA)   \n",
              "3                                                                                                                                      The RGB camera in the observation space has a shape of (3, 40, 40).   \n",
              "4                                                                                                                                         The notebook named \"optimum_openvino_inference.ipynb\" does this.   \n",
              "5                                                                                                                                                               The label of the first text is 'POSITIVE'.   \n",
              "6                                                                                                                                                 One pull request (#5904) contained font-related changes.   \n",
              "7                                                                                                      The HuggingFace Multitask Prompt Tuning paper is distributed under the Apache License, Version 2.0.   \n",
              "8                                                            Users can find the latest updates about the distilled SD models on the Discord server with the invitation link <https://discord.gg/s6E6eHJk>.   \n",
              "9  The examples for question-answering tasks are located in the GitHub repository: `huggingface/transformers`. Specifically, they can be found in the subdirectory: `examples/pytorch/question-answering`.   \n",
              "\n",
              "   groundedness_score  relevance_score  standalone_score  \n",
              "0                   1                1                 5  \n",
              "1                   5                5                 5  \n",
              "2                   5                4                 3  \n",
              "3                   1                1                 5  \n",
              "4                   5                5                 5  \n",
              "5                   5                1                 1  \n",
              "6                   1                1                 5  \n",
              "7                   1                5                 5  \n",
              "8                   5                1                 1  \n",
              "9                   5                4                 5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94f43534-e417-4338-8d36-19e8969e111f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the average loading time for CPU with this library compared to pickle?\\n</td>\n",
              "      <td>The CPU loading times are faster with this library compared to pickle.</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the size (number of parameters) of \"llama-2-70b-Guanaco-QLoRA-fp16\" model?\\n</td>\n",
              "      <td>3537</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are the two structures introduced in Dense Layer Aggregation (DLA) for deep layer aggregation?\\n</td>\n",
              "      <td>Iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA)</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the shape of the RGB camera in the observation space?\\n</td>\n",
              "      <td>The RGB camera in the observation space has a shape of (3, 40, 40).</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which OpenVINO notebook from Hugging Face provides instructions on exporting a model and running inference?\\n</td>\n",
              "      <td>The notebook named \"optimum_openvino_inference.ipynb\" does this.</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What is the label of the first text in the pipe(\"This restaurant is awesome\") output?\\n</td>\n",
              "      <td>The label of the first text is 'POSITIVE'.</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>In how many pull requests were font-related changes made between commit hashes \"4e62b8493dfce50bafafe49f1a5deb929d822103\" and \"baa7ab85ede2a5eadb56c274b0ed2785\"?\\n</td>\n",
              "      <td>One pull request (#5904) contained font-related changes.</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>In which license is the HuggingFace Multitask Prompt Tuning paper distributed?\\n</td>\n",
              "      <td>The HuggingFace Multitask Prompt Tuning paper is distributed under the Apache License, Version 2.0.</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>In which Discord server can users find the latest updates about the distilled SD models?\\n</td>\n",
              "      <td>Users can find the latest updates about the distilled SD models on the Discord server with the invitation link &lt;https://discord.gg/s6E6eHJk&gt;.</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>In which GitHub repository are the examples for question-answering tasks located?\\n</td>\n",
              "      <td>The examples for question-answering tasks are located in the GitHub repository: `huggingface/transformers`. Specifically, they can be found in the subdirectory: `examples/pytorch/question-answering`.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94f43534-e417-4338-8d36-19e8969e111f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-94f43534-e417-4338-8d36-19e8969e111f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-94f43534-e417-4338-8d36-19e8969e111f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \")\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"In which Discord server can users find the latest updates about the distilled SD models?\\n\",\n          \"What is the size (number of parameters) of \\\"llama-2-70b-Guanaco-QLoRA-fp16\\\" model?\\n\",\n          \"What is the label of the first text in the pipe(\\\"This restaurant is awesome\\\") output?\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Users can find the latest updates about the distilled SD models on the Discord server with the invitation link <https://discord.gg/s6E6eHJk>.\",\n          \"3537\",\n          \"The label of the first text is 'POSITIVE'.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"groundedness_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"standalone_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================\n",
            "Final evaluation dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                                                                        question  \\\n",
              "1                           What is the size (number of parameters) of \"llama-2-70b-Guanaco-QLoRA-fp16\" model?\\n   \n",
              "4  Which OpenVINO notebook from Hugging Face provides instructions on exporting a model and running inference?\\n   \n",
              "9                            In which GitHub repository are the examples for question-answering tasks located?\\n   \n",
              "\n",
              "                                                                                                                                                                                                    answer  \\\n",
              "1                                                                                                                                                                                                     3537   \n",
              "4                                                                                                                                         The notebook named \"optimum_openvino_inference.ipynb\" does this.   \n",
              "9  The examples for question-answering tasks are located in the GitHub repository: `huggingface/transformers`. Specifically, they can be found in the subdirectory: `examples/pytorch/question-answering`.   \n",
              "\n",
              "   groundedness_score  relevance_score  standalone_score  \n",
              "1                   5                5                 5  \n",
              "4                   5                5                 5  \n",
              "9                   5                4                 5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c56046d3-80ae-4948-8539-8d417db67b33\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the size (number of parameters) of \"llama-2-70b-Guanaco-QLoRA-fp16\" model?\\n</td>\n",
              "      <td>3537</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which OpenVINO notebook from Hugging Face provides instructions on exporting a model and running inference?\\n</td>\n",
              "      <td>The notebook named \"optimum_openvino_inference.ipynb\" does this.</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>In which GitHub repository are the examples for question-answering tasks located?\\n</td>\n",
              "      <td>The examples for question-answering tasks are located in the GitHub repository: `huggingface/transformers`. Specifically, they can be found in the subdirectory: `examples/pytorch/question-answering`.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c56046d3-80ae-4948-8539-8d417db67b33')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c56046d3-80ae-4948-8539-8d417db67b33 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c56046d3-80ae-4948-8539-8d417db67b33');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \")\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"What is the size (number of parameters) of \\\"llama-2-70b-Guanaco-QLoRA-fp16\\\" model?\\n\",\n          \"Which OpenVINO notebook from Hugging Face provides instructions on exporting a model and running inference?\\n\",\n          \"In which GitHub repository are the examples for question-answering tasks located?\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"3537\",\n          \"The notebook named \\\"optimum_openvino_inference.ipynb\\\" does this.\",\n          \"The examples for question-answering tasks are located in the GitHub repository: `huggingface/transformers`. Specifically, they can be found in the subdirectory: `examples/pytorch/question-answering`.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"groundedness_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"standalone_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset.\n",
        "\n",
        "We have generated only a few QA couples here to reduce time and cost. But let's kickstart the next part by loading a pre-generated dataset:"
      ],
      "metadata": {
        "id": "r01ikRqoA1Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "9a5d5a0f99474c64a2addfa9a7de11fe",
            "e8cf32bda73743a9bc76951fee1a4b22",
            "0fe1e5eb10644f1e86240a801d6883fb",
            "019db5c2da354eb196574bc14635f61c",
            "c73e3b63834747acafaf570fc26fdcb0",
            "f9814ac7438f4a61ab93867896ac210b",
            "a85b28b16ed641e886afc8a6824677e9",
            "5d10b2070eb54da29b94b65baae1bdc4",
            "a6bf12790c1a4dc8b3b4e2da18330d33",
            "2433e06600104b24bae5530e4468ae5e",
            "11a0497c4c824339b5b514230eb41a04",
            "6c88c802b5884b7a87dd6c3508476886",
            "edf9552b9cea4d59923473a734ed873e",
            "badd665411d6439da8552e7603c20a23",
            "229bf92377d7402990b67fb7aecb4a11",
            "49a3e6dbab6c45b399c4b0d2b11de87c",
            "392b421fd88e4d7a9db9bcada8932077",
            "702506e4198b49e786e43fd8b4dd2202",
            "56f5c3b35518482d9f097608cb122a51",
            "32d70e9c2577459d99fea09717744b2f",
            "c6ad1d842cf44d27b4dc4233fcaff2c0",
            "cb1eed43000445c3b8b2229c02495647",
            "f3919bb913f140ec94f96a08122306a2",
            "6d9f36dd918e4c6c865dcc6a3e781f14",
            "7ccc6f7cec7b4186bc49831dea01aac4",
            "11e8f67155fc414187b411162f61cb53",
            "c9d44e6aca5d46d19353a9d7e36219e7",
            "adf77b9ba4f449ec92eaeba464bc0cae",
            "88bc9f06ffbe49ac8814000b6b38b76e",
            "1ddc0839006a4f219a942d1ed4c3f915",
            "d8fa764a964047559ed35c0c112cff2d",
            "b6189b0a76604e199cf2d89b844f9606",
            "04b45c84fcc64bc9a51d4cdd32e396a7"
          ]
        },
        "id": "ulzge13IDo1b",
        "outputId": "5c80feb6-0e82-4549-84f8-eb07b618c3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/893 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a5d5a0f99474c64a2addfa9a7de11fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/289k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c88c802b5884b7a87dd6c3508476886"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3919bb913f140ec94f96a08122306a2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build RAG SYSTEM\n",
        "\n",
        "### 2.1. Preprocessing documents to build our vector database\n"
      ],
      "metadata": {
        "id": "yir-ds4xV3Uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document as LangchainDocument\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm(ds)\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3689a1d53aed49269d4a8f1ee64ac301",
            "aec569879cfe405b9bcce7622a0f957d",
            "ab6e699079474cadbea9ce0482983897",
            "af511b2328ee4e3896019023ef072219",
            "6695ed417878432fa4c55a266d9bceab",
            "41396c21bce74c5cb86d962d7c0dd2d0",
            "415bf991e48d432983a877727cc01c1d",
            "b9ae5f90e46545489800fcb2eb872613",
            "2b0a96895dcf489da1e7fd58f8e84f40",
            "535a9c3bc22748129d55f1e28e20c511",
            "2ff070b046ef4cb2841e4c55c96aa094"
          ]
        },
        "id": "wj2Sr6WrV2ts",
        "outputId": "bc4475a0-bb08-492a-e657-869459051225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2647 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3689a1d53aed49269d4a8f1ee64ac301"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: str,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        tokenizer,\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique"
      ],
      "metadata": {
        "id": "mfOm3DFNWbEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Retriever - embeddings 🗂️"
      ],
      "metadata": {
        "id": "4IxqcgCYB2vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfdqovtRpSYs",
        "outputId": "d461c5a4-2014-46e7-a5db-7fe3e265cce7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (1.2.6)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (2.12.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2026.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.16.0)\n",
            "Downloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: langchain-huggingface\n",
            "Successfully installed langchain-huggingface-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "import os\n",
        "\n",
        "def load_embeddings(\n",
        "    langchain_docs: List[LangchainDocument],\n",
        "    chunk_size: int,\n",
        "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
        ") -> FAISS:\n",
        "    # load embedding_model\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=embedding_model_name,\n",
        "        multi_process=True,\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\n",
        "            \"normalize_embeddings\": True\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # Check if embeddings already exist on disk\n",
        "    index_name = (\n",
        "        f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
        "    )\n",
        "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
        "    if os.path.isdir(index_folder_path):\n",
        "        return FAISS.load_local(\n",
        "            index_folder_path,\n",
        "            embedding_model,\n",
        "            distance_strategy=DistanceStrategy.COSINE,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        print(\"Index not found, generating it...\")\n",
        "        docs_processed = split_documents(\n",
        "            chunk_size,\n",
        "            langchain_docs,\n",
        "            embedding_model_name,\n",
        "        )\n",
        "        knowledge_index = FAISS.from_documents(\n",
        "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "        )\n",
        "        knowledge_index.save_local(index_folder_path)\n",
        "        return knowledge_index"
      ],
      "metadata": {
        "id": "BWs-YYzVWkLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Reader - LLM 💬\n",
        "\n",
        "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__"
      ],
      "metadata": {
        "id": "Nx_40CMJB_F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\"\n",
        "<|system|>\n",
        "Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
        "<|user|>\n",
        "Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gj49S-sDW6Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "base_llm = HuggingFaceEndpoint(\n",
        "    repo_id=repo_id,\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=hf_token,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "\n",
        "READER_LLM = ChatHuggingFace(llm=base_llm)"
      ],
      "metadata": {
        "id": "KKc9NxftcS_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "# repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "# READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
        "\n",
        "# READER_LLM = HuggingFaceEndpoint(\n",
        "#     repo_id=repo_id,\n",
        "#     task=\"conversational\",\n",
        "#     max_new_tokens=512,\n",
        "#     top_k=30,\n",
        "#     temperature=0.1,\n",
        "#     repetition_penalty=1.03,\n",
        "# )"
      ],
      "metadata": {
        "id": "NMQ21z_YI-Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reimport 2 error files\n",
        "\n",
        "\n",
        "**from langchain_core.documents.compressor import BaseDocumentCompressor**\n"
      ],
      "metadata": {
        "id": "_zl5plPjmp0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_core.language_models.llms import LLM\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: LLM,\n",
        "    knowledge_index: VectorStore,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 7,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
        "    # Gather documents with retriever\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    answer = llm.invoke(final_prompt)\n",
        "\n",
        "    return answer, relevant_docs"
      ],
      "metadata": {
        "id": "MwTv6GfhYXOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Benchmarking the RAG system\n",
        "\n",
        "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evaluation dataset."
      ],
      "metadata": {
        "id": "7NmzQdllCK4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.language_models import BaseChatModel\n",
        "\n",
        "def run_rag_tests(\n",
        "    eval_dataset: datasets.Dataset,\n",
        "    llm,\n",
        "    knowledge_index: VectorStore,\n",
        "    output_file: str,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    verbose: Optional[bool] = True,\n",
        "    test_settings: Optional[str] = None,  # To document the test settings used\n",
        "):\n",
        "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
        "    try:  # load previous generations if they exist\n",
        "        with open(output_file, \"r\") as f:\n",
        "            outputs = json.load(f)\n",
        "    except:\n",
        "        outputs = []\n",
        "\n",
        "    for example in tqdm(eval_dataset):\n",
        "        question = example[\"question\"]\n",
        "        if question in [output[\"question\"] for output in outputs]:\n",
        "            continue\n",
        "\n",
        "        answer, relevant_docs = answer_with_rag(\n",
        "            question, llm, knowledge_index, reranker=reranker\n",
        "        )\n",
        "        if verbose:\n",
        "            print(\"=======================================================\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f'True answer: {example[\"answer\"]}')\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"true_answer\": example[\"answer\"],\n",
        "            \"source_doc\": example[\"source_doc\"],\n",
        "            \"generated_answer\": answer.content, # Fixed: Accessing the content attribute\n",
        "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
        "        }\n",
        "        if test_settings:\n",
        "            result[\"test_settings\"] = test_settings\n",
        "        outputs.append(result)\n",
        "\n",
        "        with open(output_file, \"w\") as f:\n",
        "            json.dump(outputs, f)"
      ],
      "metadata": {
        "id": "dBs1h5nIew6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
        "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
        "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
        "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
        "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
        "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
        "\n",
        "###The instruction to evaluate:\n",
        "{instruction}\n",
        "\n",
        "###Response to evaluate:\n",
        "{response}\n",
        "\n",
        "###Reference Answer (Score 5):\n",
        "{reference_answer}\n",
        "\n",
        "###Score Rubrics:\n",
        "[Is the response correct, accurate, and factual based on the reference answer?]\n",
        "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
        "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
        "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
        "Score 4: The response is mostly correct, accurate, and factual.\n",
        "Score 5: The response is completely correct, accurate, and factual.\n",
        "\n",
        "###Feedback:\"\"\"\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
        "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "rZ4n47ulgQO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# eval_chat_model = ChatOpenAI(\n",
        "#     model=\"gpt-4o-mini\",\n",
        "#     temperature=0,\n",
        "#     openai_api_key=userdata.get('key_openai'))\n",
        "# evaluator_name = \"GPT4\"\n",
        "\n",
        "eval_chat_model = ChatOpenAI(\n",
        "    model=\"gpt-4.1\", # Model's name\n",
        "    temperature=0,\n",
        "    openai_api_key=userdata.get('key_ptn'), # PTN's key\n",
        "    base_url=\"https://llm.ptnglobalcorp.com\"\n",
        ")\n",
        "evaluator_name = \"GPT4.1\"\n",
        "\n",
        "def evaluate_answers(\n",
        "    answer_path: str,\n",
        "    eval_chat_model,\n",
        "    evaluator_name: str,\n",
        "    evaluation_prompt_template: ChatPromptTemplate,\n",
        ") -> None:\n",
        "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
        "    answers = []\n",
        "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
        "        with open(answer_path, \"r\") as f:\n",
        "            answers = json.load(f)\n",
        "\n",
        "    for experiment in tqdm(answers):\n",
        "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
        "            continue\n",
        "\n",
        "        eval_prompt = evaluation_prompt_template.format_messages(\n",
        "            instruction=experiment[\"question\"],\n",
        "            response=experiment[\"generated_answer\"],\n",
        "            reference_answer=experiment[\"true_answer\"],\n",
        "        )\n",
        "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
        "\n",
        "        feedback, score = [\n",
        "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
        "        ]\n",
        "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
        "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
        "\n",
        "        with open(answer_path, \"w\") as f:\n",
        "            json.dump(answers, f)"
      ],
      "metadata": {
        "id": "f9Rh_lIYl8BU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚀 Let's run the tests and evaluate answers!👇"
      ],
      "metadata": {
        "id": "ZBOfoqPPCUEu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e4ce26d493404446bf8ef1d006c63b73",
            "aca878a367f24e5eaeaf48376331cf16",
            "c2d90b5568a3431c803c5ef206713648",
            "d492d1f9ac9c45ccb3fecb64d18bdb7a",
            "47cdf2a1496346a3a5cd568a50c4dd77",
            "e16cf22f5e7f4381ac961914c3d7cebe",
            "3388d99f080a43d298bf1609e9f41b00",
            "f018a8b0e2e34371b9efaefc057293b2",
            "0c674b4aeb854a88bd59b690cbf2aa7f",
            "8afa32fa01b4420c9a5dd6fb783c469b",
            "e656da250f954b76bad047c380c68546",
            "496d3ea96f1a4730bbecb164d269b2bd",
            "ac93b6953e4544eaafe7c74a47f09c20",
            "6e132bfeafc040dba1eec37fac8d46d0",
            "9331f900c19a4b689f139db0a88a7d0d",
            "53edc179da214a46b860ca7a9db117f8",
            "bc289aee2b974ad1be0474672ae5700b",
            "1bf8ea6489e74199af14ac827934f92a",
            "acd0fab29e3a457e8b78e5c82e1e8f7c",
            "39a75f8ad2d64e78b9d247274a21398a",
            "25fd85c5861845308a412117734a507d",
            "fb16e35f50bc4333aa8ad7bea29807c1"
          ]
        },
        "id": "K0bFj8bh-VwD",
        "outputId": "51dc8b82-a6b2-4909-97c0-39940742dab7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta:\n",
            "Loading knowledge base embeddings...\n",
            "Running RAG...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/65 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4ce26d493404446bf8ef1d006c63b73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.56it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.75it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.34it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.51it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.43it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.15it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.47it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 10.03it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.65it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.61it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.31it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 10.00it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.51it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.35it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.57it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.54it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.79it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.63it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.73it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.64it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.56it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.55it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.18it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:00<00:00,  9.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta:\n",
            "Loading knowledge base embeddings...\n",
            "Running RAG...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/65 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "496d3ea96f1a4730bbecb164d269b2bd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "if not os.path.exists(\"./output\"):\n",
        "    os.mkdir(\"./output\")\n",
        "\n",
        "READER_MODEL_NAME = \"zephyr-7b-beta\"\n",
        "\n",
        "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
        "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
        "        for rerank in [True, False]:\n",
        "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
        "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
        "\n",
        "            print(f\"Running evaluation for {settings_name}:\")\n",
        "\n",
        "            print(\"Loading knowledge base embeddings...\")\n",
        "            knowledge_index = load_embeddings(\n",
        "                RAW_KNOWLEDGE_BASE,\n",
        "                chunk_size=chunk_size,\n",
        "                embedding_model_name=embeddings,\n",
        "            )\n",
        "\n",
        "            print(\"Running RAG...\")\n",
        "            reranker = (\n",
        "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "                if rerank\n",
        "                else None\n",
        "            )\n",
        "\n",
        "            run_rag_tests(\n",
        "                eval_dataset=eval_dataset,\n",
        "                llm=READER_LLM,\n",
        "                knowledge_index=knowledge_index,\n",
        "                output_file=output_file_name,\n",
        "                reranker=reranker,\n",
        "                verbose=False,\n",
        "                test_settings=settings_name,\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into 2 sessions: run_rag_tests and evaluate_anwsers in order to fix bug\n",
        "\n",
        "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
        "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
        "        for rerank in [True, False]:\n",
        "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
        "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
        "\n",
        "            print(\"Running evaluation (GPT4.1 Judge)...\")\n",
        "            evaluate_answers(\n",
        "                output_file_name,\n",
        "                eval_chat_model,\n",
        "                evaluator_name,\n",
        "                evaluation_prompt_template,\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjCHCMU8VKyY",
        "outputId": "7facfbc0-b6ae-4298-c832-083cbbd5c37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation (GPT4.1 Judge)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 65/65 [05:30<00:00,  5.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running evaluation (GPT4.1 Judge)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 65/65 [06:52<00:00,  6.35s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "outputs = []\n",
        "for file in glob.glob(\"./output/*.json\"):\n",
        "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
        "    output[\"settings\"] = file\n",
        "    outputs.append(output)\n",
        "result = pd.concat(outputs)"
      ],
      "metadata": {
        "id": "YArEi_gMFYtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"eval_score_GPT4.1\"] = result[\"eval_score_GPT4.1\"].apply(\n",
        "    lambda x: int(x) if isinstance(x, str) else 1\n",
        ")\n",
        "result[\"eval_score_GPT4.1\"] = (result[\"eval_score_GPT4.1\"] - 1) / 4"
      ],
      "metadata": {
        "id": "oDcCHncKKCLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4.1\"].mean()\n",
        "average_scores.sort_values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "CDuPU3fWKDt2",
        "outputId": "b598edda-4c25-45ec-b340-786121da6988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "settings\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json    0.715385\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json     0.757692\n",
              "Name: eval_score_GPT4.1, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eval_score_GPT4.1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>settings</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:zephyr-7b-beta.json</th>\n",
              "      <td>0.715385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:zephyr-7b-beta.json</th>\n",
              "      <td>0.757692</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example results"
      ],
      "metadata": {
        "id": "cy-kahbmKh6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
        "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
      ],
      "metadata": {
        "id": "IcN5LaO1KHn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.bar(\n",
        "    scores,\n",
        "    color=scores,\n",
        "    labels={\n",
        "        \"value\": \"Accuracy\",\n",
        "        \"settings\": \"Configuration\",\n",
        "    },\n",
        "    color_continuous_scale=\"bluered\",\n",
        ")\n",
        "fig.update_layout(\n",
        "    width=1000,\n",
        "    height=600,\n",
        "    barmode=\"group\",\n",
        "    yaxis_range=[0, 100],\n",
        "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
        "    xaxis_title=\"RAG settings\",\n",
        "    font=dict(size=15),\n",
        ")\n",
        "fig.layout.yaxis.ticksuffix = \"%\"\n",
        "fig.update_coloraxes(showscale=False)\n",
        "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "TlPyLG7WKQAv",
        "outputId": "49c2ddf1-a966-486a-bc1d-9096b77c7212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"e3035616-08e6-4f95-9115-83ae903b9b85\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e3035616-08e6-4f95-9115-83ae903b9b85\")) {                    Plotly.newPlot(                        \"e3035616-08e6-4f95-9115-83ae903b9b85\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"index=%{x}\\u003cbr\\u003eAccuracy=%{y}\\u003cbr\\u003ecolor=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[74.6268656716418,86.94029850746269,88.4328,92.1642,94.7761],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"outside\",\"x\":[\"Baseline RAG\",\"+ Tune chunk size\",\"+ Better embeddings\",\"+ Rerank snippets\",\"+ Better reader LLM\"],\"xaxis\":\"x\",\"y\":[74.6268656716418,86.94029850746269,88.4328,92.1642,94.7761],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{y:.1f}\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"RAG settings\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Accuracy\"},\"range\":[0,100],\"ticksuffix\":\"%\"},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"color\"}},\"colorscale\":[[0.0,\"rgb(0,0,255)\"],[1.0,\"rgb(255,0,0)\"]],\"showscale\":false},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"group\",\"font\":{\"size\":15},\"width\":1000,\"height\":600,\"title\":{\"text\":\"\\u003cb\\u003eAccuracy of different RAG configurations\\u003c\\u002fb\\u003e\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e3035616-08e6-4f95-9115-83ae903b9b85');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}