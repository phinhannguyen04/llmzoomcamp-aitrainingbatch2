{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YErqpfH9jVI"
      },
      "source": [
        "# RAG Evaluation\n",
        "_Authored by: [Aymeric Roucher](https://huggingface.co/m-ric)_\n",
        "\n",
        "This notebook demonstrates how you can evaluate your RAG (Retrieval Augmented Generation), by building a synthetic evaluation dataset and using LLM-as-a-judge to compute the accuracy of your system.\n",
        "\n",
        "For an introduction to RAG, you can check [this other cookbook](rag_zephyr_langchain)!\n",
        "\n",
        "RAG systems are complex: here a RAG diagram, where we noted in blue all possibilities for system enhancement:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_workflow.png\" height=\"700\">\n",
        "\n",
        "Implementing any of these improvements can bring a huge performance boost; but changing anything is useless if you cannot monitor the impact of your changes on the system's performance!\n",
        "So let's see how to evaluate our RAG system.\n",
        "\n",
        "### Evaluating RAG performance\n",
        "\n",
        "Since there are so many moving parts to tune with a big impact on performance, benchmarking the RAG system is crucial.\n",
        "\n",
        "For our evaluation pipeline, we will need:\n",
        "1. An evaluation dataset with question - answer couples (QA couples)\n",
        "2. An evaluator to compute the accuracy of our system on the above evaluation dataset.\n",
        "\n",
        "â¡ï¸ It turns out, we can use LLMs to help us all along the way!\n",
        "1. The evaluation dataset will be synthetically generated by an LLM ğŸ¤–, and questions will be filtered out by other LLMs ğŸ¤–\n",
        "2. An [LLM-as-a-judge](https://huggingface.co/papers/2306.05685) agent ğŸ¤– will then perform the evaluation on this synthetic dataset.\n",
        "\n",
        "__Let's dig into it and start building our evaluation pipeline!__ First, we install the required model dependancies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCKBvOcp9jVK",
        "outputId": "d03c207f-3e9e-4860-de17-908ed19f7fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/2.5 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/46.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m340.3/340.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.6/102.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.0/329.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_RHSq2pJVry",
        "outputId": "d977cc9c-2da0-471e-9d6c-145722c390da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ragatouille in /usr/local/lib/python3.12/dist-packages (0.0.9.post2)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.12/dist-packages (from ragatouille) (0.14.12)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (from ragatouille) (1.13.2)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (from ragatouille) (1.2.6)\n",
            "Requirement already satisfied: colbert-ai>=0.2.19 in /usr/local/lib/python3.12/dist-packages (from ragatouille) (0.2.22)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (from ragatouille) (1.2.3)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (from ragatouille) (1.20.1)\n",
            "Requirement already satisfied: srsly in /usr/local/lib/python3.12/dist-packages (from ragatouille) (2.5.2)\n",
            "Requirement already satisfied: voyager in /usr/local/lib/python3.12/dist-packages (from ragatouille) (2.1.0)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from ragatouille) (2.9.0+cu126)\n",
            "Requirement already satisfied: fast-pytorch-kmeans in /usr/local/lib/python3.12/dist-packages (from ragatouille) (0.2.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (from ragatouille) (5.2.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (3.8.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (4.0.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (3.1.2)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (3.1.46)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.2.1)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.13.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (1.16.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (4.57.3)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.12/dist-packages (from colbert-ai>=0.2.19->ragatouille) (5.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->ragatouille) (3.5.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu->ragatouille) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu->ragatouille) (25.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain->ragatouille) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain->ragatouille) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille) (0.6.1)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->ragatouille) (0.13.0)\n",
            "Requirement already satisfied: llama-index-cli<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.5.3)\n",
            "Requirement already satisfied: llama-index-core<0.15.0,>=0.14.12 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.14.12)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.5.1)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.9.4)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.7,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.6.13)\n",
            "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.5.6)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (0.5.1)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index->ragatouille) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->ragatouille) (5.29.5)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx->ragatouille) (0.5.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->ragatouille) (1.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers->ragatouille) (0.36.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from srsly->ragatouille) (2.0.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core->ragatouille) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragatouille) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragatouille) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragatouille) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain->ragatouille) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (0.25.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (3.13.3)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (0.22.1)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (1.2.0)\n",
            "Requirement already satisfied: llama-index-workflows!=2.9.0,<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (2.12.2)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (1.6.0)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (4.5.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (2.0.45)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (0.12.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (1.17.3)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille) (2.14.0)\n",
            "Requirement already satisfied: llama-cloud==0.1.35 in /usr/local/lib/python3.12/dist-packages (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille) (0.1.35)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index->ragatouille) (2026.1.4)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (0.7.1)\n",
            "Requirement already satisfied: pandas<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (2.2.2)\n",
            "Requirement already satisfied: pypdf<7,>=6.1.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (6.6.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille) (0.6.54)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index->ragatouille) (2025.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain->ragatouille) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->ragatouille) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->colbert-ai>=0.2.19->ragatouille) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (0.3.8)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->colbert-ai>=0.2.19->ragatouille) (0.70.16)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (2.2.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask->colbert-ai>=0.2.19->ragatouille) (3.1.5)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from GitPython->colbert-ai>=0.2.19->ragatouille) (4.0.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers->ragatouille) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (1.22.0)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (1.15.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (2.8.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->GitPython->colbert-ai>=0.2.19->ragatouille) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core->ragatouille) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain->ragatouille) (1.12.1)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (0.4.2)\n",
            "Requirement already satisfied: llama-cloud-services>=0.6.54 in /usr/local/lib/python3.12/dist-packages (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index->ragatouille) (0.6.54)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index->ragatouille) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers->ragatouille) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (3.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (3.26.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=2.0.0->llama-index-readers-file<0.6,>=0.5.0->llama-index->ragatouille) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15.0,>=0.14.12->llama-index->ragatouille) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install ragatouille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCZgT_tJA8PF",
        "outputId": "725c33cc-4776-49dd-9426-7db8f45ce437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jedi==0.16\n",
            "  Downloading jedi-0.16.0-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: parso>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from jedi==0.16) (0.8.5)\n",
            "Downloading jedi-0.16.0-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install jedi==0.16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oO933pJBDuC",
        "outputId": "944a763c-9071-4587-c14c-e72212cd02c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting requests==2.32.4\n",
            "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.4) (2026.1.4)\n",
            "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/64.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: requests\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.5\n",
            "    Uninstalling requests-2.32.5:\n",
            "      Successfully uninstalled requests-2.32.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-community 0.4.1 requires requests<3.0.0,>=2.32.5, but you have requests 2.32.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed requests-2.32.4\n"
          ]
        }
      ],
      "source": [
        "!pip install requests==2.32.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oIlNZ1Mn9jVL"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "import json\n",
        "import datasets\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeW8P62J9jVM"
      },
      "source": [
        "### Load your knowledge base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YRbm5tNF9jVM"
      },
      "outputs": [],
      "source": [
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy9CKj0M9jVM"
      },
      "source": [
        "# 1. Build a synthetic dataset for evaluation\n",
        "We first build a synthetic dataset of questions and associated contexts. The method is to get elements from our knowledge base, and ask an LLM to generate questions based on these documents.\n",
        "\n",
        "Then we setup other LLM agents to act as quality filters for the generated QA couples: each of them will act as the filter for a specific flaw."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkoEgiDg9jVM"
      },
      "source": [
        "### 1.1. Prepare source documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_OYcDAtACNh9",
        "outputId": "547a9967-46b4-4a6b-d816-11d4d4f47b2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (1.2.6)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-1.2.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain_core) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain_core) (2.5.0)\n",
            "Downloading langchain_core-1.2.7-py3-none-any.whl (490 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m490.2/490.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_core\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 1.2.6\n",
            "    Uninstalling langchain-core-1.2.6:\n",
            "      Successfully uninstalled langchain-core-1.2.6\n",
            "Successfully installed langchain_core-1.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-text-splitters langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b3cce8e79e3e4ac9a7d87b31c3551e90",
            "013f6e7a28eb4001a469f5b8ec44bd61",
            "755b99c200bd454fbe61320b9c1b4417",
            "2da7074ffa00465e872a70c1c79f4a3c",
            "f77e5a87330f4897b221142785afab31",
            "ac110c35b7944c98a166b6bf6c8d7fcd",
            "5840658bf35e4c959cc8f14fbe918263",
            "2d9ebc764ada45778801b65e9280a174",
            "2dbd0a808c34457bab6b64430818973e",
            "ceb57d958b6f4da3b72237c61d097505",
            "e9309ef0171c4b8fb6449c34a50fa1de"
          ]
        },
        "id": "3gTOlRKO9jVM",
        "outputId": "fe24d7f3-f643-4943-d8cd-12054015a053"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3cce8e79e3e4ac9a7d87b31c3551e90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2647 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document as LangchainDocument\n",
        "\n",
        "langchain_docs = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm(ds)\n",
        "]\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=2000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "docs_processed = []\n",
        "for doc in langchain_docs:\n",
        "    docs_processed += text_splitter.split_documents([doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjrNhcCh9jVN"
      },
      "source": [
        "### 1.2. Setup agents for question generation\n",
        "\n",
        "We use [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) for QA couple generation because it it has excellent performance in leaderboards such as [Chatbot Arena](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP_q2wa3UHB0",
        "outputId": "fc99d8ff-c163-4c28-efd4-7ebfab5224f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Got it! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "# It will automatically look for an environment variable named OPENAI_API_KEY\n",
        "# Or you can pass it explicitly: client = OpenAI(api_key=\"your-key-here\")\n",
        "llm_client = OpenAI(api_key=\"your-api-key\")\n",
        "\n",
        "def call_llm(openai_client: OpenAI, prompt: str):\n",
        "    # Wrap your prompt in the expected chat dictionary format\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    # Call the Chat Completion API\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o\",  # or \"gpt-3.5-turbo\"\n",
        "        messages=messages,\n",
        "        max_tokens=1000,\n",
        "    )\n",
        "\n",
        "    # Extract the text content from the response object\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Test the call\n",
        "result = call_llm(llm_client, \"This is a test context\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hIM_DJRo9jVN"
      },
      "outputs": [],
      "source": [
        "QA_generation_prompt = \"\"\"\n",
        "Your task is to write a high-quality technical question and answer pair based on the given context.\n",
        "\n",
        "The question must be relevant for a Machine Learning Engineer or Data Scientist working with the Hugging Face ecosystem (Transformers, Datasets, Hub, etc.).\n",
        "\n",
        "Guidelines:\n",
        "1. **Focus on Technical Utility**: Instead of simple facts, ask about implementation details, library usage, model characteristics, or troubleshooting mentioned in the context.\n",
        "2. **Be Standalone**: The question must be understandable without seeing the context. Use specific names (e.g., \"the ViT model\", \"the GLUE dataset\") instead of \"the model\" or \"this dataset\".\n",
        "3. **Search Engine Style**: Formulate it like a query someone would type into Google or Stack Overflow.\n",
        "4. **No Meta-references**: Do NOT use phrases like \"according to the text\", \"in the context\", or \"the provided passage\".\n",
        "5. **Concise Answer**: The answer should be direct and technically accurate.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Output:::\n",
        "Factoid question: (your technical question)\n",
        "Answer: (your concise technical answer)\n",
        "\n",
        "Now here is the context.\n",
        "\n",
        "Context: {context}\\n\n",
        "Output:::\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVFc-lVy9jVN"
      },
      "source": [
        "Now let's generate our QA couples.\n",
        "\n",
        "*   Má»¥c danh sÃ¡ch\n",
        "*   Má»¥c danh sÃ¡ch\n",
        "\n",
        "\n",
        "For this example, we generate only 10 QA couples and will load the rest from the Hub.\n",
        "\n",
        "But for your specific knowledge base, given that you want to get at least ~100 test samples, and accounting for the fact that we will filter out around half of these with our critique agents later on, you should generate much more, in the >200 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fteqDDD9jVN",
        "outputId": "5b054d3c-63eb-49f8-bb19-a17add667f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating 10 QA couples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:26<00:00,  2.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HoÃ n thÃ nh! ÄÃ£ táº¡o Ä‘Æ°á»£c 10 cáº·p QA cháº¥t lÆ°á»£ng.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "N_GENERATIONS = 10\n",
        "print(f\"Generating {N_GENERATIONS} QA couples...\")\n",
        "\n",
        "outputs = []\n",
        "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
        "    output_QA_couple = call_llm(\n",
        "        llm_client, QA_generation_prompt.format(context=sampled_context.page_content)\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # 1. DÃ¹ng Regex Ä‘á»ƒ báº¯t Question vÃ  Answer khÃ´ng phÃ¢n biá»‡t hoa thÆ°á»ng, khoáº£ng tráº¯ng\n",
        "        # TÃ¬m ná»™i dung giá»¯a \"Factoid question:\" vÃ  \"Answer:\"\n",
        "        q_pattern = r\"Factoid [Qq]uestion:\\s*(.*?)(?=\\s*Answer:|$)\"\n",
        "        a_pattern = r\"Answer:\\s*(.*)\"\n",
        "\n",
        "        question_match = re.search(q_pattern, output_QA_couple, re.DOTALL | re.IGNORECASE)\n",
        "        answer_match = re.search(a_pattern, output_QA_couple, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "        if not question_match or not answer_match:\n",
        "            print(f\"Skipped: KhÃ´ng tÃ¬m tháº¥y Ä‘Ãºng cáº¥u trÃºc trong output: {output_QA_couple[:50]}...\")\n",
        "            continue\n",
        "\n",
        "        question = question_match.group(1).strip()\n",
        "        answer = answer_match.group(1).strip()\n",
        "\n",
        "        # 2. Ná»›i lá»ng giá»›i háº¡n Ä‘á»™ dÃ i (cÃ¢u tráº£ lá»i ká»¹ thuáº­t cáº§n khÃ´ng gian Ä‘á»ƒ giáº£i thÃ­ch)\n",
        "        if len(answer) > 800:\n",
        "            print(\"Skipped: Answer quÃ¡ dÃ i (> 800 chars)\")\n",
        "            continue\n",
        "\n",
        "        outputs.append({\n",
        "            \"context\": sampled_context.page_content,\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"source_doc\": sampled_context.metadata[\"source\"],\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Lá»—i há»‡ thá»‘ng khi xá»­ lÃ½ context: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"HoÃ n thÃ nh! ÄÃ£ táº¡o Ä‘Æ°á»£c {len(outputs)} cáº·p QA cháº¥t lÆ°á»£ng.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUYEb8c2de6P",
        "outputId": "c0d2fc35-ff25-4d18-a14b-bad82c139b96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'context': '`max_length`: the maximum sequence length (default value is `128`).\\n\\n`device`: either \"gpu\" or \"cpu\" (default value is `None`). \\n\\n```python\\n>>> results = frugalscore.compute(predictions=[\\'hello there\\', \\'huggingface\\'], references=[\\'hello world\\', \\'hugging face\\'], batch_size=16, max_length=64, device=\"gpu\")\\n```\\n\\n## Output values\\n\\nThe output of FrugalScore is a dictionary with the list of scores for each prediction-reference pair:\\n```python\\n{\\'scores\\': [0.6307541, 0.6449357]}\\n```\\n\\n### Values from popular papers\\nThe [original FrugalScore paper](https://arxiv.org/abs/2110.08559) reported that FrugalScore-Tiny retains 97.7/94.7% of the original performance compared to [BertScore](https://huggingface.co/metrics/bertscore) while running 54 times faster and having 84 times less parameters.\\n\\n## Examples \\n\\nMaximal values (exact match between `references` and `predictions`): \\n\\n```python\\n>>> frugalscore = evaluate.load(\"frugalscore\")\\n>>> results = frugalscore.compute(predictions=[\\'hello world\\'], references=[\\'hello world\\'])\\n>>> print(results)\\n{\\'scores\\': [0.9891098]}\\n```\\n\\nPartial values: \\n\\n```python\\n>>> frugalscore = evaluate.load(\"frugalscore\")\\n>>> results = frugalscore.compute(predictions=[\\'hello world\\'], references=[\\'hugging face\\'])\\n>>> print(results)\\n{\\'scores\\': [0.42482382]}\\n```\\n\\n## Limitations and bias\\n\\nFrugalScore is based on [BertScore](https://huggingface.co/metrics/bertscore) and [MoverScore](https://arxiv.org/abs/1909.02622), and the models used are based on the original models used for these scores.\\n\\nThe full list of available models for FrugalScore is:',\n",
              "  'question': 'How can you specify the maximum sequence length and device type when computing FrugalScore in Hugging Face?',\n",
              "  'answer': 'When computing FrugalScore using the `frugalscore.compute()` method, you can specify the maximum sequence length by setting the `max_length` parameter, and you can choose the device type by setting the `device` parameter to either \"gpu\" or \"cpu\". For example: `frugalscore.compute(predictions=[\\'hello there\\'], references=[\\'hello world\\'], max_length=64, device=\"gpu\")`.',\n",
              "  'source_doc': 'huggingface/evaluate/blob/main/metrics/frugalscore/README.md'},\n",
              " {'context': 'Output Example(s):\\n```python\\n{\\'accuracy\\': 1.0}\\n```\\n\\nThis metric outputs a dictionary, containing the accuracy score.\\n\\n\\n#### Values from Popular Papers\\n\\nTop-1 or top-5 accuracy is often used to report performance on supervised classification tasks such as image classification (e.g. on [ImageNet](https://paperswithcode.com/sota/image-classification-on-imagenet)) or sentiment analysis (e.g. on [IMDB](https://paperswithcode.com/sota/text-classification-on-imdb)). \\n\\n\\n### Examples\\n\\nExample 1-A simple example\\n```python\\n>>> accuracy_metric = evaluate.load(\"accuracy\")\\n>>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0])\\n>>> print(results)\\n{\\'accuracy\\': 0.5}\\n```\\n\\nExample 2-The same as Example 1, except with `normalize` set to `False`.\\n```python\\n>>> accuracy_metric = evaluate.load(\"accuracy\")\\n>>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\\n>>> print(results)\\n{\\'accuracy\\': 3.0}\\n```\\n\\nExample 3-The same as Example 1, except with `sample_weight` set.\\n```python\\n>>> accuracy_metric = evaluate.load(\"accuracy\")\\n>>> results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], sample_weight=[0.5, 2, 0.7, 0.5, 9, 0.4])\\n>>> print(results)\\n{\\'accuracy\\': 0.8778625954198473}\\n```\\n\\n\\n## Limitations and Bias\\nThis metric can be easily misleading, especially in the case of unbalanced classes. For example, a high accuracy might be because a model is doing well, but if the data is unbalanced, it might also be because the model is only accurately labeling the high-frequency class. In such cases, a more detailed analysis of the model\\'s behavior, or the use of a different metric entirely, is necessary to determine how well the model is actually performing.',\n",
              "  'question': 'How can you compute accuracy with normalization turned off using the Hugging Face Evaluate library?',\n",
              "  'answer': 'In the Hugging Face Evaluate library, you can compute accuracy with normalization turned off by setting the `normalize` parameter to `False` when calling the `compute` method on an accuracy metric object. For example:\\n\\n```python\\naccuracy_metric = evaluate.load(\"accuracy\")\\nresults = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\\nprint(results)\\n```\\n\\nThis will output the sum of correctly predicted samples instead of the proportion of correct predictions over total samples.',\n",
              "  'source_doc': 'huggingface/evaluate/blob/main/metrics/accuracy/README.md'},\n",
              " {'context': '### Why It Isn\\'t Useful (yet)\\n\\n**Note:** This section is intended for readers who are familiar with conventional 3D rendering techniques, such as [meshes](https://en.wikipedia.org/wiki/Polygon_mesh), [UV mapping](https://en.wikipedia.org/wiki/UV_mapping) and [photogrammetry](https://en.wikipedia.org/wiki/Photogrammetry).\\n\\nWhile view synthesis is impressive, the world of 3D runs on meshes, which are not the same as NeRFs. There is, however, [ongoing work on converting NeRFs to meshes](https://github.com/NVlabs/instant-ngp). In practice, this is reminiscient of [photogrammetry](https://en.wikipedia.org/wiki/Photogrammetry), where multiple photos of real-world objects are combined to author 3D assets.\\n\\n<figure class=\"image text-center\">\\n  <img src=\"https://github.com/NVlabs/instant-ngp/raw/master/docs/assets_readme/testbed.png\" alt=\"NeRF-to-mesh\">\\n  <figcaption>NVlabs instant-ngp, which supports NeRF-to-mesh conversion.</figcaption>\\n</figure>\\n\\nThe practical use of assets generated using the text-to-NeRF-to-mesh pipeline is limited in a similar way to assets produced using photogrammetry. That is, the resulting mesh is not immediately game-ready, and requires significant work and expertise to become a game-ready asset. In this sense, NeRF-to-mesh may be a useful tool as-is, but doesn\\'t yet reach the transformative potential of text-to-3D.\\n\\nSince NeRF-to-mesh, like photogrammetry, is currently most suited to creating ultra-high-fidelity assets with significant manual post-processing, it doesn\\'t really make sense for creating a farming game in 5 days. In which case, I decided to just use cubes of different colors to represent the crops in the game.\\n\\n<figure class=\"image text-center\">\\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/cubes.png\" alt=\"Stable Diffusion Demo Space\">\\n</figure>',\n",
              "  'question': 'How can NeRF-to-mesh conversion be applied, and what are its limitations in creating game-ready 3D assets?',\n",
              "  'answer': \"NeRF-to-mesh conversion can be applied by using tools like NVlabs' instant-ngp, which facilitate transforming neural radiance fields (NeRFs) into mesh representations. However, the resulting meshes are not immediately game-ready and require significant manual post-processing and expertise, similar to photogrammetry techniques, to become usable in game development. Consequently, while NeRF-to-mesh can create high-fidelity assets, it's often not practical for quick and straightforward game asset creation.\",\n",
              "  'source_doc': 'huggingface/blog/blob/main/ml-for-games-3.md'},\n",
              " {'context': '# Optional. This part can be used to store the feature types and size of the dataset to be used in python. This can be automatically generated using the datasets-cli.\\ndataset_info:\\n  features:\\n    - name: {feature_name_0}    # Example: id\\n      dtype: {feature_dtype_0}  # Example: int32\\n    - name: {feature_name_1}    # Example: text\\n      dtype: {feature_dtype_1}  # Example: string\\n    - name: {feature_name_2}    # Example: image\\n      dtype: {feature_dtype_2}  # Example: image\\n    # Example for SQuAD:\\n    # - name: id\\n    #   dtype: string\\n    # - name: title\\n    #   dtype: string\\n    # - name: context\\n    #   dtype: string\\n    # - name: question\\n    #   dtype: string\\n    # - name: answers\\n    #   sequence:\\n    #     - name: text\\n    #       dtype: string\\n    #     - name: answer_start\\n    #       dtype: int32\\n  config_name: {config_name}  # Example for glue: sst2\\n  splits:\\n    - name: {split_name_0}                  # Example: train\\n      num_bytes: {split_num_bytes_0}        # Example for SQuAD: 79317110\\n      num_examples: {split_num_examples_0}  # Example for SQuAD: 87599\\n  download_size: {dataset_download_size}   # Example for SQuAD: 35142551\\n  dataset_size: {dataset_size}             # Example for SQuAD: 89789763\\n\\n# It can also be a list of multiple configurations:\\n# ```yaml\\n# dataset_info:\\n#   - config_name: {config0}\\n#     features:\\n#       ...\\n#   - config_name: {config1}\\n#     features:\\n#       ...\\n# ```',\n",
              "  'question': 'How do you use datasets-cli to automatically generate feature types and sizes for a dataset in the Hugging Face Datasets library?',\n",
              "  'answer': \"To automatically generate feature types and sizes for a dataset with datasets-cli in the Hugging Face Datasets library, you can use the `datasets-cli test` command followed by the directory containing your dataset's loading script. This command will inspect your dataset and create a README.md with metadata, including feature types, dataset size, and number of examples for each split.\",\n",
              "  'source_doc': 'huggingface/hub-docs/blob/main/datasetcard.md'},\n",
              " {'context': \"![Collection image drop zone with images](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/collections/collections-image-gallery.webp)\\n\\nYou can re-order images by drag-and-dropping them. Clicking on an image will open it in full-screen mode.\\n\\n![Collection image viewer](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/collections/collections-image-viewer.webp)\\n\\n## Your feedback on collections\\n\\nWe're working on improving collections, so if you have any bugs, questions, or new features you'd like to see added, please post a message in the [dedicated discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/12).\",\n",
              "  'question': 'How can you modify the order of images within a Hugging Face dataset collection image gallery?',\n",
              "  'answer': 'To modify the order of images within a Hugging Face dataset collection image gallery, you can drag and drop the images to your desired position.',\n",
              "  'source_doc': 'huggingface/hub-docs/blob/main/docs/hub/collections.md'},\n",
              " {'context': 'with gr.Column(min_width=200):\\n            gr.Markdown(\"# ON:CHANGE\")\\n            text_ch = gr.Textbox()\\n            num_ch = gr.Number()\\n            slider_ch = gr.Slider()\\n            checkbox_ch = gr.Checkbox()\\n            checkbox_group_ch = gr.CheckboxGroup([\"a\", \"b\", \"c\"])\\n            radio_ch = gr.Radio([\"a\", \"b\", \"c\"])\\n            dropdown_ch = gr.Dropdown([\"a\", \"b\", \"c\"])\\n            colorpicker_ch = gr.ColorPicker()\\n            code_ch = gr.Code()\\n            dataframe_ch = gr.Dataframe()\\n            image_ch = gr.Image(elem_id=\"image-change\")\\n            audio_ch = gr.Audio(elem_id=\"audio-change\")\\n            video_ch = gr.Video(elem_id=\"video-change\")\\n\\n        with gr.Column(min_width=200):\\n            gr.Markdown(\"# ON:CHANGE x2\")\\n            text_ch2 = gr.Textbox()\\n            num_ch2 = gr.Number()\\n            slider_ch2 = gr.Slider()\\n            checkbox_ch2 = gr.Checkbox()\\n            checkbox_group_ch2 = gr.CheckboxGroup([\"a\", \"b\", \"c\"])\\n            radio_ch2 = gr.Radio([\"a\", \"b\", \"c\"])\\n            dropdown_ch2 = gr.Dropdown([\"a\", \"b\", \"c\"])\\n            colorpicker_ch2 = gr.ColorPicker()\\n            code_ch2 = gr.Code()\\n            dataframe_ch2 = gr.Dataframe()\\n            image_ch2 = gr.Image(elem_id=\"image-change-2\")\\n            audio_ch2 = gr.Audio(elem_id=\"audio-change-2\")\\n            video_ch2 = gr.Video(elem_id=\"video-change-2\")\\n\\n    counter = gr.Number(label=\"Change counter\")\\n\\n    lion = os.path.join(os.path.abspath(\\'\\'), \"files/lion.jpg\")\\n    cantina = os.path.join(os.path.abspath(\\'\\'), \"files/cantina.wav\")\\n    world = os.path.join(os.path.abspath(\\'\\'), \"files/world.mp4\")\\n\\n    set_button.click(\\n        lambda: [\"asdf\", 555, 12, True, [\"a\", \"c\"], \"b\", \"b\", \"#FF0000\", \"import gradio as gr\", [[\"a\", \"b\", \"c\", \"d\"], [\"1\", \"2\", \"3\", \"4\"]], lion, cantina, world], \\n        None, \\n        [text, num, slider, checkbox, checkbox_group, radio, dropdown, colorpicker, code, dataframe, image, audio, video])',\n",
              "  'question': 'How can you configure Gradio UI components like Textbox and Checkbox to trigger specific actions on their state changes?',\n",
              "  'answer': \"In Gradio, you can set up UI components like Textbox and Checkbox to trigger actions on their state changes by using the `on_change` event. You define a function that specifies what should happen when the component's state changes. Then, you attach this function to the `on_change` event of the component, allowing for dynamic interactions on user input modifications.\",\n",
              "  'source_doc': 'gradio-app/gradio/blob/main/demo/change_vs_input/run.ipynb'},\n",
              " {'context': 'gr.Dataset(\\n        components=[i],\\n        label=\"Image\",\\n        samples=[[img], [img], [img], [img], [img], [img]],\\n    )\\n    m = gr.Markdown(visible=False)\\n    gr.Dataset(\\n        components=[m],\\n        label=\"Markdown\",\\n        samples=[\\n            [\"# hi\"],\\n            [\"# hi\"],\\n            [\"# hi\"],\\n            [\"# hi\"],\\n            [\"# hi\"],\\n            [\"# hi\"],\\n        ],\\n    )\\n    m_2 = gr.Model3D(visible=False)\\n    gr.Dataset(\\n        components=[m_2],\\n        label=\"Model3D\",\\n        samples=[[model], [model], [model], [model], [model], [model]],\\n    )\\n    n = gr.Number(visible=False)\\n    gr.Dataset(\\n        label=\"Number\",\\n        components=[n],\\n        samples=[[1], [1], [1], [1], [1], [1]],\\n    )\\n    r = gr.Radio(visible=False, choices=[\"one\", \"two\", \"three\"])\\n    gr.Dataset(\\n        components=[r],\\n        label=\"Radio\",\\n        samples=[[\"one\"], [\"two\"], [\"three\"], [\"one\"], [\"two\"], [\"three\"]],\\n    )\\n    s = gr.Slider(visible=False)\\n    gr.Dataset(\\n        label=\"Slider\",\\n        components=[s],\\n        samples=[[1], [1], [1], [1], [1], [1]],\\n    )\\n    t = gr.Textbox(visible=False)\\n    gr.Dataset(\\n        label=\"Textbox\",\\n        components=[t],\\n        samples=[\\n            [\"Some value\"],\\n            [\"Some value\"],\\n            [\"Some value\"],\\n            [\"Some value\"],\\n            [\"Some value\"],\\n            [\"Some value\"],\\n        ],\\n    )\\n    v = gr.Video(visible=False)\\n    gr.Dataset(\\n        components=[v],\\n        label=\"Video\",\\n        samples=[[vid], [vid], [vid], [vid], [vid], [vid]],\\n    )',\n",
              "  'question': 'How can I use Gradio to create datasets with different types of components like images, markdown, and sliders?',\n",
              "  'answer': \"In Gradio, you can create datasets with different types of components by using the `gr.Dataset` class, passing the appropriate component type to the `components` parameter. For example, use `gr.Dataset(components=[gr.Image()])` for images, `gr.Dataset(components=[gr.Markdown()])` for markdown, and `gr.Dataset(components=[gr.Slider()])` for sliders. Each dataset can include a list of sample values corresponding to the component's type to showcase different input possibilities or responses.\",\n",
              "  'source_doc': 'gradio-app/gradio/blob/main/demo/dataset/run.ipynb'},\n",
              " {'context': 'def _info(self):\\n\\n    def _split_generators(self, dl_manager):\\n\\n    def _generate_examples(self, images, metadata_path):\\n```\\n\\n#### Multiple configurations\\n\\nIn some cases, a dataset may have more than one configuration. For example, if you check out the [Imagenette dataset](https://huggingface.co/datasets/frgfm/imagenette), you\\'ll notice there are three subsets. \\n\\nTo create different configurations, use the [`BuilderConfig`] class to create a subclass for your dataset. Provide the links to download the images and labels in `data_url` and `metadata_urls`:\\n\\n```py\\nclass Food101Config(datasets.BuilderConfig):\\n    \"\"\"Builder Config for Food-101\"\"\"\\n \\n    def __init__(self, data_url, metadata_urls, **kwargs):\\n        \"\"\"BuilderConfig for Food-101.\\n        Args:\\n          data_url: `string`, url to download the zip file from.\\n          metadata_urls: dictionary with keys \\'train\\' and \\'validation\\' containing the archive metadata URLs\\n          **kwargs: keyword arguments forwarded to super.\\n        \"\"\"\\n        super(Food101Config, self).__init__(version=datasets.Version(\"1.0.0\"), **kwargs)\\n        self.data_url = data_url\\n        self.metadata_urls = metadata_urls\\n```\\n\\nNow you can define your subsets at the top of [`GeneratorBasedBuilder`]. Imagine you want to create two subsets in the Food-101 dataset based on whether it is a breakfast or dinner food.\\n\\n1. Define your subsets with `Food101Config` in a list in `BUILDER_CONFIGS`.\\n2. For each configuration, provide a name, description, and where to download the images and labels from.',\n",
              "  'question': 'How do you create multiple configurations for a dataset using the Hugging Face `datasets` library, and what role does `BuilderConfig` play in this process?',\n",
              "  'answer': 'To create multiple configurations for a dataset using the Hugging Face `datasets` library, you need to use the `BuilderConfig` class to define different configurations. Each configuration represents a different subset of the dataset. First, create a subclass of `BuilderConfig` for your dataset, including parameters like `data_url` and `metadata_urls` to specify where to download the data. Then, define your configurations in the `BUILDER_CONFIGS` list of your `GeneratorBasedBuilder` subclass. Each configuration should include a unique name, description, and the corresponding URLs for downloading its data and metadata. This allows you to handle different versions or subsets of a dataset, such as different data splits or categorically distinct datasets.',\n",
              "  'source_doc': 'huggingface/datasets/blob/main/docs/source/image_dataset.mdx'},\n",
              " {'context': \"- The number of cores: although using as many cores as you have is often a good idea, it does not always provide the best performance because it also means more communication between the different threads. On top of that, having better performance with fewer cores can be very useful as it allows to run multiple instances at the same time, resulting in both better latency and throughput.\\n- The memory allocator: which memory allocator out of the default malloc, Google's tcmalloc and Facebook's jemalloc provides the best performance?\\n- The parallelism library: which parallelism library out of GNU OpenMP and Intel OpenMP provides the best performance?\\n- Transparent Huge Pages: does enabling Transparent Huge Pages (THP) on the system provide better performance?\\n- KMP block time parameter: sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping.\\n\\nOf course, the brute force approach, consisting of trying out all the possibilities will provide the best knob values to use to get optimal performance but, \\nthe size of the search space being `N x 3 x 2 x 2 x 2 = 24N`, it can take a lot of time: on a machine with 80 physical cores, this means trying out at most `24 x 80 = 1920` different setups! ğŸ˜±\\n\\nFortunately, Intel's [SigOpt](https://sigopt.com/), through Bayesian optimization, allows us to make these tuning experiments both faster and more convenient to analyse, while providing similar performance than the brute force approach.\\n\\nWhen we analyse the relative difference between the absolute best latency and what SigOpt provides, we observe that although it is often not as good as brute force (except for sequence length = 512 in that specific case),\\nit gives very close performance, with **8.6%** being the biggest gap on this figure.\",\n",
              "  'question': \"How can Intel's SigOpt be used to optimize performance settings for machine learning workloads on systems with multiple tuning parameters?\",\n",
              "  'answer': \"Intel's SigOpt can be employed to optimize performance settings by using Bayesian optimization to efficiently explore the parameter space and find near-optimal configurations. This approach significantly reduces the number of experiments compared to a brute force search. By tuning parameters such as the number of cores, memory allocator, parallelism library, Transparent Huge Pages, and KMP block time parameter, SigOpt helps achieve performance close to the best possible while analyzing fewer configurations, thus saving time and computational resources.\",\n",
              "  'source_doc': 'huggingface/blog/blob/main/bert-cpu-scaling-part-2.md'},\n",
              " {'context': \"!--âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\\nrendered properly in your Markdown viewer.\\n-->\\n\\n# Manage `huggingface_hub` cache-system\\n\\n## Understand caching\\n\\nThe Hugging Face Hub cache-system is designed to be the central cache shared across libraries\\nthat depend on the Hub. It has been updated in v0.8.0 to prevent re-downloading same files\\nbetween revisions.\\n\\nThe caching system is designed as follows:\\n\\n```\\n<CACHE_DIR>\\nâ”œâ”€ <MODELS>\\nâ”œâ”€ <DATASETS>\\nâ”œâ”€ <SPACES>\\n```\\n\\nThe `<CACHE_DIR>` is usually your user's home directory. However, it is customizable with the `cache_dir` argument on all methods, or by specifying either `HF_HOME` or `HF_HUB_CACHE` environment variable.\\n\\nModels, datasets and spaces share a common root. Each of these repositories contains the\\nrepository type, the namespace (organization or username) if it exists and the\\nrepository name:\\n\\n```\\n<CACHE_DIR>\\nâ”œâ”€ models--julien-c--EsperBERTo-small\\nâ”œâ”€ models--lysandrejik--arxiv-nlp\\nâ”œâ”€ models--bert-base-cased\\nâ”œâ”€ datasets--glue\\nâ”œâ”€ datasets--huggingface--DataMeasurementsFiles\\nâ”œâ”€ spaces--dalle-mini--dalle-mini\\n```\\n\\nIt is within these folders that all files will now be downloaded from the Hub. Caching ensures that\\na file isn't downloaded twice if it already exists and wasn't updated; but if it was updated,\\nand you're asking for the latest file, then it will download the latest file (while keeping\\nthe previous file intact in case you need it again).\\n\\nIn order to achieve this, all folders contain the same skeleton:\\n\\n```\\n<CACHE_DIR>\\nâ”œâ”€ datasets--glue\\nâ”‚  â”œâ”€ refs\\nâ”‚  â”œâ”€ blobs\\nâ”‚  â”œâ”€ snapshots\\n...\\n```\\n\\nEach folder is designed to contain the following:\\n\\n### Refs\",\n",
              "  'question': 'How can you customize the cache directory location for the Hugging Face Hub cache-system?',\n",
              "  'answer': 'You can customize the cache directory location for the Hugging Face Hub cache-system by using the `cache_dir` argument in methods or by setting the `HF_HOME` or `HF_HUB_CACHE` environment variable.',\n",
              "  'source_doc': 'huggingface/huggingface_hub/blob/main/docs/source/en/guides/manage-cache.md'}]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 899
        },
        "id": "aUlOUDv59jVN",
        "outputId": "67a03650-070b-45df-c30f-3dd64f7a1d7b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"`max_length`: the maximum sequence length (default value is `128`).\\n\\n`device`: either \\\"gpu\\\" or \\\"cpu\\\" (default value is `None`). \\n\\n```python\\n>>> results = frugalscore.compute(predictions=['hello there', 'huggingface'], references=['hello world', 'hugging face'], batch_size=16, max_length=64, device=\\\"gpu\\\")\\n```\\n\\n## Output values\\n\\nThe output of FrugalScore is a dictionary with the list of scores for each prediction-reference pair:\\n```python\\n{'scores': [0.6307541, 0.6449357]}\\n```\\n\\n### Values from popular papers\\nThe [original FrugalScore paper](https://arxiv.org/abs/2110.08559) reported that FrugalScore-Tiny retains 97.7/94.7% of the original performance compared to [BertScore](https://huggingface.co/metrics/bertscore) while running 54 times faster and having 84 times less parameters.\\n\\n## Examples \\n\\nMaximal values (exact match between `references` and `predictions`): \\n\\n```python\\n>>> frugalscore = evaluate.load(\\\"frugalscore\\\")\\n>>> results = frugalscore.compute(predictions=['hello world'], references=['hello world'])\\n>>> print(results)\\n{'scores': [0.9891098]}\\n```\\n\\nPartial values: \\n\\n```python\\n>>> frugalscore = evaluate.load(\\\"frugalscore\\\")\\n>>> results = frugalscore.compute(predictions=['hello world'], references=['hugging face'])\\n>>> print(results)\\n{'scores': [0.42482382]}\\n```\\n\\n## Limitations and bias\\n\\nFrugalScore is based on [BertScore](https://huggingface.co/metrics/bertscore) and [MoverScore](https://arxiv.org/abs/1909.02622), and the models used are based on the original models used for these scores.\\n\\nThe full list of available models for FrugalScore is:\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"How can you specify the maximum sequence length and device type when computing FrugalScore in Hugging Face?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"When computing FrugalScore using the `frugalscore.compute()` method, you can specify the maximum sequence length by setting the `max_length` parameter, and you can choose the device type by setting the `device` parameter to either \\\"gpu\\\" or \\\"cpu\\\". For example: `frugalscore.compute(predictions=['hello there'], references=['hello world'], max_length=64, device=\\\"gpu\\\")`.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source_doc\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"huggingface/evaluate/blob/main/metrics/frugalscore/README.md\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-cb34a5af-3464-43f6-8cf3-42335da567aa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>source_doc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>`max_length`: the maximum sequence length (default value is `128`).\\n\\n`device`: either \"gpu\" or \"cpu\" (default value is `None`). \\n\\n```python\\n&gt;&gt;&gt; results = frugalscore.compute(predictions=['hello there', 'huggingface'], references=['hello world', 'hugging face'], batch_size=16, max_length=64, device=\"gpu\")\\n```\\n\\n## Output values\\n\\nThe output of FrugalScore is a dictionary with the list of scores for each prediction-reference pair:\\n```python\\n{'scores': [0.6307541, 0.6449357]}\\n```\\n\\n### Values from popular papers\\nThe [original FrugalScore paper](https://arxiv.org/abs/2110.08559) reported that FrugalScore-Tiny retains 97.7/94.7% of the original performance compared to [BertScore](https://huggingface.co/metrics/bertscore) while running 54 times faster and having 84 times less parameters.\\n\\n## Examples \\n\\nMaximal values (exact match between `references` and `predictions`): \\n\\n```python\\n&gt;&gt;&gt; frugalscore = evaluate.load(\"frugalscore\")\\n&gt;&gt;&gt; results = frugalscore.compute(predictions=['hello world'], references=['hello world'])\\n&gt;&gt;&gt; print(results)\\n{'scores': [0.9891098]}\\n```\\n\\nPartial values: \\n\\n```python\\n&gt;&gt;&gt; frugalscore = evaluate.load(\"frugalscore\")\\n&gt;&gt;&gt; results = frugalscore.compute(predictions=['hello world'], references=['hugging face'])\\n&gt;&gt;&gt; print(results)\\n{'scores': [0.42482382]}\\n```\\n\\n## Limitations and bias\\n\\nFrugalScore is based on [BertScore](https://huggingface.co/metrics/bertscore) and [MoverScore](https://arxiv.org/abs/1909.02622), and the models used are based on the original models used for these scores.\\n\\nThe full list of available models for FrugalScore is:</td>\n",
              "      <td>How can you specify the maximum sequence length and device type when computing FrugalScore in Hugging Face?</td>\n",
              "      <td>When computing FrugalScore using the `frugalscore.compute()` method, you can specify the maximum sequence length by setting the `max_length` parameter, and you can choose the device type by setting the `device` parameter to either \"gpu\" or \"cpu\". For example: `frugalscore.compute(predictions=['hello there'], references=['hello world'], max_length=64, device=\"gpu\")`.</td>\n",
              "      <td>huggingface/evaluate/blob/main/metrics/frugalscore/README.md</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb34a5af-3464-43f6-8cf3-42335da567aa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-cb34a5af-3464-43f6-8cf3-42335da567aa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-cb34a5af-3464-43f6-8cf3-42335da567aa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           context  \\\n",
              "0  `max_length`: the maximum sequence length (default value is `128`).\\n\\n`device`: either \"gpu\" or \"cpu\" (default value is `None`). \\n\\n```python\\n>>> results = frugalscore.compute(predictions=['hello there', 'huggingface'], references=['hello world', 'hugging face'], batch_size=16, max_length=64, device=\"gpu\")\\n```\\n\\n## Output values\\n\\nThe output of FrugalScore is a dictionary with the list of scores for each prediction-reference pair:\\n```python\\n{'scores': [0.6307541, 0.6449357]}\\n```\\n\\n### Values from popular papers\\nThe [original FrugalScore paper](https://arxiv.org/abs/2110.08559) reported that FrugalScore-Tiny retains 97.7/94.7% of the original performance compared to [BertScore](https://huggingface.co/metrics/bertscore) while running 54 times faster and having 84 times less parameters.\\n\\n## Examples \\n\\nMaximal values (exact match between `references` and `predictions`): \\n\\n```python\\n>>> frugalscore = evaluate.load(\"frugalscore\")\\n>>> results = frugalscore.compute(predictions=['hello world'], references=['hello world'])\\n>>> print(results)\\n{'scores': [0.9891098]}\\n```\\n\\nPartial values: \\n\\n```python\\n>>> frugalscore = evaluate.load(\"frugalscore\")\\n>>> results = frugalscore.compute(predictions=['hello world'], references=['hugging face'])\\n>>> print(results)\\n{'scores': [0.42482382]}\\n```\\n\\n## Limitations and bias\\n\\nFrugalScore is based on [BertScore](https://huggingface.co/metrics/bertscore) and [MoverScore](https://arxiv.org/abs/1909.02622), and the models used are based on the original models used for these scores.\\n\\nThe full list of available models for FrugalScore is:   \n",
              "\n",
              "                                                                                                      question  \\\n",
              "0  How can you specify the maximum sequence length and device type when computing FrugalScore in Hugging Face?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                             answer  \\\n",
              "0  When computing FrugalScore using the `frugalscore.compute()` method, you can specify the maximum sequence length by setting the `max_length` parameter, and you can choose the device type by setting the `device` parameter to either \"gpu\" or \"cpu\". For example: `frugalscore.compute(predictions=['hello there'], references=['hello world'], max_length=64, device=\"gpu\")`.   \n",
              "\n",
              "                                                     source_doc  \n",
              "0  huggingface/evaluate/blob/main/metrics/frugalscore/README.md  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(pd.DataFrame(outputs).head(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KG4dNtg9jVN"
      },
      "source": [
        "### 1.3. Setup critique agents\n",
        "\n",
        "The questions generated by the previous agent can have many flaws: we should do a quality check before validating these questions.\n",
        "\n",
        "We thus build critique agents that will rate each question on several criteria, given in [this paper](https://huggingface.co/papers/2312.10003):\n",
        "- **Groundedness:** can the question be answered from the given context?\n",
        "- **Relevance:** is the question relevant to users? For instance, `\"What is the date when transformers 4.29.1 was released?\"` is not relevant for ML practitioners.\n",
        "\n",
        "One last failure case we've noticed is when a function is tailored for the particular setting where the question was generated, but undecipherable by itself, like `\"What is the name of the function used in this guide?\"`.\n",
        "We also build a critique agent for this criteria:\n",
        "- **Stand-alone**: is the question understandable free of any context, for someone with domain knowledge/Internet access? The opposite of this would be `What is the function used in this article?` for a question generated from a specific blog article.\n",
        "\n",
        "We systematically score functions with all these agents, and whenever the score is too low for any one of the agents, we eliminate the question from our eval dataset.\n",
        "\n",
        "ğŸ’¡ ___When asking the agents to output a score, we first ask them to produce its rationale. This will help us verify scores, but most importantly, asking it to first output rationale gives the model more tokens to think and elaborate an answer before summarizing it into a single score token.___\n",
        "\n",
        "We now build and run these critique agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "05aSgTGs9jVO"
      },
      "outputs": [],
      "source": [
        "question_groundedness_critique_prompt = \"\"\"\n",
        "You will be given a context and a question.\n",
        "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here are the question and context.\n",
        "\n",
        "Question: {question}\\n\n",
        "Context: {context}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_relevance_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\"\n",
        "\n",
        "question_standalone_critique_prompt = \"\"\"\n",
        "You will be given a question.\n",
        "Your task is to provide a 'total rating' representing how context-independent this question is.\n",
        "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
        "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
        "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
        "\n",
        "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independent from the context.\n",
        "\n",
        "Provide your answer as follows:\n",
        "\n",
        "Answer:::\n",
        "Evaluation: (your rationale for the rating, as a text)\n",
        "Total rating: (your rating, as a number between 1 and 5)\n",
        "\n",
        "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
        "\n",
        "Now here is the question.\n",
        "\n",
        "Question: {question}\\n\n",
        "Answer::: \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9tbk7ME9jVO",
        "outputId": "74b18dc2-1484-42ef-9a8e-62fb401ba8a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating critique for each QA couple...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [01:22<00:00,  8.21s/it]\n"
          ]
        }
      ],
      "source": [
        "print(\"Generating critique for each QA couple...\")\n",
        "for output in tqdm(outputs):\n",
        "    evaluations = {\n",
        "        \"groundedness\": call_llm(\n",
        "            llm_client,\n",
        "            question_groundedness_critique_prompt.format(\n",
        "                context=output[\"context\"], question=output[\"question\"]\n",
        "            ),\n",
        "        ),\n",
        "        \"relevance\": call_llm(\n",
        "            llm_client,\n",
        "            question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
        "        ),\n",
        "        \"standalone\": call_llm(\n",
        "            llm_client,\n",
        "            question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
        "        ),\n",
        "    }\n",
        "    try:\n",
        "        for criterion, evaluation in evaluations.items():\n",
        "            score, eval = (\n",
        "                int(evaluation.split(\"Total rating: \")[-1].strip()),\n",
        "                evaluation.split(\"Total rating: \")[-2].split(\"Evaluation: \")[1],\n",
        "            )\n",
        "            output.update(\n",
        "                {\n",
        "                    f\"{criterion}_score\": score,\n",
        "                    f\"{criterion}_eval\": eval,\n",
        "                }\n",
        "            )\n",
        "    except Exception as e:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQv36Y_f9jVO"
      },
      "source": [
        "Now let us filter out bad questions based on our critique agent scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oBWuOu1b9jVO",
        "outputId": "28186354-1db1-4629-c6f5-0d8d2b67f8f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation dataset before filtering:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \")\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"How can Intel's SigOpt be used to optimize performance settings for machine learning workloads on systems with multiple tuning parameters?\",\n          \"How can you compute accuracy with normalization turned off using the Hugging Face Evaluate library?\",\n          \"How can you configure Gradio UI components like Textbox and Checkbox to trigger specific actions on their state changes?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Intel's SigOpt can be employed to optimize performance settings by using Bayesian optimization to efficiently explore the parameter space and find near-optimal configurations. This approach significantly reduces the number of experiments compared to a brute force search. By tuning parameters such as the number of cores, memory allocator, parallelism library, Transparent Huge Pages, and KMP block time parameter, SigOpt helps achieve performance close to the best possible while analyzing fewer configurations, thus saving time and computational resources.\",\n          \"In the Hugging Face Evaluate library, you can compute accuracy with normalization turned off by setting the `normalize` parameter to `False` when calling the `compute` method on an accuracy metric object. For example:\\n\\n```python\\naccuracy_metric = evaluate.load(\\\"accuracy\\\")\\nresults = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\\nprint(results)\\n```\\n\\nThis will output the sum of correctly predicted samples instead of the proportion of correct predictions over total samples.\",\n          \"In Gradio, you can set up UI components like Textbox and Checkbox to trigger actions on their state changes by using the `on_change` event. You define a function that specifies what should happen when the component's state changes. Then, you attach this function to the `on_change` event of the component, allowing for dynamic interactions on user input modifications.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"groundedness_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 2,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          1,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"standalone_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c0fda4e2-b32d-4cf9-8df3-0534d766b583\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How can you specify the maximum sequence length and device type when computing FrugalScore in Hugging Face?</td>\n",
              "      <td>When computing FrugalScore using the `frugalscore.compute()` method, you can specify the maximum sequence length by setting the `max_length` parameter, and you can choose the device type by setting the `device` parameter to either \"gpu\" or \"cpu\". For example: `frugalscore.compute(predictions=['hello there'], references=['hello world'], max_length=64, device=\"gpu\")`.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How can you compute accuracy with normalization turned off using the Hugging Face Evaluate library?</td>\n",
              "      <td>In the Hugging Face Evaluate library, you can compute accuracy with normalization turned off by setting the `normalize` parameter to `False` when calling the `compute` method on an accuracy metric object. For example:\\n\\n```python\\naccuracy_metric = evaluate.load(\"accuracy\")\\nresults = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\\nprint(results)\\n```\\n\\nThis will output the sum of correctly predicted samples instead of the proportion of correct predictions over total samples.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How can NeRF-to-mesh conversion be applied, and what are its limitations in creating game-ready 3D assets?</td>\n",
              "      <td>NeRF-to-mesh conversion can be applied by using tools like NVlabs' instant-ngp, which facilitate transforming neural radiance fields (NeRFs) into mesh representations. However, the resulting meshes are not immediately game-ready and require significant manual post-processing and expertise, similar to photogrammetry techniques, to become usable in game development. Consequently, while NeRF-to-mesh can create high-fidelity assets, it's often not practical for quick and straightforward game asset creation.</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do you use datasets-cli to automatically generate feature types and sizes for a dataset in the Hugging Face Datasets library?</td>\n",
              "      <td>To automatically generate feature types and sizes for a dataset with datasets-cli in the Hugging Face Datasets library, you can use the `datasets-cli test` command followed by the directory containing your dataset's loading script. This command will inspect your dataset and create a README.md with metadata, including feature types, dataset size, and number of examples for each split.</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can you modify the order of images within a Hugging Face dataset collection image gallery?</td>\n",
              "      <td>To modify the order of images within a Hugging Face dataset collection image gallery, you can drag and drop the images to your desired position.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How can you configure Gradio UI components like Textbox and Checkbox to trigger specific actions on their state changes?</td>\n",
              "      <td>In Gradio, you can set up UI components like Textbox and Checkbox to trigger actions on their state changes by using the `on_change` event. You define a function that specifies what should happen when the component's state changes. Then, you attach this function to the `on_change` event of the component, allowing for dynamic interactions on user input modifications.</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How can I use Gradio to create datasets with different types of components like images, markdown, and sliders?</td>\n",
              "      <td>In Gradio, you can create datasets with different types of components by using the `gr.Dataset` class, passing the appropriate component type to the `components` parameter. For example, use `gr.Dataset(components=[gr.Image()])` for images, `gr.Dataset(components=[gr.Markdown()])` for markdown, and `gr.Dataset(components=[gr.Slider()])` for sliders. Each dataset can include a list of sample values corresponding to the component's type to showcase different input possibilities or responses.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How do you create multiple configurations for a dataset using the Hugging Face `datasets` library, and what role does `BuilderConfig` play in this process?</td>\n",
              "      <td>To create multiple configurations for a dataset using the Hugging Face `datasets` library, you need to use the `BuilderConfig` class to define different configurations. Each configuration represents a different subset of the dataset. First, create a subclass of `BuilderConfig` for your dataset, including parameters like `data_url` and `metadata_urls` to specify where to download the data. Then, define your configurations in the `BUILDER_CONFIGS` list of your `GeneratorBasedBuilder` subclass. Each configuration should include a unique name, description, and the corresponding URLs for downloading its data and metadata. This allows you to handle different versions or subsets of a dataset, such as different data splits or categorically distinct datasets.</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How can Intel's SigOpt be used to optimize performance settings for machine learning workloads on systems with multiple tuning parameters?</td>\n",
              "      <td>Intel's SigOpt can be employed to optimize performance settings by using Bayesian optimization to efficiently explore the parameter space and find near-optimal configurations. This approach significantly reduces the number of experiments compared to a brute force search. By tuning parameters such as the number of cores, memory allocator, parallelism library, Transparent Huge Pages, and KMP block time parameter, SigOpt helps achieve performance close to the best possible while analyzing fewer configurations, thus saving time and computational resources.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How can you customize the cache directory location for the Hugging Face Hub cache-system?</td>\n",
              "      <td>You can customize the cache directory location for the Hugging Face Hub cache-system by using the `cache_dir` argument in methods or by setting the `HF_HOME` or `HF_HUB_CACHE` environment variable.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0fda4e2-b32d-4cf9-8df3-0534d766b583')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c0fda4e2-b32d-4cf9-8df3-0534d766b583 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c0fda4e2-b32d-4cf9-8df3-0534d766b583');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                                                                                                      question  \\\n",
              "0                                                  How can you specify the maximum sequence length and device type when computing FrugalScore in Hugging Face?   \n",
              "1                                                          How can you compute accuracy with normalization turned off using the Hugging Face Evaluate library?   \n",
              "2                                                   How can NeRF-to-mesh conversion be applied, and what are its limitations in creating game-ready 3D assets?   \n",
              "3                            How do you use datasets-cli to automatically generate feature types and sizes for a dataset in the Hugging Face Datasets library?   \n",
              "4                                                               How can you modify the order of images within a Hugging Face dataset collection image gallery?   \n",
              "5                                     How can you configure Gradio UI components like Textbox and Checkbox to trigger specific actions on their state changes?   \n",
              "6                                               How can I use Gradio to create datasets with different types of components like images, markdown, and sliders?   \n",
              "7  How do you create multiple configurations for a dataset using the Hugging Face `datasets` library, and what role does `BuilderConfig` play in this process?   \n",
              "8                   How can Intel's SigOpt be used to optimize performance settings for machine learning workloads on systems with multiple tuning parameters?   \n",
              "9                                                                    How can you customize the cache directory location for the Hugging Face Hub cache-system?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                          When computing FrugalScore using the `frugalscore.compute()` method, you can specify the maximum sequence length by setting the `max_length` parameter, and you can choose the device type by setting the `device` parameter to either \"gpu\" or \"cpu\". For example: `frugalscore.compute(predictions=['hello there'], references=['hello world'], max_length=64, device=\"gpu\")`.   \n",
              "1                                                                                                                                                                                                                               In the Hugging Face Evaluate library, you can compute accuracy with normalization turned off by setting the `normalize` parameter to `False` when calling the `compute` method on an accuracy metric object. For example:\\n\\n```python\\naccuracy_metric = evaluate.load(\"accuracy\")\\nresults = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\\nprint(results)\\n```\\n\\nThis will output the sum of correctly predicted samples instead of the proportion of correct predictions over total samples.   \n",
              "2                                                                                                                                                                                                                                                              NeRF-to-mesh conversion can be applied by using tools like NVlabs' instant-ngp, which facilitate transforming neural radiance fields (NeRFs) into mesh representations. However, the resulting meshes are not immediately game-ready and require significant manual post-processing and expertise, similar to photogrammetry techniques, to become usable in game development. Consequently, while NeRF-to-mesh can create high-fidelity assets, it's often not practical for quick and straightforward game asset creation.   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                        To automatically generate feature types and sizes for a dataset with datasets-cli in the Hugging Face Datasets library, you can use the `datasets-cli test` command followed by the directory containing your dataset's loading script. This command will inspect your dataset and create a README.md with metadata, including feature types, dataset size, and number of examples for each split.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          To modify the order of images within a Hugging Face dataset collection image gallery, you can drag and drop the images to your desired position.   \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                          In Gradio, you can set up UI components like Textbox and Checkbox to trigger actions on their state changes by using the `on_change` event. You define a function that specifies what should happen when the component's state changes. Then, you attach this function to the `on_change` event of the component, allowing for dynamic interactions on user input modifications.   \n",
              "6                                                                                                                                                                                                                                                                             In Gradio, you can create datasets with different types of components by using the `gr.Dataset` class, passing the appropriate component type to the `components` parameter. For example, use `gr.Dataset(components=[gr.Image()])` for images, `gr.Dataset(components=[gr.Markdown()])` for markdown, and `gr.Dataset(components=[gr.Slider()])` for sliders. Each dataset can include a list of sample values corresponding to the component's type to showcase different input possibilities or responses.   \n",
              "7  To create multiple configurations for a dataset using the Hugging Face `datasets` library, you need to use the `BuilderConfig` class to define different configurations. Each configuration represents a different subset of the dataset. First, create a subclass of `BuilderConfig` for your dataset, including parameters like `data_url` and `metadata_urls` to specify where to download the data. Then, define your configurations in the `BUILDER_CONFIGS` list of your `GeneratorBasedBuilder` subclass. Each configuration should include a unique name, description, and the corresponding URLs for downloading its data and metadata. This allows you to handle different versions or subsets of a dataset, such as different data splits or categorically distinct datasets.   \n",
              "8                                                                                                                                                                                                            Intel's SigOpt can be employed to optimize performance settings by using Bayesian optimization to efficiently explore the parameter space and find near-optimal configurations. This approach significantly reduces the number of experiments compared to a brute force search. By tuning parameters such as the number of cores, memory allocator, parallelism library, Transparent Huge Pages, and KMP block time parameter, SigOpt helps achieve performance close to the best possible while analyzing fewer configurations, thus saving time and computational resources.   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     You can customize the cache directory location for the Hugging Face Hub cache-system by using the `cache_dir` argument in methods or by setting the `HF_HOME` or `HF_HUB_CACHE` environment variable.   \n",
              "\n",
              "   groundedness_score  relevance_score  standalone_score  \n",
              "0                   5                4                 4  \n",
              "1                   5                4                 5  \n",
              "2                   4                1                 5  \n",
              "3                   2                4                 4  \n",
              "4                   5                4                 5  \n",
              "5                   2                4                 5  \n",
              "6                   5                4                 5  \n",
              "7                   5                5                 5  \n",
              "8                   5                4                 5  \n",
              "9                   5                4                 5  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================\n",
            "Final evaluation dataset:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \")\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"How can you specify the maximum sequence length and device type when computing FrugalScore in Hugging Face?\",\n          \"How can you compute accuracy with normalization turned off using the Hugging Face Evaluate library?\",\n          \"How can Intel's SigOpt be used to optimize performance settings for machine learning workloads on systems with multiple tuning parameters?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"When computing FrugalScore using the `frugalscore.compute()` method, you can specify the maximum sequence length by setting the `max_length` parameter, and you can choose the device type by setting the `device` parameter to either \\\"gpu\\\" or \\\"cpu\\\". For example: `frugalscore.compute(predictions=['hello there'], references=['hello world'], max_length=64, device=\\\"gpu\\\")`.\",\n          \"In the Hugging Face Evaluate library, you can compute accuracy with normalization turned off by setting the `normalize` parameter to `False` when calling the `compute` method on an accuracy metric object. For example:\\n\\n```python\\naccuracy_metric = evaluate.load(\\\"accuracy\\\")\\nresults = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\\nprint(results)\\n```\\n\\nThis will output the sum of correctly predicted samples instead of the proportion of correct predictions over total samples.\",\n          \"Intel's SigOpt can be employed to optimize performance settings by using Bayesian optimization to efficiently explore the parameter space and find near-optimal configurations. This approach significantly reduces the number of experiments compared to a brute force search. By tuning parameters such as the number of cores, memory allocator, parallelism library, Transparent Huge Pages, and KMP block time parameter, SigOpt helps achieve performance close to the best possible while analyzing fewer configurations, thus saving time and computational resources.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"groundedness_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"standalone_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 4,\n        \"max\": 5,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-763141f5-e883-4298-98aa-01fb21fa76ab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>groundedness_score</th>\n",
              "      <th>relevance_score</th>\n",
              "      <th>standalone_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How can you specify the maximum sequence length and device type when computing FrugalScore in Hugging Face?</td>\n",
              "      <td>When computing FrugalScore using the `frugalscore.compute()` method, you can specify the maximum sequence length by setting the `max_length` parameter, and you can choose the device type by setting the `device` parameter to either \"gpu\" or \"cpu\". For example: `frugalscore.compute(predictions=['hello there'], references=['hello world'], max_length=64, device=\"gpu\")`.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How can you compute accuracy with normalization turned off using the Hugging Face Evaluate library?</td>\n",
              "      <td>In the Hugging Face Evaluate library, you can compute accuracy with normalization turned off by setting the `normalize` parameter to `False` when calling the `compute` method on an accuracy metric object. For example:\\n\\n```python\\naccuracy_metric = evaluate.load(\"accuracy\")\\nresults = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\\nprint(results)\\n```\\n\\nThis will output the sum of correctly predicted samples instead of the proportion of correct predictions over total samples.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can you modify the order of images within a Hugging Face dataset collection image gallery?</td>\n",
              "      <td>To modify the order of images within a Hugging Face dataset collection image gallery, you can drag and drop the images to your desired position.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How can I use Gradio to create datasets with different types of components like images, markdown, and sliders?</td>\n",
              "      <td>In Gradio, you can create datasets with different types of components by using the `gr.Dataset` class, passing the appropriate component type to the `components` parameter. For example, use `gr.Dataset(components=[gr.Image()])` for images, `gr.Dataset(components=[gr.Markdown()])` for markdown, and `gr.Dataset(components=[gr.Slider()])` for sliders. Each dataset can include a list of sample values corresponding to the component's type to showcase different input possibilities or responses.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How do you create multiple configurations for a dataset using the Hugging Face `datasets` library, and what role does `BuilderConfig` play in this process?</td>\n",
              "      <td>To create multiple configurations for a dataset using the Hugging Face `datasets` library, you need to use the `BuilderConfig` class to define different configurations. Each configuration represents a different subset of the dataset. First, create a subclass of `BuilderConfig` for your dataset, including parameters like `data_url` and `metadata_urls` to specify where to download the data. Then, define your configurations in the `BUILDER_CONFIGS` list of your `GeneratorBasedBuilder` subclass. Each configuration should include a unique name, description, and the corresponding URLs for downloading its data and metadata. This allows you to handle different versions or subsets of a dataset, such as different data splits or categorically distinct datasets.</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How can Intel's SigOpt be used to optimize performance settings for machine learning workloads on systems with multiple tuning parameters?</td>\n",
              "      <td>Intel's SigOpt can be employed to optimize performance settings by using Bayesian optimization to efficiently explore the parameter space and find near-optimal configurations. This approach significantly reduces the number of experiments compared to a brute force search. By tuning parameters such as the number of cores, memory allocator, parallelism library, Transparent Huge Pages, and KMP block time parameter, SigOpt helps achieve performance close to the best possible while analyzing fewer configurations, thus saving time and computational resources.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How can you customize the cache directory location for the Hugging Face Hub cache-system?</td>\n",
              "      <td>You can customize the cache directory location for the Hugging Face Hub cache-system by using the `cache_dir` argument in methods or by setting the `HF_HOME` or `HF_HUB_CACHE` environment variable.</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-763141f5-e883-4298-98aa-01fb21fa76ab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-763141f5-e883-4298-98aa-01fb21fa76ab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-763141f5-e883-4298-98aa-01fb21fa76ab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                                                                                                                      question  \\\n",
              "0                                                  How can you specify the maximum sequence length and device type when computing FrugalScore in Hugging Face?   \n",
              "1                                                          How can you compute accuracy with normalization turned off using the Hugging Face Evaluate library?   \n",
              "4                                                               How can you modify the order of images within a Hugging Face dataset collection image gallery?   \n",
              "6                                               How can I use Gradio to create datasets with different types of components like images, markdown, and sliders?   \n",
              "7  How do you create multiple configurations for a dataset using the Hugging Face `datasets` library, and what role does `BuilderConfig` play in this process?   \n",
              "8                   How can Intel's SigOpt be used to optimize performance settings for machine learning workloads on systems with multiple tuning parameters?   \n",
              "9                                                                    How can you customize the cache directory location for the Hugging Face Hub cache-system?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                                                          When computing FrugalScore using the `frugalscore.compute()` method, you can specify the maximum sequence length by setting the `max_length` parameter, and you can choose the device type by setting the `device` parameter to either \"gpu\" or \"cpu\". For example: `frugalscore.compute(predictions=['hello there'], references=['hello world'], max_length=64, device=\"gpu\")`.   \n",
              "1                                                                                                                                                                                                                               In the Hugging Face Evaluate library, you can compute accuracy with normalization turned off by setting the `normalize` parameter to `False` when calling the `compute` method on an accuracy metric object. For example:\\n\\n```python\\naccuracy_metric = evaluate.load(\"accuracy\")\\nresults = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 1, 2, 1, 0], normalize=False)\\nprint(results)\\n```\\n\\nThis will output the sum of correctly predicted samples instead of the proportion of correct predictions over total samples.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          To modify the order of images within a Hugging Face dataset collection image gallery, you can drag and drop the images to your desired position.   \n",
              "6                                                                                                                                                                                                                                                                             In Gradio, you can create datasets with different types of components by using the `gr.Dataset` class, passing the appropriate component type to the `components` parameter. For example, use `gr.Dataset(components=[gr.Image()])` for images, `gr.Dataset(components=[gr.Markdown()])` for markdown, and `gr.Dataset(components=[gr.Slider()])` for sliders. Each dataset can include a list of sample values corresponding to the component's type to showcase different input possibilities or responses.   \n",
              "7  To create multiple configurations for a dataset using the Hugging Face `datasets` library, you need to use the `BuilderConfig` class to define different configurations. Each configuration represents a different subset of the dataset. First, create a subclass of `BuilderConfig` for your dataset, including parameters like `data_url` and `metadata_urls` to specify where to download the data. Then, define your configurations in the `BUILDER_CONFIGS` list of your `GeneratorBasedBuilder` subclass. Each configuration should include a unique name, description, and the corresponding URLs for downloading its data and metadata. This allows you to handle different versions or subsets of a dataset, such as different data splits or categorically distinct datasets.   \n",
              "8                                                                                                                                                                                                            Intel's SigOpt can be employed to optimize performance settings by using Bayesian optimization to efficiently explore the parameter space and find near-optimal configurations. This approach significantly reduces the number of experiments compared to a brute force search. By tuning parameters such as the number of cores, memory allocator, parallelism library, Transparent Huge Pages, and KMP block time parameter, SigOpt helps achieve performance close to the best possible while analyzing fewer configurations, thus saving time and computational resources.   \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     You can customize the cache directory location for the Hugging Face Hub cache-system by using the `cache_dir` argument in methods or by setting the `HF_HOME` or `HF_HUB_CACHE` environment variable.   \n",
              "\n",
              "   groundedness_score  relevance_score  standalone_score  \n",
              "0                   5                4                 4  \n",
              "1                   5                4                 5  \n",
              "4                   5                4                 5  \n",
              "6                   5                4                 5  \n",
              "7                   5                5                 5  \n",
              "8                   5                4                 5  \n",
              "9                   5                4                 5  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "generated_questions = pd.DataFrame.from_dict(outputs)\n",
        "\n",
        "print(\"Evaluation dataset before filtering:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "generated_questions = generated_questions.loc[\n",
        "    (generated_questions[\"groundedness_score\"] >= 4)\n",
        "    & (generated_questions[\"relevance_score\"] >= 4)\n",
        "    & (generated_questions[\"standalone_score\"] >= 4)\n",
        "]\n",
        "print(\"============================================\")\n",
        "print(\"Final evaluation dataset:\")\n",
        "display(\n",
        "    generated_questions[\n",
        "        [\n",
        "            \"question\",\n",
        "            \"answer\",\n",
        "            \"groundedness_score\",\n",
        "            \"relevance_score\",\n",
        "            \"standalone_score\",\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "eval_dataset = datasets.Dataset.from_pandas(\n",
        "    generated_questions, split=\"train\", preserve_index=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaOMZyu69jVO"
      },
      "source": [
        "Now our synthetic evaluation dataset is complete! We can evaluate different RAG systems on this evaluation dataset.\n",
        "\n",
        "We have generated only a few QA couples here to reduce time and cost. But let's kickstart the next part by loading a pre-generated dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "65169bba7d68420d930d35d9f16e5134",
            "ac1e699071bd4911bd11953908ce52e9",
            "b58548a24aca4deb844f82b72ae2a07d",
            "a4266f0fe3364e1a81af6fabdbb501e9",
            "343c453e6d714a40a8a2d374f2ae6c50",
            "682f8f838a6947deaf3b1bee9bd710ab",
            "10692fe379bc42c388ff2c9925353509",
            "feda2e697dd34347a5270ad1301f004e",
            "d7f1e475833c4c0283be3828b4100ffd",
            "c7a1be638b1246089d9ac6a59b95731f",
            "83b0c5d46d9b43a1904fe94dd118e7df",
            "c70325a0dfb04bca9cb5e1ee67a52d43",
            "c92acbd353e546a6b1019593872a39cd",
            "6c2de4d74f0f4dcdabacbbf3c5b47249",
            "49e9d05c60474b5bbc4f579835129b7c",
            "23d170b784ec4d51a2744b5fa7b5319d",
            "2013c1159a854416bf7ba6ef5b965e87",
            "945a0ef0f62b409fb58f1fd452c49a8b",
            "fc30b7a9bf0e43ffb4c8db27c2f7f1dd",
            "b5923aa824954744866fd9ed4d3a8c9f",
            "ea30e2eb024141b08129373fe95e12a6",
            "662aca1919c249f88ac4656e47003b1f",
            "349210cb00cd4393ae57461ceba242fe",
            "5ac84713c78b4bceab1e309856e9933a",
            "48ed20807e9b4d58a3d3d08521dc258a",
            "d8b654f4d7494882bd12c5327612b20f",
            "b67bd32e58b243d390cdb28abb3a488c",
            "19027954ef554e219de962244b0bfcaa",
            "d37fc3039b88449397deea32f286b215",
            "d6a8d5ce2f664eb5a4f3cda4790d8fd3",
            "648056a05e3d48f1a7ba71903bc07d27",
            "87053ee07fc2402482e1fd183ff57c97",
            "a047a96c37824b72b157ef81326e359e"
          ]
        },
        "id": "Q3RRz4W79jVO",
        "outputId": "7cfd54ee-d748-4979-afd6-54a73655db32"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65169bba7d68420d930d35d9f16e5134",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/893 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c70325a0dfb04bca9cb5e1ee67a52d43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/289k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "349210cb00cd4393ae57461ceba242fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/65 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5s19uTd9jVO"
      },
      "source": [
        "# 2. Build our RAG System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-mET8Dy9jVO"
      },
      "source": [
        "### 2.1. Preprocessing documents to build our vector database\n",
        "\n",
        "- In this part, __we split the documents from our knowledge base into smaller chunks__: these will be the snippets that are picked by the Retriever, to then be ingested by the Reader LLM as supporting elements for its answer.\n",
        "- The goal is to build semantically relevant snippets: not too small to be sufficient for supporting an answer, and not too large too avoid diluting individual ideas.\n",
        "\n",
        "Many options exist for text splitting:\n",
        "- split every `n` words / characters, but this has the risk of cutting in half paragraphs or even sentences\n",
        "- split after `n` words / character, but only on sentence boundaries\n",
        "- **recursive split** tries to preserve even more of the document structure, by processing it tree-like way, splitting first on the largest units (chapters) then recursively splitting on smaller units (paragraphs, sentences).\n",
        "\n",
        "To learn more about chunking, I recommend you read [this great notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb) by Greg Kamradt.\n",
        "\n",
        "[This space](https://huggingface.co/spaces/m-ric/chunk_visualizer) lets you visualize how different splitting options affect the chunks you get.\n",
        "\n",
        "> In the following, we use Langchain's `RecursiveCharacterTextSplitter`.\n",
        "\n",
        "ğŸ’¡ _To measure chunk length in our Text Splitter, our length function will not be the count of characters, but the count of tokens in the tokenized text: indeed, for subsequent embedder that processes token, measuring length in tokens is more relevant and empirically performs better._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4fhm55Q9jVO",
        "outputId": "39be9a92-4f3f-418e-b570-834794ac4c82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2647/2647 [00:00<00:00, 24562.06it/s]\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.documents import Document as LangchainDocument\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
        "    for doc in tqdm(ds)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sz9Jw2_q9jVO"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: str,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of size `chunk_size` characters and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzBYfNG79jVO"
      },
      "source": [
        "### 2.2. Retriever - embeddings ğŸ—‚ï¸\n",
        "The __retriever acts like an internal search engine__: given the user query, it returns the most relevant documents from your knowledge base.\n",
        "\n",
        "> For the knowledge base, we use Langchain vector databases since __it offers a convenient [FAISS](https://github.com/facebookresearch/faiss) index and allows us to keep document metadata throughout the processing__.\n",
        "\n",
        "ğŸ› ï¸ __Options included:__\n",
        "\n",
        "- Tune the chunking method:\n",
        "    - Size of the chunks\n",
        "    - Method: split on different separators, use [semantic chunking](https://python.langchain.com/docs/modules/data_connection/document_transformers/semantic-chunker)...\n",
        "- Change the embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ozCw6BfuTeh7"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-community faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "LqJlIDZR9jVO"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "import os\n",
        "\n",
        "\n",
        "def load_embeddings(\n",
        "    langchain_docs: List[LangchainDocument],\n",
        "    chunk_size: int,\n",
        "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
        ") -> FAISS:\n",
        "    \"\"\"\n",
        "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
        "\n",
        "    Args:\n",
        "        langchain_docs: list of documents\n",
        "        chunk_size: size of the chunks to split the documents into\n",
        "        embedding_model_name: name of the embedding model to use\n",
        "\n",
        "    Returns:\n",
        "        FAISS index\n",
        "    \"\"\"\n",
        "    # load embedding_model\n",
        "    embedding_model = HuggingFaceEmbeddings(\n",
        "        model_name=embedding_model_name,\n",
        "        multi_process=True,\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\n",
        "            \"normalize_embeddings\": True\n",
        "        },  # set True to compute cosine similarity\n",
        "    )\n",
        "\n",
        "    # Check if embeddings already exist on disk\n",
        "    index_name = (\n",
        "        f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
        "    )\n",
        "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
        "    if os.path.isdir(index_folder_path):\n",
        "        return FAISS.load_local(\n",
        "            index_folder_path,\n",
        "            embedding_model,\n",
        "            distance_strategy=DistanceStrategy.COSINE,\n",
        "            allow_dangerous_deserialization=True  # <--- ADD THIS LINE\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        print(\"Index not found, generating it...\")\n",
        "        docs_processed = split_documents(\n",
        "            chunk_size,\n",
        "            langchain_docs,\n",
        "            embedding_model_name,\n",
        "        )\n",
        "        knowledge_index = FAISS.from_documents(\n",
        "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        "        )\n",
        "        knowledge_index.save_local(index_folder_path)\n",
        "        return knowledge_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6y1mQJX9jVO"
      },
      "source": [
        "### 2.3. Reader - LLM ğŸ’¬\n",
        "\n",
        "In this part, the __LLM Reader reads the retrieved documents to formulate its answer.__\n",
        "\n",
        "ğŸ› ï¸ Here we tried the following options to improve results:\n",
        "- Switch reranking on/off\n",
        "- Change the reader model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "9PdpuWyP9jVP"
      },
      "outputs": [],
      "source": [
        "RAG_PROMPT_TEMPLATE = \"\"\"\n",
        "<|system|>\n",
        "Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
        "<|user|>\n",
        "Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rubl0oL1hi7X",
        "outputId": "45eee3e7-1db0-4dad-a64c-9bf245b75c11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_huggingface\n",
            "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.36.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (1.2.7)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (2.12.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2026.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain_huggingface) (0.16.0)\n",
            "Downloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: langchain_huggingface\n",
            "Successfully installed langchain_huggingface-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RUQFnzaVCCU",
        "outputId": "fc597f2e-0bc7-4bb3-8c17-50f4f4330ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.6 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.2.7)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (2.14.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.13.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain_openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<3.0.0,>=1.109.1->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.6->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain_openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.6->langchain_openai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1.0.0,>=0.7.0->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-1.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5I3ZD4GU9qZ",
        "outputId": "ef9cabe4-02a5-4f37-a1c8-e69c6c0d19ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3473: UserWarning: Parameters {'top_p'} should be specified explicitly. Instead they were passed in as part of `model_kwargs` parameter.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# It is best practice to set your key as an environment variable\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
        "\n",
        "# 1. Initialize the ChatOpenAI model\n",
        "READER_LLM = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    openai_api_key=\"your-api-key\",\n",
        "    max_tokens=512,\n",
        "    temperature=0.1,\n",
        "    model_kwargs={\"top_p\": 0.9}\n",
        ")\n",
        "\n",
        "# 2. Maintain your specialized patch for callable behavior\n",
        "def chat_patch(self, prompt, **kwargs):\n",
        "    response = self.invoke([HumanMessage(content=prompt)])\n",
        "    return response.content\n",
        "\n",
        "# Apply the patch to the class\n",
        "ChatOpenAI.__call__ = chat_patch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ62CbcZ9jVP",
        "outputId": "f503323d-edbc-4814-e3e4-b97f6d947490"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3993874634.py:1: UserWarning: \n",
            "********************************************************************************\n",
            "RAGatouille WARNING: Future Release Notice\n",
            "--------------------------------------------\n",
            "RAGatouille version 0.0.10 will be migrating to a PyLate backend \n",
            "instead of the current Stanford ColBERT backend.\n",
            "PyLate is a fully mature, feature-equivalent backend, that greatly facilitates compatibility.\n",
            "However, please pin version <0.0.10 if you require the Stanford ColBERT backend.\n",
            "********************************************************************************\n",
            "  from ragatouille import RAGPretrainedModel\n"
          ]
        }
      ],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "from langchain_core.vectorstores import VectorStore\n",
        "from langchain_core.language_models.llms import LLM\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: LLM,\n",
        "    knowledge_index: VectorStore,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 7,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    \"\"\"Answer a question using RAG with the given knowledge index.\"\"\"\n",
        "    # Gather documents with retriever\n",
        "    relevant_docs = knowledge_index.similarity_search(\n",
        "        query=question, k=num_retrieved_docs\n",
        "    )\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join(\n",
        "        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n",
        "    )\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    answer = llm.invoke(final_prompt)\n",
        "\n",
        "    return answer, relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiygbqfT9jVP"
      },
      "source": [
        "# 3. Benchmarking the RAG system\n",
        "\n",
        "The RAG system and the evaluation datasets are now ready. The last step is to judge the RAG system's output on this evaluation dataset.\n",
        "\n",
        "To this end, __we setup a judge agent__. âš–ï¸ğŸ¤–\n",
        "\n",
        "Out of [the different RAG evaluation metrics](https://docs.ragas.io/en/latest/concepts/metrics/index.html), we choose to focus only on Answer Correctness since it is the best end-to-end metric of our system's performance.\n",
        "\n",
        "> We use GPT4 as a judge for its empirically good performance, but you could try with other models such as [kaist-ai/prometheus-13b-v1.0](https://huggingface.co/kaist-ai/prometheus-13b-v1.0) or [BAAI/JudgeLM-33B-v1.0](https://huggingface.co/BAAI/JudgeLM-33B-v1.0).\n",
        "\n",
        "ğŸ’¡ _In the evaluation prompt, we give a detailed description each metric on the scale 1-5, as is done in [Prometheus's prompt template](https://huggingface.co/kaist-ai/prometheus-13b-v1.0): this helps the model ground its metric precisely. If instead you give the judge LLM a vague scale to work with, the outputs will not be consistent enough between different examples._\n",
        "\n",
        "ğŸ’¡ _Again, prompting the LLM to output rationale before giving its final score gives it more tokens to help it formalize and elaborate a judgement._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "VrlMh_ZI9jVP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.language_models import BaseChatModel\n",
        "\n",
        "def run_rag_tests(\n",
        "    eval_dataset: datasets.Dataset,\n",
        "    llm,\n",
        "    knowledge_index: VectorStore,\n",
        "    output_file: str,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    verbose: Optional[bool] = True,\n",
        "    test_settings: Optional[str] = None,  # To document the test settings used\n",
        "):\n",
        "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
        "    try:  # load previous generations if they exist\n",
        "        with open(output_file, \"r\") as f:\n",
        "            outputs = json.load(f)\n",
        "    except:\n",
        "        outputs = []\n",
        "\n",
        "    for example in tqdm(eval_dataset):\n",
        "        question = example[\"question\"]\n",
        "        if question in [output[\"question\"] for output in outputs]:\n",
        "            continue\n",
        "\n",
        "        answer, relevant_docs = answer_with_rag(\n",
        "            question, llm, knowledge_index, reranker=reranker\n",
        "        )\n",
        "        # 1. Extract string from AIMessage if necessary\n",
        "        generated_answer = answer.content if hasattr(answer, 'content') else str(answer)\n",
        "\n",
        "        # 2. Extract page_content from Document objects\n",
        "        retrieved_docs_content = [\n",
        "            doc.page_content if hasattr(doc, 'page_content') else str(doc)\n",
        "            for doc in relevant_docs\n",
        "        ]\n",
        "\n",
        "        if verbose:\n",
        "            print(\"=======================================================\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Answer: {answer}\")\n",
        "            print(f'True answer: {example[\"answer\"]}')\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"true_answer\": example[\"answer\"],\n",
        "            \"source_doc\": example[\"source_doc\"],\n",
        "            # \"generated_answer\": answer,\n",
        "            # \"retrieved_docs\": [doc for doc in relevant_docs],\n",
        "            \"generated_answer\": generated_answer,\n",
        "            \"retrieved_docs\": retrieved_docs_content,\n",
        "        }\n",
        "        if test_settings:\n",
        "            result[\"test_settings\"] = test_settings\n",
        "        outputs.append(result)\n",
        "\n",
        "        with open(output_file, \"w\") as f:\n",
        "            json.dump(outputs, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Ae-3KWzK9jVP"
      },
      "outputs": [],
      "source": [
        "EVALUATION_PROMPT = \"\"\"###Task Description:\n",
        "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
        "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
        "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
        "3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 5}}\\\"\n",
        "4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n",
        "\n",
        "###The instruction to evaluate:\n",
        "{instruction}\n",
        "\n",
        "###Response to evaluate:\n",
        "{response}\n",
        "\n",
        "###Reference Answer (Score 5):\n",
        "{reference_answer}\n",
        "\n",
        "###Score Rubrics:\n",
        "[Is the response correct, accurate, and factual based on the reference answer?]\n",
        "Score 1: The response is completely incorrect, inaccurate, and/or not factual.\n",
        "Score 2: The response is mostly incorrect, inaccurate, and/or not factual.\n",
        "Score 3: The response is somewhat correct, accurate, and/or factual.\n",
        "Score 4: The response is mostly correct, accurate, and factual.\n",
        "Score 5: The response is completely correct, accurate, and factual.\n",
        "\n",
        "###Feedback:\"\"\"\n",
        "\n",
        "from langchain_core.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "\n",
        "evaluation_prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content=\"You are a fair evaluator language model.\"),\n",
        "        HumanMessagePromptTemplate.from_template(EVALUATION_PROMPT),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia9Mvn859jVP",
        "outputId": "98bed6a7-b6dc-44bd-97ab-b4b9701e27a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1844943501.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the `langchain-openai package and should be used instead. To use it run `pip install -U `langchain-openai` and import as `from `langchain_openai import ChatOpenAI``.\n",
            "  eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY)\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.chat_models import ChatOpenAI\n",
        "\n",
        "OPENAI_API_KEY = \"your-api-key\"\n",
        "\n",
        "eval_chat_model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
        "evaluator_name = \"GPT4\"\n",
        "\n",
        "\n",
        "def evaluate_answers(\n",
        "    answer_path: str,\n",
        "    eval_chat_model,\n",
        "    evaluator_name: str,\n",
        "    evaluation_prompt_template: ChatPromptTemplate,\n",
        ") -> None:\n",
        "    \"\"\"Evaluates generated answers. Modifies the given answer file in place for better checkpointing.\"\"\"\n",
        "    answers = []\n",
        "    if os.path.isfile(answer_path):  # load previous generations if they exist\n",
        "        answers = json.load(open(answer_path, \"r\"))\n",
        "\n",
        "    for experiment in tqdm(answers):\n",
        "        if f\"eval_score_{evaluator_name}\" in experiment:\n",
        "            continue\n",
        "\n",
        "        eval_prompt = evaluation_prompt_template.format_messages(\n",
        "            instruction=experiment[\"question\"],\n",
        "            response=experiment[\"generated_answer\"],\n",
        "            reference_answer=experiment[\"true_answer\"],\n",
        "        )\n",
        "        eval_result = eval_chat_model.invoke(eval_prompt)\n",
        "        feedback, score = [\n",
        "            item.strip() for item in eval_result.content.split(\"[RESULT]\")\n",
        "        ]\n",
        "        experiment[f\"eval_score_{evaluator_name}\"] = score\n",
        "        experiment[f\"eval_feedback_{evaluator_name}\"] = feedback\n",
        "\n",
        "        with open(answer_path, \"w\") as f:\n",
        "            json.dump(answers, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXH-szLe9jVP"
      },
      "source": [
        "ğŸš€ Let's run the tests and evaluate answers!ğŸ‘‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW2nnvUT9jVQ",
        "outputId": "3de79c60-3b56-43b4-ba41-82528946bf61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai:\n",
            "Loading knowledge base embeddings...\n",
            "Running RAG...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:00<00:00, 7314.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:00<00:00, 646042.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation for chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:mistralai:\n",
            "Loading knowledge base embeddings...\n",
            "Running RAG...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [00:00<00:00, 6502.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65/65 [01:21<00:00,  1.26s/it]\n"
          ]
        }
      ],
      "source": [
        "# READER_MODEL_NAME = \"mistralai\"\n",
        "READER_MODEL_NAME = \"gpt4\"\n",
        "if not os.path.exists(\"./output\"):\n",
        "    os.mkdir(\"./output\")\n",
        "\n",
        "for chunk_size in [200]:  # Add other chunk sizes (in tokens) as needed\n",
        "    for embeddings in [\"thenlper/gte-small\"]:  # Add other embeddings as needed\n",
        "        for rerank in [True, False]:\n",
        "            settings_name = f\"chunk:{chunk_size}_embeddings:{embeddings.replace('/', '~')}_rerank:{rerank}_reader-model:{READER_MODEL_NAME}\"\n",
        "            output_file_name = f\"./output/rag_{settings_name}.json\"\n",
        "\n",
        "            print(f\"Running evaluation for {settings_name}:\")\n",
        "\n",
        "            print(\"Loading knowledge base embeddings...\")\n",
        "            knowledge_index = load_embeddings(\n",
        "                RAW_KNOWLEDGE_BASE,\n",
        "                chunk_size=chunk_size,\n",
        "                embedding_model_name=embeddings,\n",
        "            )\n",
        "\n",
        "            print(\"Running RAG...\")\n",
        "            reranker = (\n",
        "                RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "                if rerank\n",
        "                else None\n",
        "            )\n",
        "            run_rag_tests(\n",
        "                eval_dataset=eval_dataset,\n",
        "                llm=READER_LLM,\n",
        "                knowledge_index=knowledge_index,\n",
        "                output_file=output_file_name,\n",
        "                reranker=reranker,\n",
        "                verbose=False,\n",
        "                test_settings=settings_name,\n",
        "            )\n",
        "\n",
        "            print(\"Running evaluation...\")\n",
        "            evaluate_answers(\n",
        "                output_file_name,\n",
        "                eval_chat_model,\n",
        "                evaluator_name,\n",
        "                evaluation_prompt_template,\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tytXV5-h9jVT"
      },
      "source": [
        "### Inspect results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "D4YDSfmr9jVT"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "outputs = []\n",
        "for file in glob.glob(\"./output/*.json\"):\n",
        "    output = pd.DataFrame(json.load(open(file, \"r\")))\n",
        "    output[\"settings\"] = file\n",
        "    outputs.append(output)\n",
        "result = pd.concat(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "CdkXMNvS9jVT"
      },
      "outputs": [],
      "source": [
        "result[\"eval_score_GPT4\"] = result[\"eval_score_GPT4\"].apply(\n",
        "    lambda x: int(x) if isinstance(x, str) else 1\n",
        ")\n",
        "result[\"eval_score_GPT4\"] = (result[\"eval_score_GPT4\"] - 1) / 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "lgxBpid29jVT",
        "outputId": "c4ab987f-3a21-4d95-995c-e993261de314"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eval_score_GPT4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>settings</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:gpt4.json</th>\n",
              "      <td>0.853846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:gpt4.json</th>\n",
              "      <td>0.884615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ],
            "text/plain": [
              "settings\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:gpt4.json    0.853846\n",
              "./output/rag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:gpt4.json     0.884615\n",
              "Name: eval_score_GPT4, dtype: float64"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "average_scores = result.groupby(\"settings\")[\"eval_score_GPT4\"].mean()\n",
        "average_scores.sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "ZokDc1ngrbSc",
        "outputId": "cabc4775-71ac-4344-933d-9e3bb2240ddd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"12fea4ac-5d38-47ee-aeac-d26bc6e5cdab\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"12fea4ac-5d38-47ee-aeac-d26bc6e5cdab\")) {                    Plotly.newPlot(                        \"12fea4ac-5d38-47ee-aeac-d26bc6e5cdab\",                        [{\"alignmentgroup\":\"True\",\"customdata\":[[\".\\u002foutput\\u002frag_chunk:200_embeddings:thenlper~gte-small_rerank:False_reader-model:gpt4.json\"],[\".\\u002foutput\\u002frag_chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:gpt4.json\"]],\"hovertemplate\":\"Accuracy (%)=%{marker.color}\\u003cbr\\u003esettings=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[85.38461538461539,88.46153846153845],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[85.38461538461539,88.46153846153845],\"textposition\":\"outside\",\"x\":[\"Rerank:False | Model:gpt4\",\"Rerank:True | Model:gpt4\"],\"xaxis\":\"x\",\"y\":[85.38461538461539,88.46153846153845],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{text:.1f}%\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Settings\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Accuracy (%)\"},\"range\":[0,110]},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"Accuracy (%)\"}},\"colorscale\":[[0.0,\"rgb(0,0,255)\"],[1.0,\"rgb(255,0,0)\"]],\"showscale\":false},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60,\"b\":150},\"barmode\":\"relative\",\"font\":{\"size\":14},\"width\":1000,\"height\":600,\"title\":{\"text\":\"\\u003cb\\u003eRAG Performance by Configuration\\u003c\\u002fb\\u003e\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('12fea4ac-5d38-47ee-aeac-d26bc6e5cdab');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# 1. Chuáº©n bá»‹ dá»¯ liá»‡u\n",
        "df_plot = average_scores.reset_index()\n",
        "df_plot.columns = ['settings', 'eval_score']\n",
        "df_plot['eval_score_pct'] = df_plot['eval_score'] * 100\n",
        "\n",
        "# 2. HÃ€M RÃšT Gá»ŒN TÃŠN (Shorten Names)\n",
        "def shorten_settings(name):\n",
        "    # Loáº¡i bá» tiá»n tá»‘ vÃ  háº­u tá»‘ file\n",
        "    name = name.replace(\"./output/\", \"\").replace(\".json\", \"\")\n",
        "    # Thay tháº¿ cÃ¡c cá»¥m tá»« dÃ i báº±ng kÃ½ hiá»‡u ngáº¯n hÆ¡n\n",
        "    name = name.replace(\"rag_chunk:200_embeddings:thenlper~gte-small\", \"\")\n",
        "    name = name.replace(\"_reader-model:\", \" | Model:\")\n",
        "    name = name.replace(\"_rerank:\", \"Rerank:\")\n",
        "    return name\n",
        "\n",
        "df_plot['short_settings'] = df_plot['settings'].apply(shorten_settings)\n",
        "\n",
        "# 3. Táº¡o biá»ƒu Ä‘á»“ vá»›i tÃªn Ä‘Ã£ rÃºt gá»n\n",
        "fig = px.bar(\n",
        "    df_plot,\n",
        "    x=\"short_settings\",      # Sá»­ dá»¥ng cá»™t tÃªn Ä‘Ã£ rÃºt gá»n\n",
        "    y=\"eval_score_pct\",\n",
        "    color=\"eval_score_pct\",\n",
        "    # ThÃªm hover_data Ä‘á»ƒ khi di chuá»™t vÃ o váº«n xem Ä‘Æ°á»£c Ä‘Æ°á»ng dáº«n gá»‘c Ä‘áº§y Ä‘á»§\n",
        "    hover_data={\"settings\": True, \"short_settings\": False},\n",
        "    labels={\n",
        "        \"eval_score_pct\": \"Accuracy (%)\",\n",
        "        \"short_settings\": \"Config Summary\",\n",
        "    },\n",
        "    color_continuous_scale=\"bluered\",\n",
        "    text=\"eval_score_pct\"\n",
        ")\n",
        "\n",
        "# 4. Tinh chá»‰nh Layout\n",
        "fig.update_layout(\n",
        "    width=1000,\n",
        "    height=600,\n",
        "    title=\"<b>RAG Performance by Configuration</b>\",\n",
        "    xaxis_title=\"Settings\",\n",
        "    yaxis_title=\"Accuracy (%)\",\n",
        "    yaxis_range=[0, 110],\n",
        "    font=dict(size=14),\n",
        "    margin=dict(b=150), # Giáº£m lá» dÆ°á»›i vÃ¬ tÃªn Ä‘Ã£ ngáº¯n hÆ¡n\n",
        ")\n",
        "\n",
        "\n",
        "fig.update_traces(\n",
        "    texttemplate='%{text:.1f}%',\n",
        "    textposition='outside'\n",
        ")\n",
        "\n",
        "fig.update_coloraxes(showscale=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSPH9DYI9jVT"
      },
      "source": [
        "## Example results\n",
        "\n",
        "Let us load the results that I obtained by tweaking the different options available in this notebook.\n",
        "For more detail on why these options could work or not, see the notebook on [advanced_RAG](advanced_rag).\n",
        "\n",
        "As you can see in the graph below, some tweaks do not bring any improvement, some give huge performance boosts.\n",
        "\n",
        "â¡ï¸ ___There is no single good recipe: you should try several different directions when tuning your RAG systems.___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "372d199195d34ab7a0bc99e42a3987a1",
            "de520e078a8746e890578ffe3e23401a",
            "682db0aad01f43dbabd998eedb46a431",
            "3421036f1ed04971bafeeb1ac357321a",
            "2a10430fa9e046fca75d30bb571f358c",
            "4538b0c6ba5c4ef1ba1e3c3dd8e85701",
            "3cd966ec236e447d852c52f79de4a6df",
            "286980da88ba4b079dfe075dc4684a7f",
            "4af24b83324e40a78e2b368de14935f1",
            "4d30225c81a746b4ba032918dbcaee9f",
            "a4722d3543e742d2a13d11a6c780609a",
            "a135408ec6b844d1a8eb5547651ae715",
            "41a944bb2a42434b8008bb64ca7acb6f",
            "a4ee98c54a034b5da403411c5a044e68",
            "0ddbea70cb6b40e886b31efe0520d74b",
            "bfd459816f684f2987bc957abe366a89",
            "218ec84578354a17b5ad3aa801b682f8",
            "6d61922b66f64b4f94d7a3e5af6bf547",
            "a53aff74a64c4a11b33e14e7f711d50b",
            "1fd5e06a5e3147ff96459a323a0ec984",
            "0c889e7a8cbc47aaa21c6877a0560ced",
            "45523c498c1d4c0dab0a70c3f9f2c6a0",
            "80b50053112d43618536e55a3c93a86c",
            "e1b596596b8c44afac8297fd5170e1e7",
            "6237f492b0184ebdb72bf8afa117f718",
            "5cbf74cb5f754bc19ebd05b99d923b7a",
            "d3f7af9b9e0e470ea3aa3f96de9fc514",
            "0ad60116f3f0499cb6cad6dcba2d4615",
            "12d136811c66428fb72f09b09a54dc4e",
            "290e040c9699479eb48cb79b4edb2d1f",
            "a035744492bb4d06a2f8712524df001d",
            "cc28e7d066a34606a5586ebf40ecba48",
            "8e86bdb54e60498a94921f54946d6cb0"
          ]
        },
        "id": "RVOxatv99jVT",
        "outputId": "88646f86-4e49-4330-c160-d99b43e88a39"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "372d199195d34ab7a0bc99e42a3987a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/319 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a135408ec6b844d1a8eb5547651ae715",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/1.45k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80b50053112d43618536e55a3c93a86c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "scores = datasets.load_dataset(\"m-ric/rag_scores_cookbook\", split=\"train\")\n",
        "scores = pd.Series(scores[\"score\"], index=scores[\"settings\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "vqK0Dg2Q9jVT",
        "outputId": "62b0633f-8244-45f6-f2f3-b8fc82535b5b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"cddcd54e-1bec-4b6d-bd00-48fcb0350d6f\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"cddcd54e-1bec-4b6d-bd00-48fcb0350d6f\")) {                    Plotly.newPlot(                        \"cddcd54e-1bec-4b6d-bd00-48fcb0350d6f\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"index=%{x}\\u003cbr\\u003eAccuracy=%{y}\\u003cbr\\u003ecolor=%{marker.color}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":[74.6268656716418,86.94029850746269,88.4328,92.1642,94.7761],\"coloraxis\":\"coloraxis\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"outside\",\"x\":[\"Baseline RAG\",\"+ Tune chunk size\",\"+ Better embeddings\",\"+ Rerank snippets\",\"+ Better reader LLM\"],\"xaxis\":\"x\",\"y\":[74.6268656716418,86.94029850746269,88.4328,92.1642,94.7761],\"yaxis\":\"y\",\"type\":\"bar\",\"texttemplate\":\"%{y:.1f}\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"RAG settings\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Accuracy\"},\"range\":[0,100],\"ticksuffix\":\"%\"},\"coloraxis\":{\"colorbar\":{\"title\":{\"text\":\"color\"}},\"colorscale\":[[0.0,\"rgb(0,0,255)\"],[1.0,\"rgb(255,0,0)\"]],\"showscale\":false},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"group\",\"font\":{\"size\":15},\"width\":1000,\"height\":600,\"title\":{\"text\":\"\\u003cb\\u003eAccuracy of different RAG configurations\\u003c\\u002fb\\u003e\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('cddcd54e-1bec-4b6d-bd00-48fcb0350d6f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = px.bar(\n",
        "    scores,\n",
        "    color=scores,\n",
        "    labels={\n",
        "        \"value\": \"Accuracy\",\n",
        "        \"settings\": \"Configuration\",\n",
        "    },\n",
        "    color_continuous_scale=\"bluered\",\n",
        ")\n",
        "fig.update_layout(\n",
        "    width=1000,\n",
        "    height=600,\n",
        "    barmode=\"group\",\n",
        "    yaxis_range=[0, 100],\n",
        "    title=\"<b>Accuracy of different RAG configurations</b>\",\n",
        "    xaxis_title=\"RAG settings\",\n",
        "    font=dict(size=15),\n",
        ")\n",
        "fig.layout.yaxis.ticksuffix = \"%\"\n",
        "fig.update_coloraxes(showscale=False)\n",
        "fig.update_traces(texttemplate=\"%{y:.1f}\", textposition=\"outside\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPUOMWGk9jVT"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface/cookbook-images/resolve/main/RAG_settings_accuracy.png\" height=\"500\" width=\"800\">\n",
        "\n",
        "As you can see, these had varying impact on performance. In particular, tuning the chunk size is both easy and very impactful.\n",
        "\n",
        "But this is our case: your results could be very different: now that you have a robust evaluation pipeline, you can set on to explore other options! ğŸ—ºï¸"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "013f6e7a28eb4001a469f5b8ec44bd61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac110c35b7944c98a166b6bf6c8d7fcd",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5840658bf35e4c959cc8f14fbe918263",
            "value": "100%"
          }
        },
        "0ad60116f3f0499cb6cad6dcba2d4615": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c889e7a8cbc47aaa21c6877a0560ced": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ddbea70cb6b40e886b31efe0520d74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c889e7a8cbc47aaa21c6877a0560ced",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_45523c498c1d4c0dab0a70c3f9f2c6a0",
            "value": "â€‡1.45k/1.45kâ€‡[00:00&lt;00:00,â€‡3.29kB/s]"
          }
        },
        "10692fe379bc42c388ff2c9925353509": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12d136811c66428fb72f09b09a54dc4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19027954ef554e219de962244b0bfcaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fd5e06a5e3147ff96459a323a0ec984": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2013c1159a854416bf7ba6ef5b965e87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "218ec84578354a17b5ad3aa801b682f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23d170b784ec4d51a2744b5fa7b5319d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "286980da88ba4b079dfe075dc4684a7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "290e040c9699479eb48cb79b4edb2d1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a10430fa9e046fca75d30bb571f358c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9ebc764ada45778801b65e9280a174": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2da7074ffa00465e872a70c1c79f4a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceb57d958b6f4da3b72237c61d097505",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e9309ef0171c4b8fb6449c34a50fa1de",
            "value": "â€‡2647/2647â€‡[00:00&lt;00:00,â€‡12527.48it/s]"
          }
        },
        "2dbd0a808c34457bab6b64430818973e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3421036f1ed04971bafeeb1ac357321a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d30225c81a746b4ba032918dbcaee9f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a4722d3543e742d2a13d11a6c780609a",
            "value": "â€‡319/319â€‡[00:00&lt;00:00,â€‡33.3kB/s]"
          }
        },
        "343c453e6d714a40a8a2d374f2ae6c50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "349210cb00cd4393ae57461ceba242fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ac84713c78b4bceab1e309856e9933a",
              "IPY_MODEL_48ed20807e9b4d58a3d3d08521dc258a",
              "IPY_MODEL_d8b654f4d7494882bd12c5327612b20f"
            ],
            "layout": "IPY_MODEL_b67bd32e58b243d390cdb28abb3a488c"
          }
        },
        "372d199195d34ab7a0bc99e42a3987a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de520e078a8746e890578ffe3e23401a",
              "IPY_MODEL_682db0aad01f43dbabd998eedb46a431",
              "IPY_MODEL_3421036f1ed04971bafeeb1ac357321a"
            ],
            "layout": "IPY_MODEL_2a10430fa9e046fca75d30bb571f358c"
          }
        },
        "3cd966ec236e447d852c52f79de4a6df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41a944bb2a42434b8008bb64ca7acb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_218ec84578354a17b5ad3aa801b682f8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_6d61922b66f64b4f94d7a3e5af6bf547",
            "value": "data/train-00000-of-00001.parquet:â€‡100%"
          }
        },
        "4538b0c6ba5c4ef1ba1e3c3dd8e85701": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45523c498c1d4c0dab0a70c3f9f2c6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48ed20807e9b4d58a3d3d08521dc258a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6a8d5ce2f664eb5a4f3cda4790d8fd3",
            "max": 65,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_648056a05e3d48f1a7ba71903bc07d27",
            "value": 65
          }
        },
        "49e9d05c60474b5bbc4f579835129b7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea30e2eb024141b08129373fe95e12a6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_662aca1919c249f88ac4656e47003b1f",
            "value": "â€‡289k/289kâ€‡[00:00&lt;00:00,â€‡549kB/s]"
          }
        },
        "4af24b83324e40a78e2b368de14935f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d30225c81a746b4ba032918dbcaee9f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5840658bf35e4c959cc8f14fbe918263": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ac84713c78b4bceab1e309856e9933a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19027954ef554e219de962244b0bfcaa",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d37fc3039b88449397deea32f286b215",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "5cbf74cb5f754bc19ebd05b99d923b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc28e7d066a34606a5586ebf40ecba48",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8e86bdb54e60498a94921f54946d6cb0",
            "value": "â€‡5/5â€‡[00:00&lt;00:00,â€‡281.07â€‡examples/s]"
          }
        },
        "6237f492b0184ebdb72bf8afa117f718": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_290e040c9699479eb48cb79b4edb2d1f",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a035744492bb4d06a2f8712524df001d",
            "value": 5
          }
        },
        "648056a05e3d48f1a7ba71903bc07d27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65169bba7d68420d930d35d9f16e5134": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac1e699071bd4911bd11953908ce52e9",
              "IPY_MODEL_b58548a24aca4deb844f82b72ae2a07d",
              "IPY_MODEL_a4266f0fe3364e1a81af6fabdbb501e9"
            ],
            "layout": "IPY_MODEL_343c453e6d714a40a8a2d374f2ae6c50"
          }
        },
        "662aca1919c249f88ac4656e47003b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "682db0aad01f43dbabd998eedb46a431": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_286980da88ba4b079dfe075dc4684a7f",
            "max": 319,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4af24b83324e40a78e2b368de14935f1",
            "value": 319
          }
        },
        "682f8f838a6947deaf3b1bee9bd710ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c2de4d74f0f4dcdabacbbf3c5b47249": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc30b7a9bf0e43ffb4c8db27c2f7f1dd",
            "max": 289016,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5923aa824954744866fd9ed4d3a8c9f",
            "value": 289016
          }
        },
        "6d61922b66f64b4f94d7a3e5af6bf547": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "755b99c200bd454fbe61320b9c1b4417": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d9ebc764ada45778801b65e9280a174",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2dbd0a808c34457bab6b64430818973e",
            "value": 2647
          }
        },
        "80b50053112d43618536e55a3c93a86c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1b596596b8c44afac8297fd5170e1e7",
              "IPY_MODEL_6237f492b0184ebdb72bf8afa117f718",
              "IPY_MODEL_5cbf74cb5f754bc19ebd05b99d923b7a"
            ],
            "layout": "IPY_MODEL_d3f7af9b9e0e470ea3aa3f96de9fc514"
          }
        },
        "83b0c5d46d9b43a1904fe94dd118e7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87053ee07fc2402482e1fd183ff57c97": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e86bdb54e60498a94921f54946d6cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "945a0ef0f62b409fb58f1fd452c49a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a035744492bb4d06a2f8712524df001d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a047a96c37824b72b157ef81326e359e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a135408ec6b844d1a8eb5547651ae715": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41a944bb2a42434b8008bb64ca7acb6f",
              "IPY_MODEL_a4ee98c54a034b5da403411c5a044e68",
              "IPY_MODEL_0ddbea70cb6b40e886b31efe0520d74b"
            ],
            "layout": "IPY_MODEL_bfd459816f684f2987bc957abe366a89"
          }
        },
        "a4266f0fe3364e1a81af6fabdbb501e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7a1be638b1246089d9ac6a59b95731f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_83b0c5d46d9b43a1904fe94dd118e7df",
            "value": "â€‡893/893â€‡[00:00&lt;00:00,â€‡101kB/s]"
          }
        },
        "a4722d3543e742d2a13d11a6c780609a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4ee98c54a034b5da403411c5a044e68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a53aff74a64c4a11b33e14e7f711d50b",
            "max": 1453,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1fd5e06a5e3147ff96459a323a0ec984",
            "value": 1453
          }
        },
        "a53aff74a64c4a11b33e14e7f711d50b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac110c35b7944c98a166b6bf6c8d7fcd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac1e699071bd4911bd11953908ce52e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_682f8f838a6947deaf3b1bee9bd710ab",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_10692fe379bc42c388ff2c9925353509",
            "value": "README.md:â€‡100%"
          }
        },
        "b3cce8e79e3e4ac9a7d87b31c3551e90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_013f6e7a28eb4001a469f5b8ec44bd61",
              "IPY_MODEL_755b99c200bd454fbe61320b9c1b4417",
              "IPY_MODEL_2da7074ffa00465e872a70c1c79f4a3c"
            ],
            "layout": "IPY_MODEL_f77e5a87330f4897b221142785afab31"
          }
        },
        "b58548a24aca4deb844f82b72ae2a07d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feda2e697dd34347a5270ad1301f004e",
            "max": 893,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7f1e475833c4c0283be3828b4100ffd",
            "value": 893
          }
        },
        "b5923aa824954744866fd9ed4d3a8c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b67bd32e58b243d390cdb28abb3a488c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfd459816f684f2987bc957abe366a89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c70325a0dfb04bca9cb5e1ee67a52d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c92acbd353e546a6b1019593872a39cd",
              "IPY_MODEL_6c2de4d74f0f4dcdabacbbf3c5b47249",
              "IPY_MODEL_49e9d05c60474b5bbc4f579835129b7c"
            ],
            "layout": "IPY_MODEL_23d170b784ec4d51a2744b5fa7b5319d"
          }
        },
        "c7a1be638b1246089d9ac6a59b95731f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c92acbd353e546a6b1019593872a39cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2013c1159a854416bf7ba6ef5b965e87",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_945a0ef0f62b409fb58f1fd452c49a8b",
            "value": "data/train-00000-of-00001.parquet:â€‡100%"
          }
        },
        "cc28e7d066a34606a5586ebf40ecba48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceb57d958b6f4da3b72237c61d097505": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37fc3039b88449397deea32f286b215": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3f7af9b9e0e470ea3aa3f96de9fc514": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6a8d5ce2f664eb5a4f3cda4790d8fd3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f1e475833c4c0283be3828b4100ffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8b654f4d7494882bd12c5327612b20f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87053ee07fc2402482e1fd183ff57c97",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_a047a96c37824b72b157ef81326e359e",
            "value": "â€‡65/65â€‡[00:00&lt;00:00,â€‡908.80â€‡examples/s]"
          }
        },
        "de520e078a8746e890578ffe3e23401a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4538b0c6ba5c4ef1ba1e3c3dd8e85701",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3cd966ec236e447d852c52f79de4a6df",
            "value": "README.md:â€‡100%"
          }
        },
        "e1b596596b8c44afac8297fd5170e1e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ad60116f3f0499cb6cad6dcba2d4615",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_12d136811c66428fb72f09b09a54dc4e",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "e9309ef0171c4b8fb6449c34a50fa1de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea30e2eb024141b08129373fe95e12a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f77e5a87330f4897b221142785afab31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc30b7a9bf0e43ffb4c8db27c2f7f1dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feda2e697dd34347a5270ad1301f004e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
