{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\AI Training PTN\\Rag\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The theory of relativity, developed by Albert Einstein, is a fundamental concept in modern physics that explains how space and time are connected. It's a complex topic, but I'll try to break it down in simple terms.\n",
      "\n",
      "**Special Relativity (1905)**\n",
      "\n",
      "Einstein's special theory of relativity states that the laws of physics are the same for all observers in uniform motion relative to one another. This means that:\n",
      "\n",
      "1. **Time and space are relative**: Time and space are not\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    "    token=\"HF-Token-here\",\n",
    ")\n",
    "\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.chat_completion(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "print(\n",
    "    call_llm(\n",
    "        llm_client,\n",
    "        \"Explain the theory of relativity in simple terms.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save token, we predefine outputs for benchmarking\n",
    "outputs = [\n",
    "    {\n",
    "        \"context\": \"Gravity pulls objects toward Earth, causing them to fall.\",\n",
    "        \"question\": \"Why do things fall?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Light travels faster than sound in air.\",\n",
    "        \"question\": \"Does light or sound travel faster?\"\n",
    "    },\n",
    "    {\n",
    "        \"context\": \"Cells are the basic building blocks of living organisms.\",\n",
    "        \"question\": \"What are cells?\"\n",
    "    },\n",
    "]\n",
    "\n",
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You are evaluating whether the question is grounded in the given context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Evaluation: Explain if the question can be answered using only the context or not.\n",
    "Total rating: Give a score from 1-5 (5 means fully grounded, 1 means not grounded at all).\n",
    "\"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You are evaluating whether the question is relevant and meaningful.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Evaluation: Explain whether the question is relevant or useful.\n",
    "Total rating: Give a score from 1-5.\n",
    "\"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You are evaluating whether the question can stand on its own without needing extra context.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Evaluation: Explain whether the question can be understood independently.\n",
    "Total rating: Give a score from 1-5.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating critique for each QA couple...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:16<00:00,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'Gravity pulls objects toward Earth, causing them to fall.',\n",
      " 'groundedness_eval': 'The question \"Why do things fall?\" can be partially '\n",
      "                      'answered using the given context. \\n'\n",
      "                      '\\n'\n",
      "                      'The context states that \"Gravity pulls objects toward '\n",
      "                      'Earth, causing them to fall.\" This statement directly '\n",
      "                      'answ',\n",
      " 'groundedness_score': 4,\n",
      " 'question': 'Why do things fall?',\n",
      " 'relevance_eval': 'The question \"Why do things fall?\" is a fundamental '\n",
      "                   'inquiry that has been a subject of interest for centuries. '\n",
      "                   'It touches upon the basic concept of gravity, which is a '\n",
      "                   'crucial aspect of our understanding of the physical world. '\n",
      "                   'The question is relevant and useful because it:\\n'\n",
      "                   '\\n'\n",
      "                   '1. Encourages critical thinking: The question prompts the '\n",
      "                   'inquirer to think about the underlying reasons behind a '\n",
      "                   'common phenomenon, developing their analytical skills.\\n'\n",
      "                   '2. Explores a fundamental concept: Gravity is a '\n",
      "                   'fundamental force of',\n",
      " 'relevance_score': 1,\n",
      " 'standalone_eval': 'The question \"Why do things fall?\" can be understood '\n",
      "                    \"independently, and it's a self-contained question that \"\n",
      "                    \"doesn't require any additional context to comprehend its \"\n",
      "                    'meaning.\\n'\n",
      "                    '\\n'\n",
      "                    'However, the answer to th',\n",
      " 'standalone_score': 5}\n",
      "----------------------------------------\n",
      "{'context': 'Light travels faster than sound in air.',\n",
      " 'groundedness_eval': 'The question can be answered using only the context. '\n",
      "                      'The context explicitly states that \"Light travels '\n",
      "                      'faster than sound in air.\" This provides a definitive '\n",
      "                      'answer to the question, which is a simple \"light.\"',\n",
      " 'groundedness_score': 5,\n",
      " 'question': 'Does light or sound travel faster?',\n",
      " 'relevance_eval': 'The question \"Does light or sound travel faster?\" is '\n",
      "                   'relevant and useful because it compares the speeds of two '\n",
      "                   'fundamental physical phenomena. Understanding which one '\n",
      "                   'travels faster provides insight into the properties of '\n",
      "                   'light and sound, as well as their interactions with '\n",
      "                   'matter.\\n'\n",
      "                   '\\n'\n",
      "                   'The answer to this question has practical implications in '\n",
      "                   'fields such as physics, engineering, and '\n",
      "                   'telecommunications. For instance, it helps us understand '\n",
      "                   'how data is transmitted through fiber optic cables (light) '\n",
      "                   'versus through radio waves or other sound-based '\n",
      "                   'technologies.\\n'\n",
      "                   '\\n'\n",
      "                   'Moreover, this',\n",
      " 'relevance_score': None,\n",
      " 'standalone_eval': 'The question \"Does light or sound travel faster?\" can be '\n",
      "                    'understood independently, as it clearly states the topic '\n",
      "                    'of comparison and the outcome being sought (which one '\n",
      "                    'travels faster).\\n'\n",
      "                    '\\n'\n",
      "                    'However, to fu',\n",
      " 'standalone_score': 4}\n",
      "----------------------------------------\n",
      "{'context': 'Cells are the basic building blocks of living organisms.',\n",
      " 'groundedness_eval': 'Given the context, \"Cells are the basic building blocks '\n",
      "                      'of living organisms,\" the question \"What are cells?\" '\n",
      "                      'can be partially answered using the provided context.\\n'\n",
      "                      '\\n'\n",
      "                      'The context gives a brief definition',\n",
      " 'groundedness_score': 2,\n",
      " 'question': 'What are cells?',\n",
      " 'relevance_eval': 'The question \"What are cells?\" is a fundamental and '\n",
      "                   'relevant question in the field of biology and science '\n",
      "                   'education. Cells are the basic structural and functional '\n",
      "                   'units of living organisms, and understanding their '\n",
      "                   'characteristics and functions is essential for grasping '\n",
      "                   'various biological concepts, such as genetics, evolution, '\n",
      "                   'and ecology.\\n'\n",
      "                   '\\n'\n",
      "                   'This question is useful because it:\\n'\n",
      "                   '\\n'\n",
      "                   '1. Provides a foundation for understanding complex '\n",
      "                   'biological processes.\\n'\n",
      "                   '2. Helps students and researchers appreciate the diversity '\n",
      "                   'and complexity of living organisms.\\n'\n",
      "                   '3. Encourages curiosity',\n",
      " 'relevance_score': 1,\n",
      " 'standalone_eval': 'The question \"What are cells?\" can be understood '\n",
      "                    'independently. It is a basic and straightforward inquiry '\n",
      "                    'that requires minimal context. The term \"cells\" is '\n",
      "                    'commonly known, referring to the basic unit',\n",
      " 'standalone_score': 5}\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating critique for each QA couple...\")\n",
    "\n",
    "for output in tqdm(outputs):\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            question_groundedness_critique_prompt.format(\n",
    "                context=output[\"context\"], \n",
    "                question=output[\"question\"]\n",
    "            ),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            question_relevance_critique_prompt.format(\n",
    "                question=output[\"question\"]\n",
    "            ),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            question_standalone_critique_prompt.format(\n",
    "                question=output[\"question\"]\n",
    "            ),\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            # Extract score (supports 4, 5, 4/5, 4 out of 5)\n",
    "            match = re.search(r'(\\d+)', evaluation)\n",
    "            score = int(match.group(1)) if match else None\n",
    "\n",
    "            # Extract evaluation text\n",
    "            if \"Evaluation:\" in evaluation:\n",
    "                eval_ = evaluation.split(\"Evaluation:\", 1)[1] \\\n",
    "                                .split(\"Rating:\", 1)[0] \\\n",
    "                                .split(\"Total rating:\", 1)[0] \\\n",
    "                                .strip()\n",
    "            else:\n",
    "                eval_ = evaluation[:200].strip()\n",
    "\n",
    "            output.update({\n",
    "                f\"{criterion}_score\": score,\n",
    "                f\"{criterion}_eval\": eval_,\n",
    "            })\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "for o in outputs:\n",
    "    pprint(o)\n",
    "    print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[Document],\n",
    "    tokenizer_name: str,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of size `chunk_size` tokens and return a list of documents.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10),\n",
    "        add_start_index=True,\n",
    "        strip_whitespace=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\", \",\", \".\", \"!\", \"?\"],\n",
    "    )\n",
    "\n",
    "    docs_processed: List[Document] = []\n",
    "    for doc in knowledge_base:\n",
    "        docs_processed.extend(text_splitter.split_documents([doc]))\n",
    "\n",
    "    # Remove duplicates based on text content\n",
    "    seen = set()\n",
    "    docs_unique: List[Document] = []\n",
    "\n",
    "    for doc in docs_processed:\n",
    "        if doc.page_content not in seen:\n",
    "            seen.add(doc.page_content)\n",
    "            docs_unique.append(doc)\n",
    "\n",
    "    return docs_unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from typing import List, Optional\n",
    "from langchain_core.documents import Document as LangchainDocument\n",
    "import os\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = \"thenlper/gte-small\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: List of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\n",
    "            \"normalize_embeddings\": True\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # check if embeddings already exist on disk\n",
    "    index_name = (\n",
    "        f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '_')}\"\n",
    "    )\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.language_models.llms import LLM\n",
    "from typing import List, Tuple\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: LLM,\n",
    "    knowledge_index: VectorStore,\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    "    reranker = None,\n",
    ") -> Tuple[str, List[str]]:\n",
    "\n",
    "    # retrieve\n",
    "    relevant_docs = knowledge_index.similarity_search(\n",
    "        question, k=num_retrieved_docs\n",
    "    )\n",
    "\n",
    "    # keep only text content\n",
    "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
    "\n",
    "    # truncate to final doc count\n",
    "    relevant_docs = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # build prompt\n",
    "    context = \"\\n\\nExtracted documents:\\n\"\n",
    "    context += \"\\n\".join(\n",
    "        [f\"[Doc {i}]:\\n{doc}\" for i, doc in enumerate(relevant_docs)]\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Answer the following question using the documents provided.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    answer = llm(prompt)\n",
    "    return answer, relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "try:\n",
    "    from ragatouille import RAGPretrainedModel\n",
    "except ImportError:\n",
    "    RAGPretrainedModel = None\n",
    "\n",
    "def run_rag_tests(\n",
    "    # eval_dataset: datasets.Dataset,\n",
    "    eval_dataset: List[Dict],\n",
    "    llm: BaseChatModel,\n",
    "    knowledge_index: VectorStore,\n",
    "    output_file: str,\n",
    "    # reranker: Optional[RAGPretrainedModel] = None,\n",
    "    reranker: None,\n",
    "    verbose: Optional[bool] = True,\n",
    "    test_settings: Optional[str] = None, # To document the test settings used\n",
    "):\n",
    "    \"\"\"Runs RAG tests on the given dataset and saves the results to the given output file.\"\"\"\n",
    "    # Load previous generations if they exist\n",
    "    try:\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        answer, relevant_docs = answer_with_rag(\n",
    "            question, llm, knowledge_index, reranker=reranker\n",
    "        )\n",
    "\n",
    "        if verbose:\n",
    "            print(\"===========================================================\")\n",
    "            print(f\"Question: {question}\")\n",
    "            print(f\"Answer: {answer}\")\n",
    "            print(f\"True answer: {example['answer']}\")\n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc for doc in relevant_docs],\n",
    "        }\n",
    "\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "\n",
    "        outputs.append(result)\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(outputs, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document as LangchainDocument\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class MockVectorStore:\n",
    "    def __init__(self, docs: List[str], vectors: np.ndarray, embedder):\n",
    "        self.docs = docs\n",
    "        self.vectors = vectors\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def _cosine_sim(self, a, b):\n",
    "        a = a / (np.linalg.norm(a) + 1e-9)\n",
    "        b = b / (np.linalg.norm(b) + 1e-9)\n",
    "        return np.dot(a, b)\n",
    "\n",
    "    def similarity_search(self, query: str, k: int = 4):\n",
    "        q_vec = self.embedder.embed_query(query)\n",
    "        sims = [(self._cosine_sim(q_vec, self.vectors[i]), i) for i in range(len(self.docs))]\n",
    "        sims = sorted(sims, key=lambda x: x[0], reverse=True)[:k]\n",
    "        return [LangchainDocument(page_content=self.docs[i]) for _, i in sims]\n",
    "\n",
    "def load_embeddings_mock(langchain_docs, chunk_size, embedding_model_name=\"thenlper/gte-small\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "    docs = [d.page_content for d in langchain_docs]\n",
    "    vectors = [embedder.embed_query(text) for text in docs]\n",
    "    vectors = np.array(vectors, dtype=np.float32)\n",
    "\n",
    "    return MockVectorStore(docs, vectors, embedder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "Question: What is RAG?\n",
      "Answer: Based on the documents provided, RAG (Retrieval-Augmented Generation) is a method or system that retrieves context before generating answers.\n",
      "True answer: A method that retrieves context before generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================\n",
      "Question: What is FAISS?\n",
      "Answer: FAISS is a vector database for fast similarity search.\n",
      "True answer: A vector index for similarity search.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [\n",
    "    LangchainDocument(page_content=\"Retrieval-Augmented Generation (RAG) retrieves context before generating answers.\"),\n",
    "    LangchainDocument(page_content=\"FAISS is a vector database for fast similarity search.\")\n",
    "]\n",
    "chunk_size = 512\n",
    "\n",
    "# knowledge_index = load_embeddings(\n",
    "#     langchain_docs=langchain_docs,\n",
    "#     chunk_size=chunk_size,\n",
    "#     embedding_model_name=\"thenlper/gte-small\",\n",
    "# )\n",
    "\n",
    "knowledge_index = load_embeddings_mock(\n",
    "    langchain_docs=langchain_docs,\n",
    "    chunk_size=512,\n",
    "    embedding_model_name=\"thenlper/gte-small\",\n",
    ")\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"question\": \"What is RAG?\",\n",
    "        \"answer\": \"A method that retrieves context before generation.\",\n",
    "        \"source_doc\": \"rag_intro.txt\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is FAISS?\",\n",
    "        \"answer\": \"A vector index for similarity search.\",\n",
    "        \"source_doc\": \"faiss_overview.txt\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "outputs_path = \"rag_outputs.json\"\n",
    "\n",
    "run_rag_tests(\n",
    "    eval_dataset=eval_dataset,\n",
    "    llm=lambda prompt: call_llm(llm_client, prompt),\n",
    "    knowledge_index=knowledge_index,\n",
    "    output_file=outputs_path,\n",
    "    reranker=None,\n",
    "    verbose=True,\n",
    "    test_settings=\"first-run\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
