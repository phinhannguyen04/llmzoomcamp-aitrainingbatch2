[{"question": "What architecture is the `tokenizers-linux-x64-musl` binary designed for?\n", "true_answer": "x86_64-unknown-linux-musl", "source_doc": "huggingface/tokenizers/blob/main/bindings/node/npm/linux-x64-musl/README.md", "generated_answer": "The `tokenizers-linux-x64-musl` binary is designed for the **x86_64-unknown-linux-musl** architecture. [Document 0]", "retrieved_docs": ["`tokenizers-linux-x64-musl`\n\nThis is the **x86_64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-arm64-musl`\n\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`", "`tokenizers-linux-x64-gnu`\n\nThis is the **x86_64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-linux-arm64-gnu`\n\nThis is the **aarch64-unknown-linux-gnu** binary for `tokenizers`", "`tokenizers-android-arm64`\n\nThis is the **aarch64-linux-android** binary for `tokenizers`", "`tokenizers-freebsd-x64`\n\nThis is the **x86_64-unknown-freebsd** binary for `tokenizers`", "`tokenizers-win32-x64-msvc`\n\nThis is the **x86_64-pc-windows-msvc** binary for `tokenizers`"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, indicating that the `tokenizers-linux-x64-musl` binary is designed for the x86_64-unknown-linux-musl architecture."}, {"question": "What is the purpose of the BLIP-Diffusion model?\n", "true_answer": "The BLIP-Diffusion model is designed for controllable text-to-image generation and editing.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/blip_diffusion.md", "generated_answer": "The purpose of the BLIP-Diffusion model is to enable zero-shot subject-driven generation and control-guided zero-shot generation. It can also be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. [Document 0, Document 1]", "retrieved_docs": ["# BLIP-Diffusion\n\nBLIP-Diffusion was proposed in [BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720). It enables zero-shot subject-driven generation and control-guided zero-shot generation. \n\n\nThe abstract from the paper is:", ". We also demonstrate that BLIP-Diffusion can be flexibly combined with existing techniques such as ControlNet and prompt-to-prompt to enable novel subject-driven generation and editing applications. Project page at [this https URL](https://dxli94.github.io/BLIP-Diffusion-website/).*", "# Stable Diffusion pipelines\n\nStable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). Latent diffusion applies the diffusion process over a lower dimensional latent space to reduce memory and compute complexity. This specific type of diffusion model was proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://huggingface.co/papers/2112.10752) by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer.", "--\ntitle: \"VQ-Diffusion\" \nthumbnail: /blog/assets/117_vq_diffusion/thumbnail.png\nauthors:\n- user: williamberman\n---\n\n# VQ-Diffusion\n\n\nVector Quantized Diffusion (VQ-Diffusion) is a conditional latent diffusion model developed by the University of Science and Technology of China and Microsoft. Unlike most commonly studied diffusion models, VQ-Diffusion's noising and denoising processes operate on a quantized latent space, i.e., the latent space is composed of a discrete set of vectors. Discrete diffusion models are less explored than their continuous counterparts and offer an interesting point of comparison with autoregressive (AR) models.", "# Super-resolution\n\nThe Stable Diffusion upscaler diffusion model was created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/), and [LAION](https://laion.ai/). It is used to enhance the resolution of input images by a factor of 4.\n\n<Tip>\n\nMake sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently!", "\ud83e\udde8 Diffusers Pipelines\n\nPipelines provide a simple way to run state-of-the-art diffusion models in inference.\nMost diffusion systems consist of multiple independently-trained models and highly adaptable scheduler\ncomponents - all of which are needed to have a functioning end-to-end diffusion system.", "## How does Stable Diffusion work?\n\nHaving seen the high-quality images that stable diffusion can produce, let's try to understand \na bit better how the model functions.\n\nStable Diffusion is based on a particular type of diffusion model called **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).\n\nGenerally speaking, diffusion models are machine learning systems that are trained to *denoise* random Gaussian noise step by step, to get to a sample of interest, such as an *image*. For a more detailed overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb)."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response correctly identifies the BLIP-Diffusion model's purpose as it relates to generation and control in image editing, which aligns with the reference answer's mention of controllable text-to-image generation and editing. The response adds specific details about zero-shot subject-driven generation and control-guided zero-shot generation, which are subsets of the broader purpose of controllable generation and editing. The mention of combining with existing techniques like ControlNet and prompt-to-prompt indicates a deeper understanding of the model's applications, which is not contradicted by the reference answer and can be seen as an expansion of the model's purpose. Therefore, the response is accurate and factual, though it provides more detail than the reference answer."}, {"question": "How can a user claim authorship of a paper on the Hugging Face Hub?\n", "true_answer": "By clicking their name on the corresponding Paper page and clicking \"claim authorship\", then confirming the request in paper settings for admin team validation.", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/paper-pages.md", "generated_answer": "A user can claim authorship of a paper on the Hugging Face Hub by clicking on their name on the corresponding Paper page and then clicking \"claim authorship.\" This action will redirect them to the paper settings, where they can confirm the request. The admin team will then validate the request, and once confirmed, the Paper page will show as verified. (Document 0)", "retrieved_docs": ["If your paper is not linked to your account, you can click in your name in the corresponding Paper page and click \"claim authorship\". This will automatically re-direct to your paper settings where you can confirm the request. The admin team will validate your request soon. Once confirmed, the Paper page will show as verified.\n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-settings-dark.png\"/>\n</div>", "<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/datasets-arxiv-dark.png\"/>\n</div>\n\n## Claiming authorship to a Paper\n\nThe Hub will attempt to automatically match paper to users based on their email.", "## Linking a Paper to a model, dataset or Space\n\nIf the repository card (`README.md`) includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID and include it in the repository's tags. Clicking on the arxiv tag will let you:\n\n* Visit the Paper page.\n* Filter for other models or datasets on the Hub that cite the same paper.", "## Can I link my model to a paper on arXiv?\n\nIf the model card includes a link to a paper on arXiv, the Hugging Face Hub will extract the arXiv ID  and include it in the model tags with the format `arxiv:<PAPER ID>`. Clicking on the tag will let you:\n\n* Visit the paper page\n* Filter for other models on the Hub that cite the same paper.", "The Hub will attempt to automatically match paper to users based on their email. \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors.png\"/>\n<img class=\"hidden dark:block\" width=\"300\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/papers-authors-dark.png\"/>\n</div>", "## Authentication\n\nIn a lot of cases, you must be authenticated with a Hugging Face account to interact with\nthe Hub: download private repos, upload files, create PRs,...\n[Create an account](https://huggingface.co/join) if you don't already have one, and then sign in\nto get your [User Access Token](https://huggingface.co/docs/hub/security-tokens) from\nyour [Settings page](https://huggingface.co/settings/tokens). The User Access Token is\nused to authenticate your identity to the Hub.\n\n<Tip>\n\nTokens can have `read` or `write` permissions. Make sure to have a `write` access token if you want to create or edit a repository. Otherwise, it's best to generate a `read` token to reduce risk in case your token is inadvertently leaked.", "## Frequently Asked Questions\n\n### Can I control which Paper pages show in my profile?\n\nYes! You can visit your Papers in [settings](https://huggingface.co/settings/papers), where you will see a list of verified papers. There, you can click the \"Show on profile\" checkbox to hide/show it in your profile. \n\n### Do you support ACL anthology?\n\nWe're starting with Arxiv as it accounts for 95% of the paper URLs Hugging Face users have linked in their repos organically. We'll check how this evolve and potentially extend to other paper hosts in the future.\n\n### Can I have a Paper page even if I have no model/dataset/Space?"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer, detailing the process of claiming authorship on the Hugging Face Hub by clicking on the user's name, claiming authorship, and confirming the request for the admin team to validate."}, {"question": "What is the purpose of the /healthcheck endpoint in the Datasets server API?\n", "true_answer": "Ensure the app is running", "source_doc": "huggingface/datasets-server/blob/main/services/api/README.md", "generated_answer": "The purpose of the `/healthcheck` endpoint in the Datasets server API is to check the health status of the server. It is configured using the Uvicorn server, as indicated by the environment variables prefixed with `WORKER_UVICORN_`. This endpoint helps ensure that the server is running properly and is accessible. (Document 5)", "retrieved_docs": ["Check dataset validity\n\nBefore you download a dataset from the Hub, it is helpful to know if a specific dataset you're interested in is available. Datasets Server provides the `/is-valid` endpoint to check if a specific dataset works without any errors.\n\nThe API endpoint will return an error for datasets that cannot be loaded with the [\ud83e\udd17 Datasets](https://github.com/huggingface/datasets) library, for example, because the data hasn't been uploaded or the format is not supported.", "Get dataset information\n\nDatasets Server provides an `/info` endpoint for exploring the general information about dataset, including such fields as description, citation, homepage, license and features.\n\nThe `/info` endpoint accepts two query parameters:\n\n- `dataset`: the dataset name\n- `config`: the configuration name", "Quickstart\n\n[[open-in-colab]]\n\nIn this quickstart, you'll learn how to use the Datasets Server's REST API to:\n\n- Check whether a dataset on the Hub is functional.\n- Return the configuration and splits of a dataset.\n- Preview the first 100 rows of a dataset.\n- Download slices of rows of a dataset.\n- Search a word in a dataset.\n- Access the dataset as parquet files.\n\n## API endpoints\n\nEach feature is served through an endpoint summarized in the table below:", "Datasets server SSE API\n\n> Server-sent events API for the Datasets server. It's used to update the Hub's backend cache.\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server", "Datasets server API - rows endpoint\n\n> /rows endpoint\n\n## Configuration\n\nThe service can be configured using environment variables. They are grouped by scope.\n\n### API service\n\nSee [../../libs/libapi/README.md](../../libs/libapi/README.md) for more information about the API configuration.\n\n### Common\n\nSee [../../libs/libcommon/README.md](../../libs/libcommon/README.md) for more information about the common configuration.\n\n## Endpoints\n\nSee https://huggingface.co/docs/datasets-server", "Datasets server - worker\n\n> Workers that pre-compute and cache the response to /splits, /first-rows, /parquet, /info and /size.\n\n## Configuration\n\nUse environment variables to configure the workers. The prefix of each environment variable gives its scope.\n\n### Uvicorn\n\nThe following environment variables are used to configure the Uvicorn server (`WORKER_UVICORN_` prefix). It is used for the /healthcheck and the /metrics endpoints:\n\n- `WORKER_UVICORN_HOSTNAME`: the hostname. Defaults to `\"localhost\"`.\n- `WORKER_UVICORN_NUM_WORKERS`: the number of uvicorn workers. Defaults to `2`.\n- `WORKER_UVICORN_PORT`: the port. Defaults to `8000`.\n\n### Prometheus", "If you have access to the internal HF notion, see https://www.notion.so/huggingface2/Datasets-server-464848da2a984e999c540a4aa7f0ece5.\n\nThe application is distributed in several components.\n\n[api](./services/api) is a web server that exposes the [API endpoints](https://huggingface.co/docs/datasets-server). Apart from some endpoints (`valid`, `is-valid`), all the responses are served from pre-computed responses. That's the main point of this project: generating these responses takes time, and the API server provides this service to the users."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided correctly identifies the purpose of the `/healthcheck` endpoint as a means to check the health status of the server, which aligns with the reference answer's indication that the endpoint is used to ensure the app is running. The additional details about the configuration using Uvicorn and environment variables, while not necessary to answer the question, do not detract from the accuracy of the response. Therefore, the response is completely correct, accurate, and factual."}, {"question": "What is the default context window size for Local Attention in the LongT5 model?\n", "true_answer": "127 tokens", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/longt5.md", "generated_answer": "The context does not provide information about the default context window size for Local Attention in the LongT5 model.", "retrieved_docs": ["## Local attention\n\n[Longformer](#longformer) uses local attention: often, the local context (e.g., what are the two tokens to the\nleft and right?) is enough to take action for a given token. Also, by stacking attention layers that have a small\nwindow, the last layer will have a receptive field of more than just the tokens in the window, allowing them to build a\nrepresentation of the whole sentence.\n\nSome preselected input tokens are also given global attention: for those few tokens, the attention matrix can access\nall tokens and this process is symmetric: all other tokens have access to those specific tokens (on top of the ones in\ntheir local window). This is shown in Figure 2d of the paper, see below for a sample attention mask:", "## Longformer Self Attention\n\nLongformer self attention employs self attention on both a \"local\" context and a \"global\" context. Most tokens only\nattend \"locally\" to each other meaning that each token attends to its \\\\(\\frac{1}{2} w\\\\) previous tokens and\n\\\\(\\frac{1}{2} w\\\\) succeeding tokens with \\\\(w\\\\) being the window length as defined in\n`config.attention_window`. Note that `config.attention_window` can be of type `List` to define a\ndifferent \\\\(w\\\\) for each layer. A selected few tokens attend \"globally\" to all other tokens, as it is\nconventionally done for all tokens in `BertSelfAttention`.", "Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to\n\\\\(\\mathcal{O}(n_s \\times w)\\\\), with \\\\(n_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize. It is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of\n\"locally\" attending tokens.\n\nFor more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf).\n\n\n## Training", "* The authors proposed the dilated windowed self-attention (Figure c) and showed that it yields better results on language modeling compared to just windowed/sparse self-attention (Figure b). The window sizes are increased through the layers. This pattern further outperforms previous architectures (such as Transformer-XL, or adaptive span attention) on downstream benchmarks.\n* Global attention allows the information to flow through the whole sequence and applying the global attention to task-motivated tokens (such as the tokens of the question in QA, CLS token for sentence classification) leads to stronger performance on downstream tasks. Using this global pattern, Longformer can be successfully applied to document-level NLP tasks in the transfer learning setting.", "- [`LongT5ForConditionalGeneration`] is an extension of [`T5ForConditionalGeneration`] exchanging the traditional\nencoder *self-attention* layer with efficient either *local* attention or *transient-global* (*tglobal*) attention.\n- Unlike the T5 model, LongT5 does not use a task prefix. Furthermore, it uses a different pre-training objective\ninspired by the pre-training of [`PegasusForConditionalGeneration`].\n- LongT5 model is designed to work efficiently and very well on long-range *sequence-to-sequence* tasks where the\ninput sequence exceeds commonly used 512 tokens. It is capable of handling input sequences of a length up to 16,384 tokens.\n- For *Local Attention*, the sparse sliding-window local attention operation allows a given token to attend only `r`", "In this case the context is not too long, but some of the examples in the dataset have very long contexts that will exceed the maximum length we set (which is 384 in this case). As we saw in [Chapter 6](/course/chapter6/4) when we explored the internals of the `question-answering` pipeline, we will deal with long contexts by creating several training features from one sample of our dataset, with a sliding window between them.\n\nTo see how this works using the current example, we can limit the length to 100 and use a sliding window of 50 tokens. As a reminder, we use:", ". It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse attention matrix instead of a full attention matrix."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided does not answer the question regarding the default context window size for Local Attention in the LongT5 model. The reference answer clearly states that the default context window size is 127 tokens, which is a specific and factual piece of information. Since the response does not mention this fact at all, it fails to meet the criteria of being correct, accurate, and factual."}, {"question": "What method is used to load a checkpoint for a task using `AutoPipeline`?\n", "true_answer": "from_pretrained()", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/auto_pipeline.md", "generated_answer": "The method used to load a checkpoint for a task using `AutoPipeline` is the `from_pretrained()` method. [Document 1]", "retrieved_docs": ["The `AutoPipeline` class is designed to simplify the variety of pipelines in \ud83e\udd17 Diffusers. It is a generic, *task-first* pipeline that lets you focus on the task. The `AutoPipeline` automatically detects the correct pipeline class to use, which makes it easier to load a checkpoint for a task without knowing the specific pipeline class name.\n\n<Tip>\n\nTake a look at the [AutoPipeline](../api/pipelines/auto_pipeline) reference to see which tasks are supported. Currently, it supports text-to-image, image-to-image, and inpainting.\n\n</Tip>\n\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline class to load for a specific task, given the pretrained weights.\n\n## Choose an AutoPipeline for your task", "Based on the task, the `AutoPipeline` class automatically retrieves the relevant pipeline given the name or path to the pretrained weights with the `from_pretrained()` method.\n\nTo seamlessly switch between tasks with the same checkpoint without reallocating additional memory, use the `from_pipe()` method to transfer the components from the original pipeline to the new one.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"", "## Use multiple pipelines\n\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory. For example, if you're using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost.\n\nThe [`~AutoPipelineForImage2Image.from_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\"stable-diffusion\"` class pipeline for text-to-image:", "## Choose an AutoPipeline for your task\n\nStart by picking a checkpoint. For example, if you're interested in text-to-image with the [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint, use [`AutoPipelineForText2Image`]:\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n).to(\"cuda\")\nprompt = \"peasant and dragon combat, wood cutting style, viking era, bevel with rune\"", "# use from_pipe to avoid consuming additional memory when loading a checkpoint\npipeline = AutoPipelineForInpainting.from_pipe(pipeline_text2image).to(\"cuda\")\n\nimg_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\nmask_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-inpaint-mask.png\"\n\ninit_image = load_image(img_url)\nmask_image = load_image(mask_url)", "# use from_pipe to avoid consuming additional memory when loading a checkpoint\npipeline = AutoPipelineForImage2Image.from_pipe(pipeline_text2image).to(\"cuda\")\n\nurl = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/sdxl-text2img.png\"\ninit_image = load_image(url)\nprompt = \"a dog catching a frisbee in the jungle\"\nimage = pipeline(prompt, image=init_image, strength=0.8, guidance_scale=10.5).images[0]\nmake_image_grid([init_image, image], rows=1, cols=2)\n```", "```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\n\npipeline = AutoPipelineForText2Image.from_pretrained(\"stabilityai/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\npipeline = pipeline.to(\"cuda\")\n```\n\nYou can also use the [`~StableDiffusionXLPipeline.from_single_file`] method to load a model checkpoint stored in a single file format (`.ckpt` or `.safetensors`) from the Hub or locally:\n\n```py\nfrom diffusers import StableDiffusionXLPipeline\nimport torch"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that the `from_pretrained()` method is used to load a checkpoint for a task using `AutoPipeline`. The inclusion of \"[Document 1]\" suggests that the response is backed by a source, although the source is not provided in the evaluation materials. However, this does not affect the correctness of the response."}, {"question": "What is the purpose of Diffusers library?\n", "true_answer": "To serve as a modular toolbox for both inference and training of state-of-the-art pretrained diffusion models across multiple modalities.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/philosophy.md", "generated_answer": "The purpose of the Diffusers library is to provide a modular toolbox for state-of-the-art pretrained diffusion models, enabling the generation of images, audio, and even 3D structures of molecules. It is designed to support both simple inference solutions and the training of custom diffusion models, with a focus on usability, simplicity, and customizability. The library aims to democratize machine learning by allowing users to experiment, research, or play with text-to-image models, and it promotes accessible usage and innovation in generative AI applications. [Document 0, Document 2]", "retrieved_docs": ["# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:", "Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub", "At Hugging Face, our mission is to democratize good machine learning by collaborating and helping each other build an open and ethical AI future together. Our mission motivated us to create the \ud83e\udd17 Diffusers library so *everyone* can experiment, research, or simply play with text-to-image models. That\u2019s why we designed the library as a modular toolbox, so you can customize a diffusion model\u2019s components or just start using it out-of-the-box.\n\nAs \ud83e\udd17 Diffusers turns 1, here\u2019s an overview of some of the most notable features we\u2019ve added to the library with the help of our community. We are proud and immensely grateful for being part of an engaged community that promotes accessible usage, pushes diffusion models beyond just text-to-image generation, and is an all-around inspiration.\n\n**Table of Contents**", "## Diffusers\n\nDiffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA is an especially popular training method for diffusion models because you can very quickly train and share diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers uses the PEFT library to help manage different adapters for inference.", "\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:", "### 9. Adding pipelines, models, schedulers\n\nPipelines, models, and schedulers are the most important pieces of the Diffusers library.\nThey provide easy access to state-of-the-art diffusion technologies and thus allow the community to\nbuild powerful generative AI applications.\n\nBy adding a new model, pipeline, or scheduler you might enable a new powerful use case for any of the user interfaces relying on Diffusers which can be of immense value for the whole generative AI ecosystem.", "For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).\nIn short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.\nFunctions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct, accurate, and factual. It correctly identifies the Diffusers library as a modular toolbox for pretrained diffusion models and mentions its capabilities in generating images, audio, and 3D structures, which aligns with the reference answer's mention of multiple modalities. The response also correctly notes the library's design for both inference and training, its focus on usability, simplicity, and customizability, and its aim to democratize machine learning and promote innovation in generative AI applications. Although the response includes additional details not present in the reference answer, these details are factual and contribute to a fuller understanding of the library's purpose. Therefore, the response aligns well with the reference answer and meets the criteria for a high score."}, {"question": "What method does the EulerAncestralDiscreteScheduler use for sampling?\n", "true_answer": "Ancestral sampling with Euler method steps.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/schedulers/euler_ancestral.md", "generated_answer": "The EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps. [Document 0]", "retrieved_docs": ["# EulerAncestralDiscreteScheduler\n\nA scheduler that uses ancestral sampling with Euler method steps. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72) implementation by [Katherine Crowson](https://github.com/crowsonkb/).\n\n## EulerAncestralDiscreteScheduler\n[[autodoc]] EulerAncestralDiscreteScheduler", "# EulerDiscreteScheduler\n\nThe Euler scheduler (Algorithm 2) is from the [Elucidating the Design Space of Diffusion-Based Generative Models](https://huggingface.co/papers/2206.00364) paper by Karras et al. This is a fast scheduler which can often generate good outputs in 20-30 steps. The scheduler is based on the original [k-diffusion](https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51) implementation by [Katherine Crowson](https://github.com/crowsonkb/).", "[`EulerDiscreteScheduler`] and [`EulerAncestralDiscreteScheduler`] can generate high quality results with as little as 30 steps.\n\n```python\nfrom diffusers import EulerDiscreteScheduler\n\npipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```", "### Swapping schedulers\n\nDifferent schedulers come with different denoising speeds and quality trade-offs. The best way to find out which one works best for you is to try them out! One of the main features of \ud83e\udde8 Diffusers is to allow you to easily switch between schedulers. For example, to replace the default [`PNDMScheduler`] with the [`EulerDiscreteScheduler`], load it with the [`~diffusers.ConfigMixin.from_config`] method:\n\n```py\n>>> from diffusers import EulerDiscreteScheduler", "### Stable Diffusion ControlNet Reference\n\nThis pipeline uses the Reference Control with ControlNet. Refer to the [sd-webui-controlnet discussion: Reference-only Control](https://github.com/Mikubill/sd-webui-controlnet/discussions/1236)[sd-webui-controlnet discussion: Reference-adain Control](https://github.com/Mikubill/sd-webui-controlnet/discussions/1280).\n\nBased on [this issue](https://github.com/huggingface/diffusers/issues/3566),\n- `EulerAncestralDiscreteScheduler` got poor results.\n- `guess_mode=True` works well for ControlNet v1.1", "<p align=\"center\">\n    <br>\n    <img src=\"https://huggingface.co/datasets/patrickvonplaten/images/resolve/main/diffusers_docs/astronaut_euler_discrete.png\" width=\"400\"/>\n    <br>\n</p>\n\n\nand:\n\n```python\nfrom diffusers import EulerAncestralDiscreteScheduler\n\npipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(pipeline.scheduler.config)\n\ngenerator = torch.Generator(device=\"cuda\").manual_seed(8)\nimage = pipeline(prompt, generator=generator, num_inference_steps=30).images[0]\nimage\n```", "### Stable Diffusion Reference\n\nThis pipeline uses the Reference Control. Refer to the [sd-webui-controlnet discussion: Reference-only Control](https://github.com/Mikubill/sd-webui-controlnet/discussions/1236)[sd-webui-controlnet discussion: Reference-adain Control](https://github.com/Mikubill/sd-webui-controlnet/discussions/1280).\n\nBased on [this issue](https://github.com/huggingface/diffusers/issues/3566),\n- `EulerAncestralDiscreteScheduler` got poor results.\n\n```py\nimport torch\nfrom diffusers import UniPCMultistepScheduler\nfrom diffusers.utils import load_image"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that the EulerAncestralDiscreteScheduler uses ancestral sampling with Euler method steps."}, {"question": "What is the name of the large multimodal model that can solve image-text tasks and is based on Flamingo?\n", "true_answer": "IDEFICS", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks/idefics.md", "generated_answer": "The large multimodal model that can solve image-text tasks and is based on Flamingo is called IDEFICS. [Document 3]", "retrieved_docs": ["-->\n\n# Image tasks with IDEFICS\n\n[[open-in-colab]]\n\nWhile individual tasks can be tackled by fine-tuning specialized models, an alternative approach \nthat has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. \nFor instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. \nThis approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can \nsolve image-text tasks with a large multimodal model called IDEFICS.", ". For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.*", ". The resulting InstructBLIP models achieve state-of-the-art zero-shot performance across all 13 held-out datasets, substantially outperforming BLIP-2 and the larger Flamingo. Our models also lead to state-of-the-art performance when finetuned on individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG). Furthermore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models.*", "IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the original closed-source model across various image-text understanding benchmarks. It comes in two variants - 80 billion parameters and 9 billion parameters.\n\n<p align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/idefics/Figure_Evals_IDEFICS.png\" width=\"600\" alt=\"Plot comparing the performance of Flamingo, OpenFlamingo and IDEFICS\"/>\n</p>", "## Multimodal\n\nMultimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image. \n\nAlthough multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings.\n\n### Document question answering", "Finally, models such as [MAPL](https://arxiv.org/abs/2210.07179) and [Flamingo](https://arxiv.org/abs/2204.14198) keep both the pre-trained vision encoder and language model frozen. Flamingo sets a new state-of-the-art in few-shot learning on a wide range of open-ended vision and language tasks by adding Perceiver Resampler modules on top of the pre-trained frozen vision model and inserting new cross-attention layers between existing pre-trained and frozen LM layers to condition the LM on visual data.", "# BLIP-2\n\n## Overview\n\nThe BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by\nJunnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer\nencoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://arxiv.org/abs/2204.14198), an 80 billion parameter model, by 8.7%\non zero-shot VQAv2 with 54x fewer trainable parameters."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided is completely incorrect. The name IDEFICS does not match the reference answer, which indicates that the correct name of the large multimodal model based on Flamingo is simply \"IDEFICS.\" There is no additional context or information given in the response that aligns with the correct answer. Therefore, the response does not meet the criteria for accuracy and factuality as per the score rubric."}, {"question": "What is the purpose of the `gradio.Blocks` API?\n", "true_answer": "The `gradio.Blocks` API allows you to have full control over the data flows and layout of your application, enabling the building of complex, multi-step applications.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/7.mdx", "generated_answer": "The purpose of the `gradio.Blocks` API is to provide a low-level approach for designing web applications with more flexible layouts and data flows. It allows developers to have full control over where components appear on the page, handle complex data flows (such as using outputs as inputs to other functions), and update properties or visibility of components based on user interaction. This flexibility enables the creation of very custom and complex applications, such as multi-step interfaces where the output of one model can become the input to another. The `gradio.Blocks` API is designed to be used entirely in Python, allowing for the construction of intricate applications like the popular image generation Automatic1111 Web UI. (Sources: Documents 0, 1, 3, 4)", "retrieved_docs": ["In the previous sections we have explored and created demos using the `Interface` class. In this section we will introduce our **newly developed** low-level API called `gradio.Blocks`.\n\nNow, what's the difference between `Interface` and `Blocks`?\n\n- \u26a1 `Interface`: a high-level API that allows you to create a full machine learning demo simply by providing a list of inputs and outputs.\n\n- \ud83e\uddf1 `Blocks`: a low-level API that allows you to have full control over the data flows and layout of your application. You can build very complex, multi-step applications using `Blocks` (as in \"building blocks\").\n\n\n### Why Blocks \ud83e\uddf1?[[why-blocks-]]", "#### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).", "### Custom Demos with `gr.Blocks`\n\nGradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction \u2014 still all in Python. \n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).", "### Contributors Shoutout:\n\n- [@nhankiet](https://github.com/nhankiet) made their first contribution in [PR 1819](https://github.com/gradio-app/gradio/pull/1819)\n\n## 3.0\n\n###### \ud83d\udd25 Gradio 3.0 is the biggest update to the library, ever.\n\n### New Features:\n\n###### 1. Blocks \ud83e\uddf1\n\nBlocks is a new, low-level API that allows you to have full control over the data flows and layout of your application. It allows you to build very complex, multi-step applications. For example, you might want to:", "Why did we create Blocks? Gradio demos are very easy to build, but what if you want more control over the layout of your demo, or more flexibility on how the data flows?  For example, you might want to:\n\n* Change the layout of your demo instead of just having all of the inputs on the left and outputs on the right\n* Have multi-step interfaces, in which the output of one model becomes the input to the next model, or have more flexible data flows in general\n* Change a component's properties (for example, the choices in a Dropdown) or its visibilty based on user input\n\nThe low-level Blocks API allows you to do all of this, right in Python.\n\nHere's an example of a Blocks demo that creates two simple demos and uses tabs to group them together:\n\n```python\nimport numpy as np\nimport gradio as gr", "Gradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n\n## The **Flag** button in `gradio.Interface`", "This simple example above introduces 4 concepts that underlie Blocks:\n\n1. Blocks allow you to build web applications that combine markdown, HTML, buttons, and interactive components simply by instantiating objects in Python inside of a `with gradio.Blocks` context.\n<Tip>\n\ud83d\ude4bIf you're not familiar with the `with` statement in Python, we recommend checking out the excellent [tutorial](https://realpython.com/python-with-statement/) from Real Python. Come back here after reading that \ud83e\udd17\n</Tip>\nThe order in which you instantiate components matters as each element gets rendered into the web app in the order it was created. (More complex layouts are discussed below)"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by mentioning the full control over data flows and layout, the ability to build complex, multi-step applications, and the flexibility in designing web applications. Additionally, it expands on the reference answer by providing examples of what can be achieved with the `gradio.Blocks` API, such as handling complex data flows and updating component properties based on user interaction. The mention of the API being used entirely in Python and the reference to a specific application (Automatic1111 Web UI) demonstrates a deep understanding of the API's purpose and its practical applications."}, {"question": "What is the purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\"?\n", "true_answer": "The purpose of the two-stage model is to generate a CLIP image embedding given a text caption and then generate an image conditioned on the image embedding.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/pipelines/unclip.md", "generated_answer": "The purpose of the two-stage model proposed in the paper \"Hierarchical Text-Conditional Image Generation with CLIP Latents\" is to first predict CLIP image embeddings from CLIP text embeddings using a Prior Transformer, and then decode these embeddings into an image through a denoising diffusion process. This hierarchical approach allows for the generation of images conditioned on text inputs. (Source: Document 1)", "retrieved_docs": ["- Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) ([Ramesh et al., 2022](https://cdn.openai.com/papers/dall-e-2.pdf)): uses a prior to turn a text caption into a CLIP image embedding, after which a diffusion model decodes it into an image\n- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) ([Saharia et al., 2022](https://arxiv.org/abs/2205.11487)): shows that combining a large pre-trained language model (e.g. T5) with cascaded diffusion works well for text-to-image synthesis", "# Prior Transformer\n\nThe Prior Transformer was originally introduced in [Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) by Ramesh et al. It is used to predict CLIP image embeddings from CLIP text embeddings; image embeddings are predicted through a denoising diffusion process.\n\nThe abstract from the paper is:", "# unCLIP\n\n[Hierarchical Text-Conditional Image Generation with CLIP Latents](https://huggingface.co/papers/2204.06125) is by Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen. The unCLIP model in \ud83e\udd17 Diffusers comes from kakaobrain's [karlo](https://github.com/kakaobrain/karlo).\n\nThe abstract from the paper is following:", "For explanation purposes, we can also initialize the two main pipelines of W\u00fcrstchen individually. W\u00fcrstchen consists of 3 stages: Stage C, Stage B, Stage A. They all have different jobs and work only together. When generating text-conditional images, Stage C will first generate the latents in a very compressed latent space. This is what happens in the `prior_pipeline`. Afterwards, the generated latents will be passed to Stage B, which decompresses the latents into a bigger latent space of a VQGAN. These latents can then be decoded by Stage A, which is a VQGAN, into the pixel-space. Stage B & Stage A are both encapsulated in the `decoder_pipeline`. For more details, take a look at the [paper](https://huggingface.co/papers/2306.00637).", "[Kandinsky 2.1](../api/pipelines/kandinsky) changes the architecture to include an image prior model ([`CLIP`](https://huggingface.co/docs/transformers/model_doc/clip)) to generate a mapping between text and image embeddings. The mapping provides better text-image alignment and it is used with the text embeddings during training, leading to higher quality results. Finally, Kandinsky 2.1 uses a [Modulating Quantized Vectors (MoVQ)](https://huggingface.co/papers/2209.09002) decoder - which adds a spatial conditional normalization layer to increase photorealism - to decode the latents into images.", "# Follow-up reads\n\nNote that the DDPM paper showed that diffusion models are a promising direction for (un)conditional image generation. This has since then (immensely) been improved, most notably for text-conditional image generation. Below, we list some important (but far from exhaustive) follow-up works:", "Generally, a text-to-image model consists of a text encoder that interprets text and a generative model that generates an image from its output.\n\nStable Diffusion uses CLIP, the language-image pre-training model from OpenAI, as its text encoder and a latent diffusion model, which is an improved version of the diffusion model, as the generative model. Stable Diffusion was trained mainly on the English subset of LAION-5B and can generate high-performance images simply by entering text prompts. In addition to its high performance, Stable Diffusion is also easy to use with inference running at a computing cost of about 10GB VRAM GPU."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by explaining the two-stage process of generating images from text inputs using CLIP embeddings and a denoising diffusion process. The response adds detail by mentioning the Prior Transformer, which is a relevant part of the model, and this additional detail does not detract from the accuracy of the answer."}, {"question": "What command is used to install the requirements for a research project using \ud83e\udd17 Transformers?\n", "true_answer": "pip install -r requirements.txt", "source_doc": "huggingface/transformers/blob/main/examples/research_projects/README.md", "generated_answer": "The command used to install the requirements for a research project using \ud83e\udd17 Transformers is:\n\n```bash\npip install -r requirements.txt\n```\n\nThis command should be run inside the folder of the specific research project. [Document 5]", "retrieved_docs": ["Research projects\n\nThis folder contains various research projects using \ud83e\udde8 Diffusers.\nThey are not really maintained by the core maintainers of this library and often require a specific version of Diffusers that is indicated in the requirements file of each folder.\nUpdating them to the most recent version of the library will require some work.\n\nTo use any of them, just run the command\n\n```\npip install -r requirements.txt\n```\ninside the folder of your choice.\n\nIf you need help with any of those, please open an issue where you directly ping the author(s), as indicated at the top of the README of each folder.", "## Install with pip\n\nYou should install \ud83e\udd17 Transformers in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, take a look at this [guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/). A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies.\n\nStart by creating a virtual environment in your project directory:\n\n```bash\npython -m venv .env\n```\n\nActivate the virtual environment. On Linux and MacOs:\n\n```bash\nsource .env/bin/activate\n```\nActivate Virtual environment on Windows\n\n```bash\n.env/Scripts/activate\n```\n\nNow you're ready to install \ud83e\udd17 Transformers with the following command:\n\n```bash\npip install transformers\n```", "\ud83d\udea8 **Do not** work on the `main` branch!\n\n4. Set up a development environment by running the following command in a virtual environment:\n\n   ```bash\n   pip install -e \".[dev]\"\n   ```\n\n   If \ud83e\udd17 Transformers was already installed in the virtual environment, remove\n   it with `pip uninstall transformers` before reinstalling it in editable\n   mode with the `-e` flag.\n   \n   Depending on your OS, and since the number of optional dependencies of Transformers is growing, you might get a\n   failure with this command. If that's the case make sure to install the Deep Learning framework you are working with\n   (PyTorch, TensorFlow and/or Flax) then do:\n\n   ```bash\n   pip install -e \".[quality]\"\n   ```\n\n   which should be enough for most use cases.\n\n5. Develop the features in your branch.", "# How to add a new example script in \ud83e\udd17 Transformers\n\nThis folder provide a template for adding a new example script implementing a training or inference task with the\nmodels in the \ud83e\udd17 Transformers library. To use it, you will need to install cookiecutter:\n```\npip install cookiecutter\n```\nor refer to the installation page of the [cookiecutter documentation](https://cookiecutter.readthedocs.io/).\n\nYou can then run the following command inside the `examples` folder of the transformers repo:\n```\ncookiecutter ../templates/adding_a_new_example_script/\n```\nand answer the questions asked, which will generate a new folder where you will find a pre-filled template for your\nexample following the best practices we recommend for them.", "## Editable install\n\nYou will need an editable install if you'd like to:\n\n* Use the `main` version of the source code.\n* Contribute to \ud83e\udd17 Transformers and need to test changes in the code.\n\nClone the repository and install \ud83e\udd17 Transformers with the following commands:\n\n```bash\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\npip install -e .\n```\n\nThese commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in `~/anaconda3/envs/main/lib/python3.7/site-packages/`, Python will also search the folder you cloned to: `~/transformers/`.\n\n<Tip warning={true}>", "!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# Research projects\n\nThis folder contains various research projects using \ud83e\udd17 Transformers. They are not maintained and require a specific\nversion of \ud83e\udd17 Transformers that is indicated in the requirements file of each folder. Updating them to the most recent version of the library will require some work.", "```\nwhich python\n```\n\n```out\n/home/<user>/transformers-course/.env/bin/python\n```\n\n### Installing dependencies[[installing-dependencies]]\n\nAs in the previous section on using Google Colab instances, you'll now need to install the packages required to continue. Again, you can install the development version of \ud83e\udd17 Transformers using the `pip` package manager:\n\n```\npip install \"transformers[sentencepiece]\"\n```\n\nYou're now all set up and ready to go!"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer and includes the correct command to install the requirements for a research project using \ud83e\udd17 Transformers. Additionally, it provides helpful context by suggesting that the command should be run inside the folder of the specific research project, which aligns with best practices for installing dependencies."}, {"question": "What task does the `roberta-large-mnli` checkpoint perform?\n", "true_answer": "Text classification", "source_doc": "huggingface/course/blob/main/chapters/en/chapter1/10.mdx", "generated_answer": "The `roberta-large-mnli` checkpoint performs the task of natural language inference (NLI), also known as recognizing textual entailment.", "retrieved_docs": ["!-- DISABLE-FRONTMATTER-SECTIONS -->\n\n# End-of-chapter quiz[[end-of-chapter-quiz]]\n\n<CourseFloatingBanner\n    chapter={1}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nThis chapter covered a lot of ground! Don't worry if you didn't grasp all the details; the next chapters will help you understand how things work under the hood.\n\nFirst, though, let's test what you learned in this chapter!\n\n\n### 1. Explore the Hub and look for the `roberta-large-mnli` checkpoint. What task does it perform?", "model_checkpoint = \"roberta-large\"\nlr = 1e-3\nbatch_size = 16\nnum_epochs = 10\n```\n\n## Load dataset and metric\n\nThe [BioNLP2004](https://huggingface.co/datasets/tner/bionlp2004) dataset includes tokens and tags for biological structures like DNA, RNA and proteins. Load the dataset:", "## Setup\n\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX_LEN=512` for all models to ensure a fair comparison.\n\n```python\nMAX_LEN = 512 \nroberta_checkpoint = \"roberta-large\"\nmistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"\nllama_checkpoint = \"meta-llama/Llama-2-7b-hf\"\n```\n\n## Data preparation\n### Data loading\n\nWe will load the dataset from Hugging Face:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"mehdiiraqui/twitter_disaster\")\n```\n Now, let's split the dataset into training and validation datasets. Then add the test set:", "There are total **3 checkpoints** available in **\ud83e\udd17Hub** (at the point of writing this article): [`bigbird-roberta-base`](https://huggingface.co/google/bigbird-roberta-base), [`bigbird-roberta-large`](https://huggingface.co/google/bigbird-roberta-large), [`bigbird-base-trivia-itc`](https://huggingface.co/google/bigbird-base-trivia-itc). The first two checkpoints come from pretraining `BigBirdForPretraining` with `masked_lm loss`; while the last one corresponds to the checkpoint after finetuning `BigBirdForQuestionAnswering` on `trivia-qa` dataset.", "## Models\n\n### RoBERTa\n\n#### Load RoBERTa Checkpoints for the Classification Task\n\nWe load the pre-trained RoBERTa model with a sequence classification head using the Hugging Face `AutoModelForSequenceClassification` class:\n\n```python\nfrom transformers import AutoModelForSequenceClassification \nroberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)\n```\n\n\n####  LoRA setup for RoBERTa classifier\n\nWe import LoRa configuration and set some parameters for RoBERTa classifier:\n- TaskType: Sequence classification\n- r(rank): Rank for our decomposition matrices\n- lora_alpha: Alpha parameter to scale the learned weights. LoRA paper advises fixing alpha at 16\n- lora_dropout: Dropout probability of the LoRA layers\n- bias: Whether to add bias term to LoRa layers", "TensorFlow 2 API was used.\n- The most important part of the BigBird code-base is [bigbird.bigbird.core](https://github.com/google-research/bigbird/tree/master/bigbird/core) which includes all logic necessary \n  to implement BigBird.\n- The first goal should be to successfully run a forward pass using the RoBERTa checkpoint `bigbr_base/model.ckpt-0.data-00000-of-00001` and `bigbr_base/model.ckpt-0.index`.", "- [Setup](#setup)\n    - [Data preparation](#data-preparation)\n        - [Data loading](#data-loading)\n        - [Data Processing](#data-processing)\n    - [Models](#models)\n        - [RoBERTa](#roberta)\n            - [Load RoBERTA Checkpoints for the Classification Task](#load-roberta-checkpoints-for-the-classification-task)\n            - [LoRA setup for RoBERTa classifier](#lora-setup-for-roberta-classifier)\n        - [Mistral](#mistral)\n            - [Load checkpoints for the classfication model](#load-checkpoints-for-the-classfication-model)\n            - [LoRA setup for Mistral 7B classifier](#lora-setup-for-mistral-7b-classifier)\n        - [Llama 2](#llama-2)"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided is somewhat correct and factual. The `roberta-large-mnli` model is indeed used for natural language inference, which is a type of text classification task. However, the response could be more aligned with the reference answer by explicitly stating \"text classification\" as the task. The response is accurate in describing the specific type of text classification that `roberta-large-mnli` performs, but it does not use the general term provided in the reference answer. Therefore, the response is not completely correct as per the score rubric."}, {"question": "What service is replacing the Paid tier of the Inference API at Hugging Face?\n", "true_answer": "Inference Endpoints", "source_doc": "huggingface/blog/blob/main/pricing-update.md", "generated_answer": "The service replacing the Paid tier of the Inference API at Hugging Face is the \"Inference Endpoints.\" [Document 1]", "retrieved_docs": ["## Demos with the Hugging Face Inference API\n\nHugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:\n\n```python\nimport gradio as gr", "--\ntitle: Introducing our new pricing\nthumbnail: /blog/assets/114_pricing-update/thumbnail.png\nauthors:\n- user: sbrandeis\n- user: pierric\n---\n\n# Introducing our new pricing\n\n\nAs you might have noticed, our [pricing page](https://huggingface.co/pricing) has changed a lot recently.\n\nFirst of all, we are sunsetting the Paid tier of the Inference API service. The Inference API will still be available for everyone to use for free. But if you're looking for a fast, enterprise-grade inference as a service, we recommend checking out our brand new solution for this: [Inference Endpoints](https://huggingface.co/inference-endpoints).", "After an extensive three-month evaluation period, we chose Hugging Face to host our models. During this time, we gradually increased transaction volume to their hosted models and ran numerous simulated load tests based on our worst-case scenario volumes. This process allowed us to fine-tune our system and monitor performance, ultimately giving us confidence in the inference API's ability to handle our transaction enrichment loads.\n\nBeyond technical capabilities, we also established a strong rapport with the team at Hugging Face. We discovered they were not just service providers, but partners who were invested in our goals and outcomes. Early in our collaboration we set up a shared Slack channel which proved invaluable. We were particularly impressed by their prompt response to issues and proactive approach to problem-solving. Their engineers and CSMs consistently demonstrated their commitment in our success and dedication to doing things right. This gave us an additional layer of confidence when it was time to make the final selection.\n\n## Integration, Evaluation, and the Final Selection", "Hugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that benefit from ultra-fast inference powered by [text-generation-inference](https://github.com/huggingface/text-generation-inference). This is a benefit on top of the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on 200,000+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the best models available today.\n\n## Contents", "# Hugging Face Platform on the AWS Marketplace: Pay with your AWS Account\n\n\nThe [Hugging Face Platform](https://aws.amazon.com/marketplace/pp/prodview-n6vsyhdjkfng2) has landed on the AWS Marketplace. Starting today, you can subscribe to the Hugging Face Platform through AWS Marketplace to pay for your Hugging Face usage directly with your AWS account. This new integrated billing method makes it easy to manage payment for usage of all our managed services by all members of your organization, including Inference Endpoints, Spaces Hardware Upgrades, and AutoTrain to easily train, test and deploy the most popular machine learning models like Llama 2, StarCoder, or BERT.", "# Run Inference on servers\n\nInference is the process of using a trained model to make predictions on new data. As this process can be compute-intensive,\nrunning on a dedicated server can be an interesting option. The `huggingface_hub` library provides an easy way to call a\nservice that runs inference for hosted models. There are several services you can connect to:\n- [Inference API](https://huggingface.co/docs/api-inference/index): a service that allows you to run accelerated inference\non Hugging Face's infrastructure for free. This service is a fast way to get started, test different models, and\nprototype AI products.\n- [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index): a product to easily deploy models to production.\nInference is run by Hugging Face in a dedicated, fully managed infrastructure on a cloud provider of your choice.", ". Don\u2019t have 8 A100s to play with? An inference API, currently backed by Google\u2019s TPU cloud and a FLAX version of the model, also allows quick tests, prototyping, and lower-scale use. You can already play with it on the Hugging Face Hub."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly by stating that \"Inference Endpoints\" is the service replacing the Paid tier of the Inference API at Hugging Face. The inclusion of \"[Document 1]\" does not detract from the accuracy of the response."}, {"question": "What architectural feature does SqueezeBERT use instead of fully-connected layers for the Q, K, V, and FFN layers?\n", "true_answer": "Grouped convolutions", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/squeezebert.md", "generated_answer": "SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers. [Document 0]", "retrieved_docs": ["-->\n\n# SqueezeBERT\n\n## Overview\n\nThe SqueezeBERT model was proposed in [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) by Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, Kurt W. Keutzer. It's a\nbidirectional transformer similar to the BERT model. The key difference between the BERT architecture and the\nSqueezeBERT architecture is that SqueezeBERT uses [grouped convolutions](https://blog.yani.io/filter-group-tutorial)\ninstead of fully-connected layers for the Q, K, V and FFN layers.\n\nThe abstract from the paper is the following:", "techniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*", "This model was contributed by [forresti](https://huggingface.co/forresti).\n\n## Usage tips\n\n- SqueezeBERT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right\n  rather than the left.\n- SqueezeBERT is similar to BERT and therefore relies on the masked language modeling (MLM) objective. It is therefore\n  efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained\n  with a causal language modeling (CLM) objective are better in that regard.\n- For best results when finetuning on sequence classification tasks, it is recommended to start with the\n  *squeezebert/squeezebert-mnli-headless* checkpoint.\n\n## Resources", "<!--\nType: model-index\nCollections:\n- Name: Legacy SENet\n  Paper:\n    Title: Squeeze-and-Excitation Networks\n    URL: https://paperswithcode.com/paper/squeeze-and-excitation-networks\nModels:\n- Name: legacy_senet154\n  In Collection: Legacy SENet\n  Metadata:\n    FLOPs: 26659556016\n    Parameters: 115090000\n    File Size: 461488402\n    Architecture:\n    - Convolution\n    - Dense Connections\n    - Global Average Pooling\n    - Max Pooling\n    - Softmax\n    - Squeeze-and-Excitation Block\n    Tasks:\n    - Image Classification\n    Training Techniques:\n    - Label Smoothing\n    - SGD with Momentum\n    - Weight Decay\n    Training Data:\n    - ImageNet\n    Training Resources: 8x NVIDIA Titan X GPUs\n    ID: legacy_senet154\n    LR: 0.6\n    Epochs: 100\n    Layers: 154\n    Dropout: 0.2\n    Crop Pct: '0.875'", "* Halo - https://arxiv.org/abs/2103.12731\n    * Involution - https://arxiv.org/abs/2103.06255\n    * Lambda Layer - https://arxiv.org/abs/2102.08602\n    * Non-Local (NL) -  https://arxiv.org/abs/1711.07971\n    * Squeeze-and-Excitation (SE) - https://arxiv.org/abs/1709.01507\n    * Selective Kernel (SK) - (https://arxiv.org/abs/1903.06586\n    * Split (SPLAT) - https://arxiv.org/abs/2004.08955\n    * Shifted Window (SWIN) - https://arxiv.org/abs/2103.14030", "Therefore, the architecture can also naturally benefit from classic acceleration and compression techniques (such as [ONNX](https://github.com/harrisonvanderbyl/rwkv-onnx), 4-bit/8-bit quantization, etc.), and we hope this will be democratized for developers and practitioners together with the transformers integration of the architecture.", "1. **[SqueezeBERT](https://huggingface.co/docs/transformers/model_doc/squeezebert)** (Berkeley \uc5d0\uc11c) Forrest N. Iandola, Albert E. Shaw, Ravi Krishna, and Kurt W. Keutzer \uc758 [SqueezeBERT: What can computer vision teach NLP about efficient neural networks?](https://arxiv.org/abs/2006.11316) \ub17c\ubb38\uacfc \ud568\uaed8 \ubc1c\ud45c\ud588\uc2b5\ub2c8\ub2e4."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns perfectly with the reference answer, stating that SqueezeBERT uses grouped convolutions instead of fully-connected layers for the Q, K, V, and FFN layers."}, {"question": "What type of license is the HuggingFace Team's software distributed under?\n", "true_answer": "Apache License, Version 2.0", "source_doc": "huggingface/simulate/blob/main/docs/source/tutorials/interaction.mdx", "generated_answer": "The HuggingFace Team's software is distributed under the Apache License, Version 2.0. [Document 0]", "retrieved_docs": ["!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n# How to contribute to huggingface_hub, the GitHub repository?\n\nEveryone is welcome to contribute, and we value everybody's contribution. Code is not the only way to help the community.\nAnswering questions, helping others, reaching out and improving the documentations are immensely valuable to the community.", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# KDPM2AncestralDiscreteScheduler", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# KDPM2DiscreteScheduler", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Text-to-image\n\n[[open-in-colab]]", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# Quickstart\n\n[[open-in-colab]]", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# UNet1DModel", "!--Copyright 2023 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\nthe License. You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on\nan \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\nspecific language governing permissions and limitations under the License.\n-->\n\n# UNet2DModel"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that the HuggingFace Team's software is distributed under the Apache License, Version 2.0, which is the exact license mentioned in the reference answer."}, {"question": "What are the two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed?\n", "true_answer": "Splitting the embedding matrix into two smaller matrices and using repeating layers split among groups.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/albert.md", "generated_answer": "The two parameter-reduction techniques proposed in the ALBERT model to lower memory consumption and increase training speed are: 1) splitting the embedding matrix into two smaller matrices, and 2) using repeating layers split among groups. [Document 0]", "retrieved_docs": ["## Overview\n\nThe ALBERT model was proposed in [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,\nRadu Soricut. It presents two parameter-reduction techniques to lower memory consumption and increase the training\nspeed of BERT:\n\n- Splitting the embedding matrix into two smaller matrices.\n- Using repeating layers split among groups.\n\nThe abstract from the paper is the following:", "However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths", "Depending on your GPU, you may need to enable `gradient_checkpointing` (\u26a0\ufe0f not supported for the prior model!), `mixed_precision`, and `gradient_accumulation_steps` to help fit the model into memory and to speedup training. You can reduce your memory-usage even more by enabling memory-efficient attention with [xFormers](../optimization/xformers) (version [v0.0.16](https://github.com/huggingface/diffusers/issues/2234#issuecomment-1416931212) fails for training on some GPUs so you may need to install a development version instead).", "To speedup training and reduce memory-usage, we recommend:\n\n- using PyTorch 2.0 or higher to automatically use [scaled dot product attention](../optimization/torch2.0#scaled-dot-product-attention) during training (you don't need to make any changes to the training code)\n- installing [xFormers](../optimization/xformers) to enable memory-efficient attention", "If you're training on a GPU with limited vRAM, try enabling `gradient_checkpointing`, `gradient_accumulation_steps`, and `mixed_precision` to reduce memory-usage and speedup training. You can reduce your memory-usage even more by enabling memory-efficient attention with [xFormers](../optimization/xformers) and [bitsandbytes'](https://github.com/TimDettmers/bitsandbytes) 8-bit optimizer.", "The dominant strategy to improve performance is to increase the model size. But training large models is computationally expensive. One way to reduce computational costs is using a smaller model like [DistilBERT](model_doc/distilbert). DistilBERT uses [knowledge distillation](https://arxiv.org/abs/1503.02531) - a compression technique - to create a smaller version of BERT while keeping nearly all of its language understanding capabilities.", "* Data throughput/training time\n* Model performance\n\nMaximizing the throughput (samples/second) leads to lower training cost. This is generally achieved by utilizing the GPU \nas much as possible and thus filling GPU memory to its limit. If the desired batch size exceeds the limits of the GPU memory, \nthe memory optimization techniques, such as gradient accumulation, can help.\n\nHowever, if the preferred batch size fits into memory, there's no reason to apply memory-optimizing techniques because they can \nslow down the training. Just because one can use a large batch size, does not necessarily mean they should. As part of \nhyperparameter tuning, you should determine which batch size yields the best results and then optimize resources accordingly.\n\nThe methods and tools covered in this guide can be classified based on the effect they have on the training process:"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It correctly identifies the two parameter-reduction techniques as splitting the embedding matrix and using repeating layers. However, the response inaccurately describes the second technique as \"split among groups,\" which is not a precise description of the ALBERT model's approach. The ALBERT model uses cross-layer parameter sharing to reduce the number of parameters, not merely splitting layers among groups. This slight inaccuracy in the description of the second technique affects the overall accuracy of the response."}, {"question": "What are the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library?\n", "true_answer": "1. Load a dataset from the Hugging Face Hub. 2. Preprocess the data with `Dataset.map()`. 3. Load and compute metrics.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter5/1.mdx", "generated_answer": "The three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library are:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics. \n\n(Source: Document 0)", "retrieved_docs": ["Introduction[[introduction]]\n\n<CourseFloatingBanner\n    chapter={5}\n    classNames=\"absolute z-10 right-0 top-0\"\n/>\n\nIn [Chapter 3](/course/chapter3) you got your first taste of the \ud83e\udd17 Datasets library and saw that there were three main steps when it came to fine-tuning a model:\n\n1. Load a dataset from the Hugging Face Hub.\n2. Preprocess the data with `Dataset.map()`.\n3. Load and compute metrics.\n\nBut this is just scratching the surface of what \ud83e\udd17 Datasets can do! In this chapter, we will take a deep dive into the library. Along the way, we'll find answers to the following questions:", "</Tip>\n\nTo fine-tune a model in TensorFlow, follow these steps:\n1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n2. Instantiate a pretrained model.\n3. Convert a \ud83e\udd17 Dataset to a `tf.data.Dataset`.\n4. Compile your model.\n5. Add callbacks to calculate metrics and upload your model to \ud83e\udd17 Hub\n6. Use the `fit()` method to run the training.\n\nStart by defining the hyperparameters, optimizer and learning rate schedule:\n\n```py\n>>> from transformers import create_optimizer", "<frameworkcontent>\n<tf>\n\n<Tip>\n\nIf you are unfamiliar with fine-tuning a model with Keras, check out the [basic tutorial](./training#train-a-tensorflow-model-with-keras) first!\n\n</Tip>\n\nTo fine-tune a model in TensorFlow, follow these steps:\n1. Define the training hyperparameters, and set up an optimizer and a learning rate schedule.\n2. Instantiate a pre-trained model.\n3. Convert a \ud83e\udd17 Dataset to a `tf.data.Dataset`.\n4. Compile your model.\n5. Add callbacks and use the `fit()` method to run the training.\n6. Upload your model to \ud83e\udd17 Hub to share with the community.\n\nStart by defining the hyperparameters, optimizer and learning rate schedule:\n\n```py\n>>> from transformers import create_optimizer", "To fine-tune a model on this dataset you can use the following commands:\n\n```python\npython train_complexity_predictor.py \\\n    --model_ckpt microsoft/unixcoder-base-nine \\\n    --num_epochs 60 \\\n    --num_warmup_steps 10 \\\n    --batch_size 8 \\\n    --learning_rate 5e-4 \n```", "To fine-tune the model on our dataset, we just have to `compile()` our model and then pass our data to the `fit()` method. This will start the fine-tuning process (which should take a couple of minutes on a GPU) and report training loss as it goes, plus the validation loss at the end of each epoch.\n\n<Tip>", "2.4 Of course to fine-tune a model you\u2019ll need to upload \u201cTraining Data\u201d. When you do, make sure the dataset is correctly formatted and in CSV file format. An example of the required format can be found [here](https://huggingface.co/docs/autotrain/main/en/llm_finetuning). If your dataset contains multiple columns, be sure to select the \u201cText Column\u201d from your file that contains the training data. In this example we\u2019ll be using the Alpaca instruction tuning dataset, more information about this dataset is available [here](https://huggingface.co/datasets/tatsu-lab/alpaca). You can also download it directly as CSV from [here](https://huggingface", "## Usage: fine-tuning\n\nHere we explain how you can fine-tune [`TapasForQuestionAnswering`] on your own dataset.\n\n**STEP 1: Choose one of the 3 ways in which you can use TAPAS - or experiment**\n\nBasically, there are 3 different ways in which one can fine-tune [`TapasForQuestionAnswering`], corresponding to the different datasets on which Tapas was fine-tuned:"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, listing the three main steps for fine-tuning a model with the \ud83e\udd17 Datasets library in the correct order and with the appropriate detail."}, {"question": "What is the maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers?\n", "true_answer": "+800%", "source_doc": "huggingface/blog/blob/main/infinity-cpu-performance.md", "generated_answer": "The maximum improvement in throughput achieved by Hugging Face Infinity compared to vanilla transformers is up to 800%. [Document 0]", "retrieved_docs": ["---\n\n## Conclusion\n\nIn this post, we showed how Hugging Face Infinity performs on the new Intel Ice Lake Xeon CPU. We created a detailed benchmark with over 190 different configurations sharing the results you can expect when using Hugging Face Infinity on CPU, what would be the best configuration to optimize your Infinity Container for latency, and what would be the best configuration to maximize throughput.\n\nHugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.\n\nThe flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before.", "## Conclusion\n\nWe successfully managed to compile a vanilla Hugging Face Transformers model to an AWS Inferentia compatible Neuron Model. After that we deployed our Neuron model to Amazon SageMaker using the new Hugging Face Inference DLC. We managed to achieve `5-6ms` latency per neuron core, which is faster than CPU in terms of latency, and achieves a higher throughput than GPUs since we ran 4 models in parallel. \n\nIf you or you company are currently using a BERT-like Transformer for encoder tasks (text-classification, token-classification, question-answering etc.), and the latency meets your requirements you should switch to AWS Inferentia. This will not only save costs, but can also increase efficiency and performance for your models. \n\nWe are planning to do a more detailed case study on cost-performance of transformers in the future, so stay tuned!", "With [Hugging Face Infinity](https://huggingface.co/infinity), we offer a containerized solution that makes it easy to deploy low-latency, high-throughput, hardware-accelerated inference pipelines for the most popular Transformer models. Companies can get both the accuracy of Transformers and the efficiency necessary for large volume deployments, all in a simple to use package. In this blog post, we want to share detailed performance results for Infinity running on the latest generation of Intel Xeon CPU, to achieve optimal cost, efficiency, and latency for your Transformer deployments.\n\n\n## What is Hugging Face Infinity\n\nHugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.", "xlarge |  8            | 16             |  _        | 43 \u00b1 31 (43s)    \n```\nWhat we see from these results is pretty encouraging. The application that will consume these endpoints serves requests in real time, so we need as low latency as possible. We can see that the vanilla Hugging Face container was more than twice as fast as our bespoke container run on ECS \u2014 the slowest response we received from the large Inference Endpoint was just 108ms.", "[Hugging Face Optimum](https://github.com/huggingface/optimum) is an open-source library and an extension of [Hugging Face Transformers](https://github.com/huggingface/transformers), that provides a unified API of performance optimization tools to achieve maximum efficiency to train and run models on accelerated hardware, including toolkits for optimized performance on [Graphcore IPU](https://github.com/huggingface/optimum-graphcore) and [Habana Gaudi](https://github.com/huggingface/optimum-habana). Optimum can be used for accelerated training, quantization, graph optimization, and now inference as well with support for [transformers pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines).", "## Solutions\n\n### **Coming soon: Infinity**\n\nTransformers latency down to 1ms? \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n\nWe have been working on a really sleek solution to achieve unmatched efficiency for state-of-the-art Transformer models, for companies to deploy in their own infrastructure.\n\n- Infinity comes as a single-container and can be deployed in any production environment.\n- It can achieve 1ms latency for BERT-like models on GPU and 4-10ms on CPU \ud83e\udd2f\ud83e\udd2f\ud83e\udd2f\n- Infinity meets the highest security requirements and can be integrated into your system without the need for internet access. You have control over all incoming and outgoing traffic.\n\n\u26a0\ufe0f  Join us for a [live announcement and demo on Sep 28](https://app.livestorm.co/hugging-face/hugging-face-infinity-launch?type=detailed), where we will be showcasing Infinity for the first time in public!\n\n### **NEW: Hardware Acceleration**", "Our collaboration with Intel through the \ud83e\udd17 [Hardware Partner Program](https://huggingface.co/hardware) enables us to make advanced efficiency and optimization techniques easily available to the community, through our new \ud83e\udd17 [Optimum open source library](https://github.com/huggingface/optimum) dedicated to production performance.\n\nFor companies looking to accelerate their Transformer models inference, our new \ud83e\udd17 [Infinity product offers a plug-and-play containerized solution](https://huggingface.co/infinity), achieving down to 1ms latency on GPU and 2ms on Intel Xeon Ice Lake CPUs.\n\nIf you found this post interesting or useful to your work, please consider giving Optimum a star. And if this post was music to your ears, consider [joining our Machine Learning Optimization team](https://apply.workable.com/huggingface/)!"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating the improvement in throughput as \"up to 800%,\" which is the exact figure given in the reference answer."}, {"question": "What is the command to upload a spaCy pipeline to the Hugging Face Hub?\n", "true_answer": "python -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl", "source_doc": "huggingface/blog/blob/main/spacy.md", "generated_answer": "The command to upload a spaCy pipeline to the Hugging Face Hub is:\n\n```bash\npython -m spacy huggingface-hub push <pipeline_name>.whl\n```\n\nThis command is used after packaging the pipeline with `spacy package` and generating a `.whl` file. (Source: Document 5)", "retrieved_docs": ["Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!\n\n### Finding models", "All spaCy canonical pipelines can now be found in the official spaCy [organization](https://huggingface.co/spacy), and any user can share their pipelines with a single command `python -m spacy huggingface-hub`. To read more about it, head to [https://huggingface.co/blog/spacy](https://huggingface.co/blog/spacy). You can try all canonical spaCy models directly in the Hub in the demo [Space](https://huggingface.co/spaces/spacy/pipeline-visualizer)!\n\n![spaCy NER example](assets/27_summer_at_huggingface/spacy_ner.jpeg)", "Once installed, you can load the model as any spaCy pipeline.\n\n```python\n# Using spacy.load().\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Importing as module.\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n```\n\n## Sharing your models\n\n### Using the spaCy CLI (recommended)\n\nThe `spacy-huggingface-hub` library extends `spaCy` native CLI so people can easily push their packaged models to the Hub.\n\nYou can install spacy-huggingface-hub from pip:\n\n```bash\npip install spacy-huggingface-hub\n```\n\nYou can then check if the command has been registered successfully\n\n```bash\npython -m spacy huggingface-hub --help\n```", "Start by creating an instance of a [`DiffusionPipeline`] and specify which pipeline checkpoint you would like to download.\nYou can use the [`DiffusionPipeline`] for any [checkpoint](https://huggingface.co/models?library=diffusers&sort=downloads) stored on the Hugging Face Hub.\nIn this quicktour, you'll load the [`stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5) checkpoint for text-to-image generation.\n\n<Tip warning={true}>", "--\ntitle: \"Welcome spaCy to the Hugging Face Hub\"\nthumbnail: /blog/assets/23_spacy/thumbnail.png\n\nauthors:\n- user: osanseviero\n- user: ines\n---\n\n# Welcome spaCy to the Hugging Face Hub\n\n\n[spaCy](https://github.com/explosion/spaCy) is a popular library for advanced Natural Language Processing used widely across industry. spaCy makes it easy to use and train pipelines for tasks like named entity recognition, text classification, part of speech tagging and more, and lets you build powerful applications to process and analyze large volumes of text.", "You can then upload any pipeline packaged with [`spacy package`](https://spacy.io/api/cli#package). Make sure to set `--build wheel` to output a binary .whl file. The uploader will read all metadata from the pipeline package, including the auto-generated pretty `README.md` and the model details available in the `meta.json`.\n\n```bash\nhuggingface-cli login\npython -m spacy package ./en_ner_fashion ./output --build wheel\ncd ./output/en_ner_fashion-0.0.0/dist\npython -m spacy huggingface-hub push en_ner_fashion-0.0.0-py3-none-any.whl\n```", "```python\ndef package_to_hub(\n    repo_id,\n    model,\n    hyperparameters,\n    eval_env,\n    video_fps=30,\n    commit_message=\"Push agent to the Hub\",\n    token=None,\n    logs=None,\n):\n    \"\"\"\n    Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n    This method does the complete pipeline:\n    - It evaluates the model\n    - It generates the model card\n    - It generates a replay video of the agent\n    - It pushes everything to the hub\n    :param repo_id: id of the model repository from the Hugging Face Hub\n    :param model: trained model\n    :param eval_env: environment used to evaluate the agent\n    :param fps: number of fps for rendering the video\n    :param commit_message: commit message\n    :param logs: directory on local machine of tensorboard logs you'd like to upload\n    \"\"\""], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "4", "eval_feedback_GPT4": "Feedback: The response provided is mostly correct and factual. It correctly identifies the command structure for uploading a spaCy pipeline to the Hugging Face Hub and includes the necessary components such as the command to execute with Python, the module and method to use, and the placeholder for the pipeline file. However, the response could be more accurate by providing an example of a pipeline file name, as shown in the reference answer, instead of a placeholder. This would demonstrate a complete understanding and application of the command."}, {"question": "What is the time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention?\n", "true_answer": "O(n)", "source_doc": "huggingface/blog/blob/main/nystromformer.md", "generated_answer": "The time and memory complexity of the Nystr\u00f6mformer's approximation of self-attention is \\(O(n)\\). [Document 1]", "retrieved_docs": ["--\ntitle: \"Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\"\nthumbnail: /blog/assets/86_nystromformer/thumbnail.png\nauthors:\n- user: asi\n  guest: true\n---\n\n# Nystr\u00f6mformer: Approximating self-attention in linear time and memory via the Nystr\u00f6m method\n\n\n<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n\n## Introduction", "The [Nystr\u00f6mformer](https://arxiv.org/abs/2102.03902) is one of many efficient Transformer models that approximates standard self-attention with \\\\(O(n)\\\\) complexity. Nystr\u00f6mformer exhibits competitive performance on various downstream NLP and CV tasks while improving upon the efficiency of standard self-attention. The aim of this blog post is to give readers an overview of the Nystr\u00f6m method and how it can be adapted to approximate self-attention.\n\n\n## Nystr\u00f6m method for matrix approximation", "This is the Nystr\u00f6m approximation of the softmax matrix in the self-attention mechanism. We multiply this matrix with the values ( \\\\(V\\\\)) to obtain a linear approximation of self-attention. Note that we never calculated the product \\\\(QK^T\\\\), avoiding the \\\\(O(n^2)\\\\) complexity. \n\n\n## How do we select landmarks?", "</div>\n\n## Conclusion\n\nNystr\u00f6mformer offers an efficient approximation to the standard self-attention mechanism, while outperforming other linear self-attention schemes. In this blog post, we went over a high-level overview of the Nystr\u00f6m method and how it can be leveraged for self-attention. Readers interested in deploying or fine-tuning Nystr\u00f6mformer for downstream tasks can find the HuggingFace documentation [here](https://huggingface.co/docs/transformers/model_doc/nystromformer).", "*Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component\nthat drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or\ndependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the\ninput sequence length has limited its application to longer sequences -- a topic being actively studied in the\ncommunity. To address this limitation, we propose Nystr\u00f6mformer -- a model that exhibits favorable scalability as a\nfunction of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention\nwith O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of\ntokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard\nsequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than", "Using Longformer self attention, the memory and time complexity of the query-key matmul operation, which usually\nrepresents the memory and time bottleneck, can be reduced from \\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to\n\\\\(\\mathcal{O}(n_s \\times w)\\\\), with \\\\(n_s\\\\) being the sequence length and \\\\(w\\\\) being the average window\nsize. It is assumed that the number of \"globally\" attending tokens is insignificant as compared to the number of\n\"locally\" attending tokens.\n\nFor more information, please refer to the official [paper](https://arxiv.org/pdf/2004.05150.pdf).\n\n\n## Training", "## How can we adapt the Nystr\u00f6m method to approximate self-attention?\n\nInstead of sampling from \\\\(S\\\\), the authors propose to sample landmarks (or Nystr\u00f6m points) from queries and keys. We denote the query landmarks and key landmarks as \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) respectively. \\\\(\\tilde{Q}\\\\) and \\\\(\\tilde{K}\\\\) can be used to construct three matrices corresponding to those in the Nystr\u00f6m approximation of \\\\(S\\\\). We define the following matrices:"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response provided is partially correct as it mentions the complexity being \\(O(n)\\), which aligns with the reference answer. However, the response fails to specify that this complexity refers to both time and memory, which is a critical aspect of the question. The reference answer succinctly states \"O(n)\" without specifying whether it is time or memory complexity, but the question explicitly asks for both. Therefore, the response is somewhat correct but lacks complete accuracy and detail."}, {"question": "What is the goal of the Named Entity Recognition task in token classification?\n", "true_answer": "The goal of the Named Entity Recognition task is to find the entities in a piece of text, such as person, location, or organization.", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/tasks.md", "generated_answer": "The goal of the Named Entity Recognition (NER) task in token classification is to identify and classify each token in a piece of text into predefined categories such as person, location, organization, or none of them. This involves labeling each token with one class for each entity and another class for tokens that do not correspond to any entity. (Sources: Document 0, Document 1, Document 3)", "retrieved_docs": ["Welcome to the Hugging Face tasks series! In this video we\u2019ll take a look at the token classification task.\nToken classification is the task of assigning a label to each token in a sentence. There are various token classification tasks and the most common are Named Entity Recognition and Part-of-Speech Tagging.\nLet\u2019s take a quick look at the Named Entity Recognition task. The goal of this task is to find the entities in a piece of text, such as person, location, or organization. This task is formulated as labelling each token with one class for each entity, and another class for tokens that have no entity.\nAnother token classification task is part-of-speech tagging. The goal of this task is to label the words for a particular part of a speech, such as noun, pronoun, adjective, verb and so on. This task is formulated as labelling each token with parts of speech.", "-   Named entity recognition (NER) on the Bengali split of [WikiANN](https://aclanthology.org/P17-1178/). The goal of this task is to classify each token in the input text into one of the following categories: person, organization, location, or none of them.\n-   News Category Classification (NCC) on the Soham articles dataset from [IndicGLUE](https://aclanthology.org/2020.findings-emnlp.445/). The goal of this task is to predict the category to which belong the input text.\n\nWe evaluated it during training on the NER task to check that everything was going well; as you can see on the following plot, this was indeed the case!", "<Youtube id=\"wVHdVlPScxA\"/>\n\nToken classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n\nThis guide will show you how to:\n\n1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [WNUT 17](https://huggingface.co/datasets/wnut_17) dataset to detect new entities.\n2. Use your finetuned model for inference.\n\n<Tip>\nThe task illustrated in this tutorial is supported by the following model architectures:\n\n<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->", "Named-Entity Recognition\n\nRelated spaces: https://huggingface.co/spaces/rajistics/biobert_ner_demo, https://huggingface.co/spaces/abidlabs/ner, https://huggingface.co/spaces/rajistics/Financial_Analyst_AI\nTags: NER, TEXT, HIGHLIGHT\n\n## Introduction\n\nNamed-entity recognition (NER), also known as token classification or text tagging, is the task of taking a sentence and classifying every word (or \"token\") into different categories, such as names of people or names of locations, or different parts of speech.\n\nFor example, given the sentence:\n\n> Does Chicago have any Pakistani restaurants?\n\nA named-entity recognition algorithm may identify:\n\n- \"Chicago\" as a **location**\n- \"Pakistani\" as an **ethnicity**\n\nand so on.", "Token classification models are evaluated on Accuracy, Recall, Precision and F1-Score. The metrics are calculated for each of the classes. We calculate true positive, true negative and false positives to calculate precision and recall, and take their harmonic mean to get F1-Score. Then we calculate it for every class and take the overall average to evaluate our model.\nAn example dataset used for this task is ConLL2003. Here, each token belongs to a certain named entity class, denoted as the indices of the list containing the labels.\nYou can extract important information from invoices using named entity recognition models, such as date, organization name or address.\nFor more information about the Token classification task, check out the Hugging Face course.", "### Token classification\n\nTo use BERT for token classification tasks like named entity recognition (NER), add a token classification head on top of the base BERT model. The token classification head is a linear layer that accepts the final hidden states and performs a linear transformation to convert them into logits. The cross-entropy loss is calculated between the logits and each token to find the most likely label.\n\nReady to try your hand at token classification? Check out our complete [token classification guide](tasks/token_classification) to learn how to finetune DistilBERT and use it for inference!\n\n### Question answering", "</Tip>\n\n#### Named Entity Recognition\n\nNamed Entity Recognition (NER) is a task of finding named entities in a piece of text, such as a person, location, or organization.\nLet's modify the instructions in the prompt to make the LLM perform this task. Here, let's also set `return_full_text = False` \nso that output doesn't contain the prompt:\n\n```python\n>>> torch.manual_seed(1) # doctest: +IGNORE_RESULT\n>>> prompt = \"\"\"Return a list of named entities in the text.\n... Text: The Golden State Warriors are an American professional basketball team based in San Francisco.\n... Named entities:\n... \"\"\"\n\n>>> sequences = pipe(\n...     prompt,\n...     max_new_tokens=15,\n...     return_full_text = False,    \n... )"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by stating the goal of the Named Entity Recognition task in token classification, which is to identify and classify tokens into predefined categories such as person, location, organization, or none. The response adds detail by mentioning the labeling process, which is an integral part of NER, although this additional information is not present in the reference answer. However, the extra detail does not detract from the correctness or factual nature of the response."}, {"question": "What is the resolution of images used by the CLIPSeg model?\n", "true_answer": "352 x 352 pixels", "source_doc": "huggingface/blog/blob/main/clipseg-zero-shot.md", "generated_answer": "The CLIPSeg model uses images with a resolution of 352 x 352 pixels. [Document 0]", "retrieved_docs": ["Currently, CLIPSeg still has its limitations. For example, the model uses images of 352 x 352 pixels, so the output is quite low-resolution. This means we cannot expect pixel-perfect results when we work with images from modern cameras. If we want more precise segmentations, we can fine-tune a state-of-the-art segmentation model, as shown in [our previous blog post](https://huggingface.co/blog/fine-tune-segformer). In that case, we can still use CLIPSeg to generate some rough labels, and then refine them in a labeling tool such as [Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg). Before we describe how to do that, let\u2019s first take a look at how CLIPSeg works.", "## CLIPSeg: image segmentation with CLIP\n\n[CLIPSeg](https://arxiv.org/abs/2112.10003) is a model that uses CLIP representations to create image segmentation masks. It was published by Timo L\u00fcddecke and Alexander Ecker. They achieved zero-shot image segmentation by training a Transformer-based decoder on top of the CLIP model, which is kept frozen. The decoder takes in the CLIP representation of an image, and the CLIP representation of the thing you want to segment. Using these two inputs, the CLIPSeg decoder creates a binary segmentation mask. To be more precise, the decoder doesn\u2019t only use the final CLIP representation of the image we want to segment, but it also uses the outputs of some of the layers of CLIP.", "-->\n\n# CLIPSeg\n\n## Overview\n\nThe CLIPSeg model was proposed in [Image Segmentation Using Text and Image Prompts](https://arxiv.org/abs/2112.10003) by Timo L\u00fcddecke\nand Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen [CLIP](clip) model for zero- and one-shot image segmentation.\n\nThe abstract from the paper is the following:", "The reason why CLIP works so well is that the model was trained on a huge dataset of images with text captions. The dataset contained a whopping 400 million image-text pairs taken from the internet. These images contain a wide variety of objects and concepts, and CLIP is great at creating a representation for each of them.\n\n## CLIPSeg: image segmentation with CLIP", "<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png\"\nalt=\"drawing\" width=\"600\"/> \n\n<small> CLIPSeg overview. Taken from the <a href=\"https://arxiv.org/abs/2112.10003\">original paper.</a> </small>\n\nThis model was contributed by [nielsr](https://huggingface.co/nielsr).\nThe original code can be found [here](https://github.com/timojl/clipseg).\n\n## Usage tips", "<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Overview of the CLIPSeg model\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/clip-tv-example.png\"></medium-zoom>\n  <figcaption>Example of image classification using CLIP (<a href=\"https://openai.com/blog/clip/\">source</a>).</figcaption>\n</figure>", "<figure class=\"image table text-center m-0 w-9/12\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Thumbnails of the final segmentation labels on Segments.ai.\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/segments-thumbs.png\"></medium-zoom>\n</figure>\n\n## Conclusion\n\nCLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so you\u2019ll probably still want to fine-tune a different segmentation model if accuracy is important."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual as it matches the reference answer exactly, stating the resolution of images used by the CLIPSeg model as 352 x 352 pixels."}, {"question": "What can you use Gradio for?\n", "true_answer": "Create a demo for your machine learning model, share your machine learning model with others, and debug your model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter9/9.mdx", "generated_answer": "Gradio can be used to create customizable web apps for machine learning models and data processing pipelines. It supports a wide range of data types, not just NLP, and provides developers with a library of pre-built components. Gradio apps can be deployed on platforms like Hugging Face Spaces or on your own web server using tools like Nginx. Additionally, Gradio can be used to query applications programmatically in Python or JavaScript. (Sources: Documents 0, 2, 3, 5)", "retrieved_docs": ["The Gradio client works with any hosted Gradio app, whether it be an image generator, a text summarizer, a stateful chatbot, a tax calculator, or anything else! The Gradio Client is mostly used with apps hosted on [Hugging Face Spaces](https://hf.space), but your app can be hosted anywhere, such as your own server.\n\n**Prequisites**: To use the Gradio client, you do _not_ need to know the `gradio` library in great detail. However, it is helpful to have general familiarity with Gradio's concepts of input and output components.\n\n## Installation\n\nThe lightweight `@gradio/client` package can be installed from the npm registry with a package manager of your choice and support node version 18 and above:\n\n```bash\nnpm i @gradio/client\n```", "## API Page\n\nYou can use almost any Gradio app as an API! In the footer of a Gradio app [like this one](https://huggingface.co/spaces/gradio/hello_world), you'll see a \"Use via API\" link.\n\n![Use via API](https://github.com/gradio-app/gradio/blob/main/guides/assets/use_via_api.png?raw=true)", "Running a Gradio App on your Web Server with Nginx\n\nTags: DEPLOYMENT, WEB SERVER, NGINX\n\n## Introduction\n\nGradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines. Gradio apps can be deployed on [Hugging Face Spaces](https://hf.space) for free.\n\nIn some cases though, you might want to deploy a Gradio app on your own web server. You might already be using [Nginx](https://www.nginx.com/), a highly performant web server, to serve your website (say `https://www.example.com`), and you want to attach Gradio to a specific subpath on your website (e.g. `https://www.example.com/gradio-demo`).", "### 4. Gradio is designed primarily for NLP models\n\n<Question\n\tchoices={[\n        {\n\t\t\ttext: \"True\",\n\t\t\texplain: \"Gradio works with pretty much any data type, not just NLP.\"\n        },\n        {\n\t\t\ttext: \"False\",\n\t\t\texplain: \"Gradio supplies developers with a library of pre-built components for pretty much all data types.\",\n            correct: true\n        }\n\t]}\n/>\n\n### 5. Which of the following features are supported by Gradio?", "## Advanced Usage\n\nFor more ways to use the Gradio Python Client, check out our dedicated Guide on the Python client, available here: https://www.gradio.app/guides/getting-started-with-the-python-client", "### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:", "#### The Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! Its an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately describes the use of Gradio for creating web apps for machine learning models, supporting various data types, and mentions the deployment on platforms like Hugging Face Spaces or using web servers. It also correctly states that Gradio can be used to query applications programmatically, which aligns well with the reference answer's points about creating demos, sharing models, and debugging. The response expands on the reference answer by providing additional context and examples of use cases, which demonstrates a comprehensive understanding of Gradio's capabilities."}, {"question": "What TensorFlow API function is used to load a saved tensor file?\n", "true_answer": "safetensors.tensorflow.load_file", "source_doc": "huggingface/safetensors/blob/main/docs/source/api/tensorflow.mdx", "generated_answer": "The TensorFlow API function used to load a saved tensor file is `safetensors.tensorflow.load_file`. [Document 0]", "retrieved_docs": ["Tensorflow API\n\n[[autodoc]] safetensors.tensorflow.load_file\n[[autodoc]] safetensors.tensorflow.load\n[[autodoc]] safetensors.tensorflow.save_file\n[[autodoc]] safetensors.tensorflow.save", "Multiple reasons for that:\n\n- *Not all frameworks support them* for instance `tensorflow` does not.\n  So if someone saves shared tensors in torch, there is no way to \n  load them in a similar fashion so we could not keep the same `Dict[str, Tensor]`\n  API.\n- *It makes lazy loading very quickly.*\n  Lazy loading is the ability to load only some tensors, or part of tensors for\n  a given file. This is trivial to do without sharing tensors but with tensor sharing\n\n  ```python\n  with safe_open(\"model.safetensors\", framework=\"pt\") as f:\n      a = f.get_tensor(\"a\")\n      b = f.get_tensor(\"b\")\n  ```", "Refer to the Performance [guide](performance) for more details about memory-saving techniques.\n\n</Tip>\n\n## Unable to load a saved TensorFlow model\n\nTensorFlow's [model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because \ud83e\udd17 Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:", "### What is TensorFlow Serving?\n\nTensorFlow Serving belongs to the set of tools provided by [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx/guide/serving) that makes the task of deploying a model to a server easier than ever. TensorFlow Serving provides two APIs, one that can be called upon using HTTP requests and another one using gRPC to run inference on the server.\n\n### What is a SavedModel?", "The serving method has to be overridden by the new `input_signature` argument of the `tf.function` decorator. See the [official documentation](https://www.tensorflow.org/api_docs/python/tf/function#args_1) to know more about the `input_signature` argument. The `serving` method is used to define how will behave a SavedModel when deployed with TensorFlow Serving. Now the SavedModel looks like as expected, see the new `inputs_embeds` input:", "```python\nimport torch_xla.distributed.parallel_loader as pl\n...\n  dataloader = pl.MpDeviceLoader(dataloader, device)\n```\n\n### Checkpoint Writing and Loading\n\nWhen a tensor is checkpointed from a XLA device and then loaded back from the checkpoint, it will be loaded back to the original device. Before checkpointing tensors in your model, you want to ensure that all of your tensors are on CPU devices instead of XLA devices. This way, when you load back the tensors, you\u2019ll load them through CPU devices and then have the opportunity to place them on whatever XLA devices you desire. We provide the `xm.save()` API for this, which already takes care of only writing to storage location from only one process on each host (or one globally if using a shared file system across hosts).", "### How to install TensorFlow Serving?\n\nThere are three ways to install and use TensorFlow Serving:\n- through a Docker container, \n- through an apt package,\n- or using [pip](https://pypi.org/project/pip/). \n\nTo make things easier and compliant with all the existing OS, we will use Docker in this tutorial.\n\n### How to create a SavedModel?\n\nSavedModel is the format expected by TensorFlow Serving. Since Transformers v4.2.0, creating a SavedModel has three additional features:\n\n1. The sequence length can be modified freely between runs.\n2. All model inputs are available for inference.\n3. `hidden states` or `attention` are now grouped into a single output when returning them with `output_hidden_states=True` or `output_attentions=True`."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the TensorFlow API function as `safetensors.tensorflow.load_file`, which matches the reference answer. The inclusion of `[Document 0]` does not detract from the accuracy of the response regarding the function name. Therefore, the response is completely correct, accurate, and factual."}, {"question": "Where can you access the logs of your Endpoints in Hugging Face Endpoints?\n", "true_answer": "In the \"Logs\" tab of your Endpoint through the UI.", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/logs.mdx", "generated_answer": "You can access the logs of your Endpoints in Hugging Face Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint. [Document 0]", "retrieved_docs": ["Access and read Logs\n\nHugging Face Endpoints provides access to the logs of your Endpoints through the UI in the \u201cLogs\u201d tab of your Endpoint. \n\nYou will have access to the build logs of your Image artifacts as well as access to the Container Logs during inference.\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/9_selection.png\" alt=\"select logs\" />\n\nThe Container Logs are only available when your Endpoint is in the \u201cRunning\u201d state. \n\n_Note: If your Endpoint creation is in the \u201cFailed\u201d state, you can check the Build Logs to see what the reason was, e.g. wrong version of a dependency, etc._\n\n**Build Logs:**", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:", "Access and view Metrics\n\nHugging Face Endpoints provides access to the metrics and analytics of your Endpoints through the UI on the detailed overview in the \u201cAnalytics\u201d tab of your Endpoints. \n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/10_metric.png\" alt=\"metric dashboard\" />\n\n## Access Metrics via API\n\nThe Hugging Face Inference Endpoints API exposes a [route to access the metrics](/docs/inference-endpoints/api_reference#getendpointmetrics) of your Endpoints. You can use this route to send customized Prometheus queries to your Endpoints.", "Create an Endpoint\n\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \n\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\n\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />", "Create custom Inference Handler\n\nHugging Face Endpoints supports all of the Transformers and Sentence-Transformers tasks and can support custom tasks, including custom pre- & post-processing. The customization can be done through a [handler.py](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/handler.py) file in your model repository on the Hugging Face Hub.", "## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:", "Access \ud83e\udd17 Inference Endpoints\n\nTo access the [Inference Endpoints web application](https://ui.endpoints.huggingface.co/), you or your organization need to [add a valid payment method](https://huggingface.co/settings/billing) to your Hugging Face account.\n\n<Tip>\n  \nYou can check your [billing](https://huggingface.co/settings/billing) if you're unsure whether you have an active payment method.\n  \n</Tip>\n\nThere are two pricing plans:"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by indicating that the logs can be accessed through the UI in the \"Logs\" tab of the Endpoint, which is the precise location for accessing logs in Hugging Face Endpoints."}, {"question": "What is the latest task added to Hugging Face AutoTrain for Computer Vision?\n", "true_answer": "Image Classification", "source_doc": "huggingface/blog/blob/main/autotrain-image-classification.md", "generated_answer": "The latest task added to Hugging Face AutoTrain for Computer Vision is Image Classification. [Document 0]", "retrieved_docs": ["[Hugging Face AutoTrain](https://huggingface.co/autotrain) lets you train models with **zero** configuration needed. Just choose your task (translation? how about question answering?), upload your data, and let Hugging Face do the rest of the work! By letting AutoTrain experiment with number of different models, there's even a good chance that you'll end up with a model that performs better than a model that's been hand-trained by an engineer \ud83e\udd2f We\u2019ve been expanding the number of tasks that we support, and we\u2019re proud to announce that **you can now use AutoTrain for Computer Vision**! Image Classification is the latest task we\u2019ve added, with more on the way. But what does this mean for you?", "## \ud83e\udd17 AutoTrain\n\n[AutoTrain](https://huggingface.co/autotrain) provides a \u201cno-code\u201d solution to train state-of-the-art Machine Learning models for tasks like text classification, text summarization, named entity recognition, and more. For Computer Vision, we currently support [image classification](https://huggingface.co/blog/autotrain-image-classification), but one can expect more task coverage.", "* Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models.\n</div>\n\n\n## Introduction \n\nTransfer learning has changed Machine Learning by reaching new levels of accuracy from Natural Language Processing (NLP) to Audio and Computer Vision tasks. At Hugging Face, we work hard to make these new complex models and large checkpoints as easily accessible and usable as possible. But while researchers and data scientists have converted to the new world of Transformers, few companies have been able to deploy these large, complex models in production at scale.\n\nThe main bottleneck is the latency of predictions which can make large deployments expensive to run and real-time use cases impractical. Solving this is a difficult engineering challenge for any Machine Learning Engineering team and requires the use of advanced techniques to optimize models all the way down to the hardware.", "## Enabling the community: One task at a time \ud83d\udc41\n\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification\n\nEach of these tasks comes with at least 10 model checkpoints on the Hub for you to explore. Furthermore, we support [tasks](https://huggingface.co/tasks) that lie at the intersection of vision and language such as:\n\n- Image-to-text (image captioning, OCR)\n- Text-to-image\n- Document question-answering\n- Visual question-answering", "## Introduction to AutoTrain\n\nAutoTrain is a no-code tool that lets non-ML Engineers, (or even non-developers \ud83d\ude2e) train state-of-the-art ML models without the need to code. It can be used for NLP, computer vision, speech, tabular data and even now for fine-tuning LLMs like we\u2019ll be doing today. \n\nYou can read more about AutoTrain [here](https://huggingface.co/docs/autotrain/index).\n\n## Introduction to ChatUI\n\nChatUI is exactly what it sounds like, it\u2019s the open-source UI built by Hugging Face that provides an interface to interact with open-source LLMs. Notably, it's the same UI behind HuggingChat, our 100% open-source alternative to ChatGPT.", "So you\u2019ve heard all about the cool things that are happening in the machine learning world, and you want to join in. There\u2019s just one problem \u2013 you don\u2019t know how to code! \ud83d\ude31 Or maybe you\u2019re a seasoned software engineer who wants to add some ML to your side-project, but you don\u2019t have the time to pick up a whole new tech stack! For many people, the technical barriers to picking up machine learning feel insurmountable. That\u2019s why Hugging Face created [AutoTrain](https://huggingface.co/autotrain), and with the latest feature we\u2019ve just added, we\u2019re making \u201cno-code\u201d machine learning better than ever. Best of all, you can create your first project for \u2728 free! \u2728", "## How can you train your own image classifier?\n\nIf you haven\u2019t [created a Hugging Face account](https://huggingface.co/join) yet, now\u2019s the time! Following that, make your way over to the [AutoTrain homepage](https://huggingface.co/autotrain) and click on \u201cCreate new project\u201d to get started. You\u2019ll be asked to fill in some basic info about your project. In the screenshot below you\u2019ll see that I created a project named `butterflies-classification`, and I chose the \u201cImage Classification\u201d task. I\u2019ve also chosen the \u201cAutomatic\u201d model option, since I want to let AutoTrain do the work of finding the best model architectures for my project."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided matches the reference answer exactly, indicating that it is completely correct, accurate, and factual."}, {"question": "What is the default repository type created by the `create_repo` function on Hugging Face Hub?\n", "true_answer": "model", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/repository.md", "generated_answer": "The default repository type created by the `create_repo` function on the Hugging Face Hub is a model repository. [Document 0]", "retrieved_docs": ["```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nBy default, [`create_repo`] creates a model repository. But you can use the `repo_type` parameter to specify another repository type. For example, if you want to create a dataset repository:\n\n```py\n>>> from huggingface_hub import create_repo\n>>> create_repo(\"lysandre/test-dataset\", repo_type=\"dataset\")\n'https://huggingface.co/datasets/lysandre/test-dataset'\n```\n\nWhen you create a repository, you can set your repository visibility with the `private` parameter.", "Let's fill the `push_to_hub` function:\n\n- `repo_id`: the name of the Hugging Face Hub Repository that will be created/updated `\n(repo_id = {username}/{repo_name})`\n\ud83d\udca1 A good `repo_id` is `{username}/q-{env_id}`\n- `model`: our model dictionary containing the hyperparameters and the Qtable.\n- `env`: the environment.\n- `commit_message`: message of the commit\n\n```python\nmodel\n```", "Finally, we need to create a model repository on the Hugging Face Hub. For this, we can use the appropriately titled \ud83e\udd17 Hub library. We just need to define a name for our repository, and the library has a utility function to combine the repository ID with the user profile:\n\n```python\nfrom huggingface_hub import get_full_repo_name\n\nmodel_name = \"test-bert-finetuned-squad-accelerate\"\nrepo_name = get_full_repo_name(model_name)\nrepo_name\n```\n\n```python out\n'lewtun/mt5-finetuned-amazon-en-es-accelerate'\n```\n\nNow we can use this repository name to clone a local version to our results directory that will store the training artifacts:\n\n```python\nfrom huggingface_hub import Repository", "Create a repository. It's a model repo by default.\n\nParameters:\n- `type`: Type of repo (dataset or space; model by default).\n- `name`: Name of repo.\n- `organization`: Name of organization (optional).\n- `private`: Whether the repo is private.\n- `sdk`: When the type is `space` (streamlit, gradio, docker or static)\n\nPayload:\n\n```js\npayload = {\n    \"type\":\"model\",\n    \"name\":\"name\",\n    \"organization\": \"organization\",\n    \"private\":\"private\",\n    \"sdk\": \"sdk\"\n}\n```\n\nThis is equivalent to `huggingface_hub.create_repo()`.\n\n### DELETE /api/repos/delete\n\nDelete a repository. It's a model repo by default.", "- `create_repo` creates a repository on the Hub.\n- `upload_file` directly uploads files to a repository on the Hub.\n\n### `create_repo`\n\nThe `create_repo` method creates a repository on the Hub. Use the `name` parameter to provide a name for your repository:\n\n```python\n>>> from huggingface_hub import create_repo\n>>> create_repo(repo_id=\"test-model\")\n'https://huggingface.co/lysandre/test-model'\n```\n\nWhen you check your Hugging Face account, you should now see a `test-model` repository under your namespace.\n\n### `upload_file`\n\nThe `upload_file` method uploads files to the Hub. This method requires the following:", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Create and manage a repository\n\nThe Hugging Face Hub is a collection of git repositories. [Git](https://git-scm.com/) is a widely used tool in software\ndevelopment to easily version projects when working collaboratively. This guide will show you how to interact with the\nrepositories on the Hub, especially:\n\n- Create and delete a repository.\n- Manage branches and tags. \n- Rename your repository.\n- Update your repository visibility.\n- Manage a local copy of your repository.\n\n<Tip warning={true}>", "# Adding HuggingFace argument\n    parser.add_argument(\"--repo-id\", type=str, default=\"ThomasSimonini/ppo-CartPole-v1\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n\n    args = parser.parse_args()\n    args.batch_size = int(args.num_envs * args.num_steps)\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n    # fmt: on\n    return args"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns perfectly with the reference answer, stating that the default repository type is a model repository."}, {"question": "How many splits does the \"duorc\" dataset have?\n", "true_answer": "Six", "source_doc": "huggingface/datasets-server/blob/main/docs/source/splits.mdx", "generated_answer": "The \"duorc\" dataset has six splits. (Document 0)", "retrieved_docs": ["The endpoint response is a JSON containing a list of the dataset's splits and configurations. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has six splits and two configurations:", "The endpoint response is a JSON containing the size of the dataset, as well as each of its configurations and splits. It provides the number of rows, the number of colums (where applicable) and the size in bytes for the different forms of the data: original files, size in memory (RAM) and auto-converted parquet files. For example, the [duorc](https://huggingface.co/datasets/duorc) dataset has 187.213 rows along all its configurations and splits, for a total of 97MB.", "The endpoint response is a JSON containing a list of the dataset's files in the Parquet format. For example, the [`duorc`](https://huggingface.co/datasets/duorc) dataset has six Parquet files, which corresponds to the `test`, `train` and `validation` splits of its two configurations, `ParaphraseRC` and `SelfRC` (see the [List splits and configurations](./splits) guide for more details about splits and configurations).\n\nThe endpoint also gives the filename and size of each file:", "The endpoint response is a JSON containing a list of the dataset's files URLs in the Parquet format for each split and configuration. For example, the [`duorc`](https://huggingface.co/datasets/duorc) dataset has one Parquet file for the train split of the \"ParaphraseRC\" configuration (see the [List splits and configurations](./splits) guide for more details about splits and configurations).", "\"split\": \"train\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/train/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 26005667\n    },\n    {\n      \"dataset\": \"duorc\",\n      \"config\": \"ParaphraseRC\",\n      \"split\": \"validation\",\n      \"url\": \"https://huggingface.co/datasets/duorc/resolve/refs%2Fconvert%2Fparquet/ParaphraseRC/validation/0000.parquet\",\n      \"filename\": \"0000.parquet\",\n      \"size\": 5566867\n    },\n    {\n      \"dataset\": \"duorc\",", "```json\n{\n    \"dataset_info\": {\n        \"description\": \"DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie.\\n\",\n        \"citation\": \"@inproceedings{DuoRC,\\nauthor = { Amrita Saha and Rahul Aralikatte and Mitesh M. Khapra and Karthik Sankaranarayanan},title = {{DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension}},\\nbooktitle = {Meeting of the Association for Computational Linguistics (ACL)},\\nyear = {2018}\\n}\\n\",\n        \"homepage\": \"https://duorc.github.io/\",", "\"num_columns\": 7\n      },\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"SelfRC\",\n        \"num_bytes_original_files\": 34462660,\n        \"num_bytes_parquet_files\": 21001846,\n        \"num_bytes_memory\": 340657958,\n        \"num_rows\": 86241,\n        \"num_columns\": 7\n      }\n    ],\n    \"splits\": [\n      {\n        \"dataset\": \"duorc\",\n        \"config\": \"ParaphraseRC\",\n        \"split\": \"train\",\n        \"num_bytes_parquet_files\": 26005668,\n        \"num_bytes_memory\": 496682909,\n        \"num_rows\": 69524,\n        \"num_columns\": 7\n      },\n      {"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual as it aligns perfectly with the reference answer which states that the \"duorc\" dataset has six splits."}, {"question": "What is the purpose of Fully Sharded Data Parallel (FSDP) in distributed training?\n", "true_answer": "FSDP is developed for distributed training of large pretrained models up to 1T parameters by sharding the model parameters, gradients, and optimizer states across data parallel processes.", "source_doc": "huggingface/peft/blob/main/docs/source/accelerate/fsdp.md", "generated_answer": "The purpose of Fully Sharded Data Parallel (FSDP) in distributed training is to enable the efficient training of large models, potentially up to 1 trillion parameters, by sharding the model's parameters, gradients, and optimizer states across multiple GPUs or devices. This sharding reduces memory usage, allowing for larger batch sizes or model sizes to be trained on fewer GPUs. FSDP also supports offloading sharded model parameters to a CPU, further enhancing memory efficiency. During training, FSDP performs operations like all-gather and reduce-scatter to manage the distribution and updating of model parameters across devices. [Documents 0, 1, 2]", "retrieved_docs": ["[Fully Sharded Data Parallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data parallel method that shards a model's parameters, gradients and optimizer states across the number of available GPUs (also called workers or *rank*). Unlike [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Fully Sharded Data Parallel\n\n[Fully sharded data parallel](https://pytorch.org/docs/stable/fsdp.html) (FSDP) is developed for distributed training of large pretrained models up to 1T parameters. FSDP achieves this by sharding the model parameters, gradients, and optimizer states across data parallel processes and it can also offload sharded model parameters to a CPU. The memory efficiency afforded by FSDP allows you to scale training to larger batch or model sizes.\n\n<Tip warning={true}>", "Fully Sharded Data Parallelism (FSDP) is a paradigm in which the optimizer states, gradients and parameters are sharded across devices. During the forward pass, each FSDP unit performs an _all-gather operation_ to get the complete weights, computation is performed followed by discarding the shards from other devices. After the forward pass, the loss is computed followed by the backward pass. In the backward pass, each FSDP unit performs an all-gather operation to get the complete weights, with computation performed to get the local gradients. These local gradients are averaged and sharded across the devices via a _reduce-scatter operation_ so that each device can update the parameters of its shard. For more information on what PyTorch FSDP is, please refer to this blog post: [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface", "## Accelerate and Trainer\n\nThe [`Trainer`] class is powered by [Accelerate](https://hf.co/docs/accelerate), a library for easily training PyTorch models in distributed environments with support for integrations such as [FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) and [DeepSpeed](https://www.deepspeed.ai/).\n\n<Tip>\n\nLearn more about FSDP sharding strategies, CPU offloading, and more with the [`Trainer`] in the [Fully Sharded Data Parallel](fsdp) guide.\n\n</Tip>", "--\ntitle: \"Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\"\nthumbnail: /blog/assets/62_pytorch_fsdp/fsdp-thumbnail.png\nauthors:\n- user: smangrul\n- user: sgugger\n---\n\n# Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel\n\n\nIn this post we will look at how we can leverage **[Accelerate](https://github.com/huggingface/accelerate)** Library for training large models which enables users to leverage the latest features of **[PyTorch FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**.\n\n# Motivation \ud83e\udd17", "[7] [Introducing PyTorch Fully Sharded Data Parallel (FSDP) API | PyTorch](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)\n\n[8] [Getting Started with Fully Sharded Data Parallel(FSDP) \u2014 PyTorch Tutorials 1.11.0+cu102 documentation](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)", ".co/blog/zero-deepspeed-fairscale) [4] and [Accelerate Large Model Training using PyTorch Fully Sharded Data Parallel](https://huggingface.co/blog/pytorch-fsdp) [5]. We defer the explanation of what goes behind the scenes to those blogs and mainly focus on leveraging DeepSpeed ZeRO using Accelerate."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by explaining the purpose of FSDP in distributed training, which is to enable the efficient training of large models by sharding the model's parameters, gradients, and optimizer states across multiple GPUs or devices. The response also correctly mentions the capability of FSDP to handle models with up to 1 trillion parameters and includes additional details about memory efficiency and specific operations like all-gather and reduce-scatter, which are part of FSDP's functionality. These details are consistent with the purpose of FSDP as described in the reference answer."}, {"question": "What file format is used to save and store PyTorch model weights more securely than `.bin` files?\n", "true_answer": "`.safetensors`", "source_doc": "huggingface/safetensors/blob/main/docs/source/convert-weights.md", "generated_answer": "The file format used to save and store PyTorch model weights more securely than `.bin` files is `safetensors`. [Document 1]", "retrieved_docs": ["Convert weights to safetensors\n\nPyTorch model weights are commonly saved and stored as `.bin` files with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. To save and store your model weights in the more secure `safetensor` format, we recommend converting your weights to `.safetensors`.\n\nThe easiest way to convert your model weights is to use the [Convert Space](https://huggingface.co/spaces/diffusers/convert), given your model weights are already stored on the Hub. The Convert Space downloads the pickled weights, converts them, and opens a Pull Request to upload the newly converted `.safetensors` file to your repository.\n\n<Tip warning={true}>", "# Load safetensors\n\n[[open-in-colab]]\n\n[safetensors](https://github.com/huggingface/safetensors) is a safe and fast file format for storing and loading tensors. Typically, PyTorch model weights are saved or *pickled* into a `.bin` file with Python's [`pickle`](https://docs.python.org/3/library/pickle.html) utility. However, `pickle` is not secure and pickled files may contain malicious code that can be executed. safetensors is a secure alternative to `pickle`, making it ideal for sharing model weights.\n\nThis guide will show you how you load `.safetensor` files, and how to convert Stable Diffusion model weights stored in other formats to `.safetensor`. Before you start, make sure you have safetensors installed:", "We call the checkpoints saved in several files like BLOOM *sharded checkpoints*, and we have standardized their format as such: \n- One file (called `pytorch_model.bin.index.json`) contains some metadata and a map parameter name to file name, indicating where to find each weight\n- All the other files are standard PyTorch state dicts, they just contain a part of the model instead of the whole one. You can have a look at the content of the index file [here](https://huggingface.co/bigscience/bloom/blob/main/pytorch_model.bin.index.json).", "For a more concrete explanation, we'll use PyTorch.\n```python\nimport torch\nfrom safetensors.torch import load_file, save_file\n\nweights = {\"embeddings\": torch.zeros((10, 100))}\nsave_file(weights, \"model.safetensors\")\nweights2 = load_file(\"model.safetensors\")\n```\n\nIt also has a number of [cool features](https://github.com/huggingface/safetensors#yet-another-format-) compared to other formats, most notably that loading files is _safe_, as we'll see later. \n\nWhen you're using `transformers`, if `safetensors` is installed, then those files will already\nbe used preferentially in order to prevent issues, which means that\n\n```\npip install safetensors\n```", "## Sharding state dicts\n\nTraditionally, PyTorch models are saved in a whole file containing a map from parameter name to weight. This map is often called a `state_dict`. Here is an excerpt from the [PyTorch documentation](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html) on saving on loading:\n\n```python\n# Save the model weights\ntorch.save(my_model.state_dict(), 'model_weights.pth')\n\n# Reload them\nnew_model = ModelClass()\nnew_model.load_state_dict(torch.load('model_weights.pth'))\n```", "This step should be iterated with the previous step until all weights of the checkpoint are correctly loaded in the\nTransformers model. Having correctly loaded the checkpoint into the \ud83e\udd17 Transformers implementation, you can then save\nthe model under a folder of your choice `/path/to/converted/checkpoint/folder` that should then contain both a\n`pytorch_model.bin` file and a `config.json` file:\n\n```python\nmodel.save_pretrained(\"/path/to/converted/checkpoint/folder\")\n```\n\n**7. Implement the forward pass**", "This step should be iterated with the previous step until all weights of\nthe checkpoint are correctly loaded in the Transformers model. Having\ncorrectly loaded the checkpoint into the \ud83e\udd17 Transformers implementation,\nyou can then save the model under a folder of your choice\n`/path/to/converted/checkpoint/folder` that should then contain both a\n`pytorch_model.bin` file and a `config.json` file:\n\n```python\nmodel.save_pretrained(\"/path/to/converted/checkpoint/folder\")\n```\n\n**7. Implement the forward pass**"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that the file format used for more secure storage of PyTorch model weights is `safetensors`. The inclusion of a document reference suggests that the information is backed by a source, although the validity of the source cannot be verified within this context. However, the response directly answers the question and aligns with the reference answer."}, {"question": "What type of security certification does Hugging Face have?\n", "true_answer": "SOC2 Type 2 certified", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/security.md", "generated_answer": "Hugging Face is SOC2 Type 2 certified. [Document 0]", "retrieved_docs": ["Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\nFor any other security questions, please feel free to send us an email at security@huggingface.co.\n\n## Contents", "## Model Security/Privacy:\n\nYou can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.\n\n## Inference Endpoints and Hub Security\n\nThe Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">\n\n## Inference Endpoint Security level\n\nWe currently offer three types of endpoints, in order or increasing security level:", "Security\n\nThe Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.\n\nHugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing).", "The Hugging Face Course", "We are happy to announce that Hugging Face has been selected by the [CNIL](https://www.cnil.fr/en/home) (French Data Protection Authority) to benefit from its [Enhanced Support program](https://www.cnil.fr/en/enhanced-support-cnil-selects-3-digital-companies-strong-potential)!\nThis new program picked three companies with \u201cstrong potential for economic development\u201d out of over 40 candidates, who will receive support in understanding and implementing their duties with respect to data protection - \na daunting and necessary endeavor in the context of the rapidly evolving field of Artificial Intelligence.", "[Explore existing organizations](https://huggingface.co/organizations), create a new organization [here](https://huggingface.co/organizations/new), and then visit the [**Organizations documentation**](./organizations) to learn more.\n\n## Security\n\nThe Hugging Face Hub supports security and access control features to give you the peace of mind that your code, models, and data are safe. Visit the [**Security**](./security) section in these docs to learn about:\n\n- User Access Tokens\n- Access Control for Organizations\n- Signing commits with GPG\n- Malware scanning\n\n<img width=\"150\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg\">", "Security & Compliance\n\n\ud83e\udd17 Inference Endpoints is built with security and secure inference at its core. Below you can find an overview of the security measures we have in place.\n\n## Data Security/Privacy\n\nHugging Face does not store any customer data in terms of payloads or tokens that are passed to the Inference Endpoint. We are storing logs for 30 days. Every Inference Endpoints uses TLS/SSL to encrypt the data in transit.\n\nWe also recommend using AWS or Azure Private Link for organizations. This allows you to access your Inference Endpoint through a private connection, without exposing it to the internet.\n\nHugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.\n\n## Model Security/Privacy:"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly by stating that Hugging Face is SOC2 Type 2 certified. The addition of \"[Document 0]\" does not detract from the accuracy of the response, although it is not necessary for the completeness of the answer."}, {"question": "What do RAG models combine to generate outputs?\n", "true_answer": "Pretrained dense retrieval (DPR) and sequence-to-sequence models.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/rag.md", "generated_answer": "RAG models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models to generate outputs. They retrieve documents, pass them to a seq2seq model, and then marginalize to generate outputs. The retriever and seq2seq modules are initialized from pretrained models and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks. [Document 0]", "retrieved_docs": ["This model was contributed by [ola13](https://huggingface.co/ola13).\n\n## Usage tips\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. \nRAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq \nmodules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt \nto downstream tasks.\n\n## RagConfig\n\n[[autodoc]] RagConfig\n\n## RagTokenizer\n\n[[autodoc]] RagTokenizer\n\n## Rag specific outputs\n\n[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput", "-->\n\n# RAG\n\n<div class=\"flex flex-wrap space-x-1\">\n<a href=\"https://huggingface.co/models?filter=rag\">\n<img alt=\"Models\" src=\"https://img.shields.io/badge/All_model_pages-rag-blueviolet\">\n</a>\n</div>\n\n## Overview\n\nRetrieval-augmented generation (\"RAG\") models combine the powers of pretrained dense retrieval (DPR) and\nsequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate\noutputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing\nboth retrieval and generation to adapt to downstream tasks.", "Intro\n\nAuthors: @patrickvonplaten and @lhoestq\n\nAimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.\n\nA RAG model encapsulates two core components: a question encoder and a generator.\nDuring a forward pass, we encode the input with the question encoder and pass it\nto the retriever to extract relevant context documents. The documents are then prepended to the input.\nSuch contextualized inputs are passed to the generator.\n\nRead more about RAG  at https://arxiv.org/abs/2005.11401.\n\n# Note\n\n\u26a0\ufe0f This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability\n\n# Finetuning", "### What is Retrieval Augmented Generation (RAG)?\n![alt_text](assets/12_ray_rag/rag_gif.gif \"image_tooltip\")\n\n_An overview of RAG. The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The  GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models)._", "The created outputs in `path/to/my_knowledge_dataset` can then be used to finetune RAG as follows:\n```bash\npython examples/research_projects/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8\n    --index_name custom\n    --passages_path path/to/data/my_knowledge_dataset\n    --index_path path/to/my_knowledge_dataset_hnsw_index.faiss\n```", "```bash\npython examples/research_projects/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8\n```\nWe publish two `base` models which can serve as a starting point for finetuning on downstream tasks (use them as `model_name_or_path`):\n- [`facebook/rag-sequence-base`](https://huggingface.co/facebook/rag-sequence-base) - a base for finetuning `RagSequenceForGeneration` models,", "python examples/rag/finetune_rag.py \\\n    --data_dir $DATA_DIR \\\n    --output_dir $OUTPUT_DIR \\\n    --model_name_or_path $MODEL_NAME_OR_PATH \\\n    --model_type rag_sequence \\\n    --fp16 \\\n    --gpus 8 \\\n    --profile \\\n    --do_train \\\n    --do_predict \\\n    --n_val -1 \\\n    --train_batch_size 8 \\\n    --eval_batch_size 1 \\\n    --max_source_length 128 \\\n    --max_target_length 25 \\\n    --val_max_target_length 25 \\\n    --test_max_target_length 25 \\\n    --label_smoothing 0.1 \\\n    --dropout 0.1 \\\n    --attention_dropout 0.1 \\\n    --weight_decay 0.001 \\"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately identifies that RAG models combine pretrained dense retrieval (DPR) and sequence-to-sequence (seq2seq) models to generate outputs, which is in line with the reference answer. It also provides additional details on how the models retrieve documents and are fine-tuned, which supports the correctness and factual nature of the response without deviating from the core answer."}, {"question": "What library does MarkupLMFeatureExtractor use to extract data from HTML and XML files?\n", "true_answer": "Beautiful Soup", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/markuplm.md", "generated_answer": "MarkupLMFeatureExtractor uses the Beautiful Soup library to extract data from HTML and XML files. [Document 0]", "retrieved_docs": ["[`MarkupLMFeatureExtractor`] uses [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/), a Python library for\npulling data out of HTML and XML files, under the hood. Note that you can still use your own parsing solution of\nchoice, and provide the nodes and xpaths yourself to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`].\n\nIn total, there are 5 use cases that are supported by the processor. Below, we list them all. Note that each of these\nuse cases work for both batched and non-batched inputs (we illustrate them for non-batched inputs).\n\n**Use case 1: web page classification (training, inference) + token classification (inference), parse_html = True**", "## Usage: MarkupLMProcessor\n\nThe easiest way to prepare data for the model is to use [`MarkupLMProcessor`], which internally combines a feature extractor\n([`MarkupLMFeatureExtractor`]) and a tokenizer ([`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]). The feature extractor is\nused to extract all nodes and xpaths from the HTML strings, which are then provided to the tokenizer, which turns them into the\ntoken-level inputs of the model (`input_ids` etc.). Note that you can still use the feature extractor and tokenizer separately,\nif you only want to handle one of the two tasks.\n\n```python\nfrom transformers import MarkupLMFeatureExtractor, MarkupLMTokenizerFast, MarkupLMProcessor", "feature_extractor = MarkupLMFeatureExtractor()\ntokenizer = MarkupLMTokenizerFast.from_pretrained(\"microsoft/markuplm-base\")\nprocessor = MarkupLMProcessor(feature_extractor, tokenizer)\n```", "In short, one can provide HTML strings (and possibly additional data) to [`MarkupLMProcessor`],\nand it will create the inputs expected by the model. Internally, the processor first uses\n[`MarkupLMFeatureExtractor`] to get a list of nodes and corresponding xpaths. The nodes and\nxpaths are then provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which converts them\nto token-level `input_ids`, `attention_mask`, `token_type_ids`, `xpath_subs_seq`, `xpath_tags_seq`.\nOptionally, one can provide node labels to the processor, which are turned into token-level `labels`.", "This is the simplest case, in which the processor will use the feature extractor to get all nodes and xpaths from the HTML.\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")\n\n>>> html_string = \"\"\"\n...  <!DOCTYPE html>\n...  <html>\n...  <head>\n...  <title>Hello world</title>\n...  </head>\n...  <body>\n...  <h1>Welcome</h1>\n...  <p>Here is my website.</p>\n...  </body>\n...  </html>\"\"\"", "**Use case 4: web page question answering (inference), parse_html=True**\n\nFor question answering tasks on web pages, you can provide a question to the processor. By default, the\nprocessor will use the feature extractor to get all nodes and xpaths, and create [CLS] question tokens [SEP] word tokens [SEP].\n\n```python\n>>> from transformers import MarkupLMProcessor\n\n>>> processor = MarkupLMProcessor.from_pretrained(\"microsoft/markuplm-base\")", "## Resources\n\n- [Demo notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MarkupLM)\n- [Text classification task guide](../tasks/sequence_classification)\n- [Token classification task guide](../tasks/token_classification)\n- [Question answering task guide](../tasks/question_answering)\n\n## MarkupLMConfig\n\n[[autodoc]] MarkupLMConfig\n    - all\n\n## MarkupLMFeatureExtractor\n\n[[autodoc]] MarkupLMFeatureExtractor\n    - __call__\n\n## MarkupLMTokenizer"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that MarkupLMFeatureExtractor uses the Beautiful Soup library to extract data from HTML and XML files."}, {"question": "What is the file size limit for syncing to HF Spaces without using Git-LFS?\n", "true_answer": "10MB", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/spaces-github-actions.md", "generated_answer": "The file size limit for syncing to HF Spaces without using Git-LFS is 10MB. [Document 0]", "retrieved_docs": ["Managing Spaces with Github Actions\n\nYou can keep your app in sync with your GitHub repository with **Github Actions**. Remember that for files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.\n\nFirst, you should set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```", "*Note: For files larger than 10MB, Spaces requires Git-LFS. If you don't want to use Git-LFS, you may need to review your files and check your history. Use a tool like [BFG Repo-Cleaner](https://rtyley.github.io/bfg-repo-cleaner/) to remove any large files from your history. BFG Repo-Cleaner will keep a local copy of your repository as a backup.*\n\nFirst, set up your GitHub repository and Spaces app together. Add your Spaces app as an additional remote to your existing Git repository.\n\n```bash\ngit remote add space https://huggingface.co/spaces/HF_USERNAME/SPACE_NAME\n```\n\nThen force push to sync everything for the first time:\n\n```bash\ngit push --force space main\n```", "leading to a slower download speed.\nIn all cases no single LFS file will be able to be >50GB. I.e. 50GB is the hard limit for single file size.\n- **Number of commits**: There is no hard limit for the total number of commits on your repo history. However, from\nour experience, the user experience on the Hub starts to degrade after a few thousand commits. We are constantly working to\nimprove the service, but one must always remember that a git repository is not meant to work as a database with a lot of\nwrites. If your repo's history gets very large, it is always possible to squash all the commits to get a", "# to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check large files\n        uses: ActionsDesk/lfs-warning@v2.0\n        with:\n          filesizelimit: 10485760 # this is 10MB so we can sync to HF Spaces\n```", "# to run this workflow manually from the Actions tab\n  workflow_dispatch:\n\njobs:\n  sync-to-hub:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n          lfs: true\n      - name: Push to hub\n        env:\n          HF_TOKEN: ${{ secrets.HF_TOKEN }}\n        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main\n```\n\nFinally, create an Action that automatically checks the file size of any new pull request:\n\n\n```yaml\nname: Check file size\non:               # or directly `on: [push]` to run the action on every push on any branch\n  pull_request:\n    branches: [main]\n\n  # to run this workflow manually from the Actions tab\n  workflow_dispatch:", "If you look at the file sizes (for example, with `ls -lh`), you should see that the model state dict file (*t5_model.h5*) is the only outlier, at more than 400 MB.\n\n{/if}\n\n<Tip>\n\u270f\ufe0f When creating the repository from the web interface, the *.gitattributes* file is automatically set up to consider files with certain extensions, such as *.bin* and *.h5*, as large files, and git-lfs will track them with no necessary setup on your side.\n</Tip> \n\nWe can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git's staging environment using the `git add` command:\n\n```bash\ngit add .\n```\n\nWe can then have a look at the files that are currently staged:", "</Tip>\n\nGit LFS automatically handles files larger than 10MB. But for very large files (>5GB), you need to install a custom transfer agent for Git LFS:\n\n```bash\nhuggingface-cli lfs-enable-largefiles\n```\n\nYou should install this for each repository that has a very large file. Once installed, you'll be able to push files larger than 5GB.\n\n### commit context manager\n\nThe `commit` context manager handles four of the most common Git commands: pull, add, commit, and push. `git-lfs` automatically tracks any file larger than 10MB. In the following example, the `commit` context manager:\n\n1. Pulls from the `text-files` repository.\n2. Adds a change made to `file.txt`.\n3. Commits the change.\n4. Pushes the change to the `text-files` repository."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, stating the file size limit as 10MB, which is the correct information for syncing to HF Spaces without using Git-LFS."}, {"question": "What is the title of the paper introducing the ByT5 model?\n", "true_answer": "ByT5: Towards a token-free future with pre-trained byte-to-byte models", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/byt5.md", "generated_answer": "The title of the paper introducing the ByT5 model is \"ByT5: Towards a token-free future with pre-trained byte-to-byte models.\" [Document 0]", "retrieved_docs": ["-->\n\n# ByT5\n\n## Overview\n\nThe ByT5 model was presented in [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir\nKale, Adam Roberts, Colin Raffel.\n\nThe abstract from the paper is the following:", "1. **[ByT5](https://huggingface.co/docs/transformers/model_doc/byt5)** (from Google Research) released with the paper [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) by Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, Colin Raffel.", "Based on the original T5 model, Google has released some follow-up works:\n\n- **T5v1.1**: T5v1.1 is an improved version of T5 with some architectural tweaks, and is pre-trained on C4 only without\n  mixing in the supervised tasks. Refer to the documentation of T5v1.1 which can be found [here](t5v1.1).\n\n- **mT5**: mT5 is a multilingual T5 model. It is pre-trained on the mC4 corpus, which includes 101 languages. Refer to\n  the documentation of mT5 which can be found [here](mt5).\n\n- **byT5**: byT5 is a T5 model pre-trained on byte sequences rather than SentencePiece subword token sequences. Refer\n  to the documentation of byT5 which can be found [here](byt5).", "This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\nfound [here](https://github.com/google-research/byt5).\n\n<Tip>\n\nByT5's architecture is based on the T5v1.1 model, refer to [T5v1.1's documentation page](t5v1.1) for the API reference. They\nonly differ in how inputs should be prepared for the model, see the code examples below.\n\n</Tip>\n\nSince ByT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task\nfine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.\n\n\n## Usage example", "Similar to [T5](t5), ByT5 was trained on the span-mask denoising task. However, \nsince the model works directly on characters, the pretraining task is a bit \ndifferent. Let's corrupt some characters of the \ninput sentence `\"The dog chases a ball in the park.\"` and ask ByT5 to predict them \nfor us.\n\n```python\n>>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n>>> import torch\n\n>>> tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-base\")\n>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/byt5-base\")", "--\ntitle: \"Speech Synthesis, Recognition, and More With SpeechT5\"\nthumbnail: /blog/assets/speecht5/thumbnail.png\nauthors:\n- user: Matthijs\n---\n\n# Speech Synthesis, Recognition, and More With SpeechT5\n\n\nWe\u2019re happy to announce that SpeechT5 is now available in \ud83e\udd17 Transformers, an open-source library that offers easy-to-use implementations of state-of-the-art machine learning models.\n\nSpeechT5 was originally described in the paper [SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing](https://arxiv.org/abs/2110.07205) by Microsoft Research Asia. The [official checkpoints](https://github.com/microsoft/SpeechT5) published by the paper\u2019s authors are available on the Hugging Face Hub.", "## Overview\n\nThe T5 model was presented in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) by [Colin Raffel](https://huggingface.co/craffel), Noam Shazeer, [Adam Roberts](https://huggingface.co/adarob), Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, [Peter J. Liu](https://huggingface.co/peterjliu).\n\nThe abstract from the paper is the following:"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided matches the reference answer exactly, indicating that it is completely correct, accurate, and factual. There are no discrepancies between the response and the reference answer."}, {"question": "What is the dimension of the feature vector for the base BERT model?\n", "true_answer": "768", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter1/05_encoders.md", "generated_answer": "The dimension of the feature vector for the base BERT model is 768. [Document 0]", "retrieved_docs": ["Let's dive in this representation. It contains one vector per word that was passed through the encoder. Each of these vector is a numerical representation of the word in question. The dimension of that vector is defined by the architecture of the model, for the base BERT model, it is 768. These representations contain the value of a word; but contextualized. For example, the vector attributed to the word \"to\", isn't the representation of only the \"to\" word. It also takes into account the words around it, which we call the \u201ccontext\u201d.As in, it looks to the left context, the word on the left of the one we're studying (here the word \"Welcome\") and the context on the right (here the word \"NYC\") and outputs a value for the word, within its context. It is therefore a contextualized value", "- `distilbert-base-uncased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-uncased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 66M parameters.\n- `distilbert-base-uncased-distilled-squad`: A finetuned version of `distilbert-base-uncased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 86.9 on the dev set (for comparison, Bert `bert-base-uncased` version reaches a 88.5 F1 score).", "- `distilbert-base-cased`: DistilBERT English language model pretrained on the same data used to pretrain Bert (concatenation of the Toronto Book Corpus and full English Wikipedia) using distillation with the supervision of the `bert-base-cased` version of Bert. The model has 6 layers, 768 dimension and 12 heads, totalizing 65M parameters.\n- `distilbert-base-cased-distilled-squad`: A finetuned version of `distilbert-base-cased` finetuned using (a second step of) knowledge distillation on SQuAD 1.0. This model reaches a F1 score of 87.1 on the dev set (for comparison, Bert `bert-base-cased` version reaches a 88.7 F1 score).", "We ran 144 experiments on 6 different model architectures:\n* Accelerators: Inf1, Inf2, NVIDIA A10G\n* Models: [BERT-base](https://huggingface.co/bert-base-uncased), [BERT-Large](https://huggingface.co/bert-large-uncased), [RoBERTa-base](https://huggingface.co/roberta-base), [DistilBERT](https://huggingface.co/distilbert-base-uncased), [ALBERT-base](https://huggingface.co/albert-base-v2), [ViT-base](https://huggingface.co/google/vit-base-patch16-224)\n* Sequence length: 8, 16, 32, 64, 128, 256, 512\n* Batch size: 1", "The result is a matrix of shape (number of examples, hidden size). The hidden size is the number of dimensions in the hidden representation. For BERT, the hidden size is 768. The hidden representation is a vector of numbers that represents the text that can be used for many different tasks. In this case, we will use it for classification with [XGBoost](https://github.com/dmlc/xgboost) afterwards.", "bert-base-uncased          8              512            0.105\n--------------------------------------------------------------------------------", "bert-base-uncased          8              128            1330\nbert-base-uncased          8              512            1770\n--------------------------------------------------------------------------------"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, stating that the dimension of the feature vector for the base BERT model is 768."}, {"question": "What special identifier does the WordPiece Model use for continuing subwords?\n", "true_answer": "##", "source_doc": "huggingface/tokenizers/blob/main/docs/source-doc-builder/components.mdx", "generated_answer": "The WordPiece Model uses the special identifier `##` for continuing subwords. [Document 0]", "retrieved_docs": ["| Name | Description |\n| :--- | :--- |\n| ByteLevel | Reverts the ByteLevel PreTokenizer. This PreTokenizer encodes at the byte-level, using a set of visible Unicode characters to represent each byte, so we need a Decoder to revert this process and get something readable again. |\n| Metaspace | Reverts the Metaspace PreTokenizer. This PreTokenizer uses a special identifer `\u2581` to identify whitespaces, and so this Decoder helps with decoding these. |\n| WordPiece | Reverts the WordPiece Model. This model uses a special identifier `##` for continuing subwords, and so this Decoder helps with decoding these. |", "<a id='wordpiece'></a>\n\n### WordPiece\n\nWordPiece is the subword tokenization algorithm used for [BERT](model_doc/bert), [DistilBERT](model_doc/distilbert), and [Electra](model_doc/electra). The algorithm was outlined in [Japanese and Korean\nVoice Search (Schuster et al., 2012)](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and is very similar to\nBPE. WordPiece first initializes the vocabulary to include every character present in the training data and\nprogressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent\nsymbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.", "</Tip>\n\nLike BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like `##` for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, `\"word\"` gets split like this:\n\n```\nw ##o ##r ##d\n```\n\nThus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.\n\nThen, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:", ". The ## comes from the BERT tokenizer, based on the WordPiece algorithm. Other tokenizers use other prefixes, which can be placed to indicate part of words like seen here, or start of words instead! There are a lot of different algorithms that can be used for subword tokenization, and most models obtaining state-of-the-art results in English today use some kind of subword-tokenization algorithm. These approaches help in reducing the vocabulary sizes by sharing information across different words, having the ability to have prefixes and suffixes understood as such. They keep meaning across very similar words, by recognizing similar tokens making them up.", "explain: \"That's correct!\",\n\t\t\tcorrect: true\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model.\",\n\t\t\texplain: \"No, that's how another tokenization algorithm works.\"\n\t\t},\n\t\t{\n\t\t\ttext: \"WordPiece tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text.\",\n\t\t\texplain: \"Yes, this is how WordPiece proceeds for the encoding.\",\n\t\t\tcorrect: true\n\t\t},\n\t]}\n/>", "</Tip>\n\n## Tokenization algorithm[[tokenization-algorithm]]\n\nTokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word `\"hugs\"` the longest subword starting from the beginning that is inside the vocabulary is `\"hug\"`, so we split there and get `[\"hug\", \"##s\"]`. We then continue with `\"##s\"`, which is in the vocabulary, so the tokenization of `\"hugs\"` is `[\"hug\", \"##s\"]`.", "| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don\u2019t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |\n| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It correctly identifies the special identifier `##` used by the WordPiece Model for continuing subwords, which matches the reference answer."}, {"question": "What is the purpose of the \ud83e\udde8 Diffusers tutorials?\n", "true_answer": "To provide a gentle introduction to diffusion models and help understand the library fundamentals.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/tutorials/tutorial_overview.md", "generated_answer": "The purpose of the \ud83e\udde8 Diffusers tutorials is to provide a gentle introduction to diffusion models and help users understand the library fundamentals, including the core components and how to use the library effectively. The tutorials aim to teach users how to use a pipeline for inference to rapidly generate outputs and deconstruct that pipeline to understand how to use the library as a modular toolbox for building their own diffusion systems. Additionally, the tutorials guide users on how to train their own diffusion models to generate desired outputs, ultimately equipping them with the necessary skills to explore the library independently for their own projects and applications. (Document 0)", "retrieved_docs": ["# Overview\n\nWelcome to \ud83e\udde8 Diffusers! If you're new to diffusion models and generative AI, and want to learn more, then you've come to the right place. These beginner-friendly tutorials are designed to provide a gentle introduction to diffusion models and help you understand the library fundamentals - the core components and how \ud83e\udde8 Diffusers is meant to be used.\n\nYou'll learn how to use a pipeline for inference to rapidly generate things, and then deconstruct that pipeline to really understand how to use the library as a modular toolbox for building your own diffusion systems. In the next lesson, you'll learn how to train your own diffusion model to generate what you want.\n\nAfter completing the tutorials, you'll have gained the necessary skills to start exploring the library on your own and see how to use it for your own projects and applications.", "# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:", "\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or training your own diffusion models, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](https://huggingface.co/docs/diffusers/conceptual/philosophy#usability-over-performance), [simple over easy](https://huggingface.co/docs/diffusers/conceptual/philosophy#simple-over-easy), and [customizability over abstractions](https://huggingface.co/docs/diffusers/conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\n\ud83e\udd17 Diffusers offers three core components:", "Using \ud83e\udde8 `diffusers` at Hugging Face\n\nDiffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you\u2019re looking for a simple inference solution or want to train your own diffusion model, Diffusers is a modular toolbox that supports both. The library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.\n\n## Exploring Diffusers in the Hub", "## \ud83e\udde8 Diffusers\n\n[Diffusers](https://huggingface.co/docs/diffusers) provides pre-trained vision and audio diffusion models, and serves as a modular toolbox for inference and training. With this library, you can generate plausible images from natural language inputs amongst other creative use cases. Here is an example:\n\n```py\nfrom diffusers import DiffusionPipeline\n\ngenerator = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\ngenerator.to(\u201ccuda\u201d)\n\nimage = generator(\"An image of a squirrel in Picasso style\").images[0]\n```", "Training examples of the Diffusers library should adhere to the following philosophy:\n- All the code necessary to run the examples should be found in a single Python file.\n- One should be able to run the example from the command line with `python <your-example>.py --args`.\n- Examples should be kept simple and serve as **an example** on how to use Diffusers for training. The purpose of example scripts is **not** to create state-of-the-art diffusion models, but rather to reproduce known training schemes without adding too much custom logic. As a byproduct of this point, our examples also strive to serve as good educational materials.", "## Diffusers\n\nDiffusers is a generative AI library for creating images and videos from text or images with diffusion models. LoRA is an especially popular training method for diffusion models because you can very quickly train and share diffusion models to generate images in new styles. To make it easier to use and try multiple LoRA models, Diffusers uses the PEFT library to help manage different adapters for inference."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "The response provided aligns well with the reference answer, covering the key aspects of providing an introduction to diffusion models and understanding the library fundamentals. It goes beyond the reference answer by detailing the use of pipelines for inference, the modular nature of the library, and the training of diffusion models. The response is factually correct and accurately expands on the purpose of the \ud83e\udde8 Diffusers tutorials as described in the reference answer."}, {"question": "What is the default setting for the `allow_flagging` parameter in Gradio's `Interface`?\n", "true_answer": "\"manual\"", "source_doc": "gradio-app/gradio/blob/main/guides/cn/07_other-tutorials/using-flagging.md", "generated_answer": "The default setting for the `allow_flagging` parameter in Gradio's `Interface` is `\"manual\"`. [Document 1]", "retrieved_docs": ["## The **Flag** button in `gradio.Interface`\n\nFlagging with Gradio's `Interface` is especially easy. By default, underneath the output components, there is a button marked **Flag**. When a user testing your model sees input with interesting output, they can click the flag button to send the input and output data back to the machine where the demo is running. The sample is saved to a CSV log file (by default). If the demo involves images, audio, video, or other types of files, these are saved separately in a parallel directory and the paths to these files are saved in the CSV file.\n\nThere are [four parameters](https://gradio.app/docs/#interface-header) in `gradio.Interface` that control how flagging works. We will go over them in greater detail.", "- `allow_flagging`: this parameter can be set to either `\"manual\"` (default), `\"auto\"`, or `\"never\"`.\n  - `manual`: users will see a button to flag, and samples are only flagged when the button is clicked.\n  - `auto`: users will not see a button to flag, but every sample will be flagged automatically.\n  - `never`: users will not see a button to flag, and no sample will be flagged.\n- `flagging_options`: this parameter can be either `None` (default) or a list of strings.\n  - If `None`, then the user simply clicks on the **Flag** button and no additional options are shown.", "iface.launch()\n```\n\nNotice that we define our own\ninstance of `gradio.HuggingFaceDatasetSaver` using our Hugging Face token and\nthe name of a dataset we'd like to save samples to. In addition, we also set `allow_flagging=\"manual\"`\nbecause on Hugging Face Spaces, `allow_flagging` is set to `\"never\"` by default. Here's our demo:\n\n<gradio-app space=\"gradio/calculator-flagging-crowdsourced/\"></gradio-app>\n\nYou can now see all the examples flagged above in this [public Hugging Face dataset](https://huggingface.co/datasets/aliabd/crowdsourced-calculator-demo).", "- Fixes flagging when `allow_flagging=\"auto\"` in `gr.Interface()` by [@abidlabs](https://github.com/abidlabs) in [PR 2695](https://github.com/gradio-app/gradio/pull/2695)\n- Fixed bug where passing a non-list value to `gr.CheckboxGroup` would crash the entire app by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2866](https://github.com/gradio-app/gradio/pull/2866)", "- If a list of strings are provided, then the user sees several buttons, corresponding to each of the strings that are provided. For example, if the value of this parameter is `[\"Incorrect\", \"Ambiguous\"]`, then buttons labeled **Flag as Incorrect** and **Flag as Ambiguous** appear. This only applies if `allow_flagging` is `\"manual\"`.\n  - The chosen option is then logged along with the input and output.\n- `flagging_dir`: this parameter takes a string.\n  - It represents what to name the directory where flagged data is stored.\n- `flagging_callback`: this parameter takes an instance of a subclass of the `FlaggingCallback` class\n  - Using this parameter allows you to write custom code that gets run when the flag button is clicked\n  - By default, this is set to an instance of `gr.CSVLogger`", "iface = gr.Interface(\n    calculator,\n    [\"number\", gr.Radio([\"add\", \"subtract\", \"multiply\", \"divide\"]), \"number\"],\n    \"number\",\n    allow_flagging=\"manual\"\n)\n\niface.launch()\n```\n\n<gradio-app space=\"gradio/calculator-flag-basic/\"></gradio-app>\n\nWhen you click the flag button above, the directory where the interface was launched will include a new flagged subfolder, with a csv file inside it. This csv file includes all the data that was flagged.\n\n```directory\n+-- flagged/\n|   +-- logs.csv\n```\n\n_flagged/logs.csv_", "Gradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.\n\n## The **Flag** button in `gradio.Interface`"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual as it matches the reference answer which states that the default setting for the `allow_flagging` parameter in Gradio's `Interface` is `\"manual\"`."}, {"question": "Where can the full code for the Stable Diffusion demo be found?\n", "true_answer": "https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "source_doc": "gradio-app/gradio/blob/main/demo/stable-diffusion/DESCRIPTION.md", "generated_answer": "The full code for the Stable Diffusion demo can be found at https://hf.co/spaces/stabilityai/stable-diffusion/tree/main. [Document 0]", "retrieved_docs": ["ote: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main", "The original codebase can be found at [Xiang-cd/DiffEdit-stable-diffusion](https://github.com/Xiang-cd/DiffEdit-stable-diffusion), and you can try it out in this [demo](https://blog.problemsolversguild.com/technical/research/2022/11/02/DiffEdit-Implementation.html).\n\nThis pipeline was contributed by [clarencechen](https://github.com/clarencechen). \u2764\ufe0f\n\n## Tips", "Gradio Demo: stable-diffusion\n### Note: This is a simplified version of the code needed to create the Stable Diffusion demo. See full code here: https://hf.co/spaces/stabilityai/stable-diffusion/tree/main\n        \n\n\n```\n!pip install -q gradio diffusers transformers nvidia-ml-py3 ftfy torch\n```\n\n\n```\nimport gradio as gr\nimport torch\nfrom diffusers import StableDiffusionPipeline\nfrom PIL import Image\nimport os", "You can find the original codebase for Stable Diffusion v1.0 at [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion) and Stable Diffusion v2.0 at [Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion) as well as their original scripts for various tasks. Additional official checkpoints for the different Stable Diffusion versions and tasks can be found on the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations. Explore these organizations to find the best checkpoint for your use-case!", "The original codebase can be found at [LambdaLabsML/lambda-diffusers](https://github.com/LambdaLabsML/lambda-diffusers#stable-diffusion-image-variations) and additional official checkpoints for image variation can be found at [lambdalabs/sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers).\n\n<Tip>\n\nMake sure to check out the Stable Diffusion [Tips](./overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently!\n\n</Tip>\n\n## StableDiffusionImageVariationPipeline", "Let's get started! Code samples are available on [Gitlab](https://gitlab.com/juliensimon/huggingface-demos/-/tree/main/optimum/stable_diffusion_intel).\n\n## The Diffusers library\n\nThe [Diffusers](https://huggingface.co/docs/diffusers/index) library makes it extremely simple to generate images with Stable Diffusion models. If you're not familiar with these models, here's a great [illustrated introduction](https://jalammar.github.io/illustrated-stable-diffusion/).\n\nFirst, let's create a virtual environment with the required libraries: Transformers, Diffusers, Accelerate, and PyTorch.", "## Tips:\n\n- Stable Diffusion has the same architecture as [Latent Diffusion](https://arxiv.org/abs/2112.10752) but uses a frozen CLIP Text Encoder instead of training the text encoder jointly with the diffusion model.\n- An in-detail explanation of the Stable Diffusion model can be found under [Stable Diffusion with \ud83e\udde8 Diffusers](https://huggingface.co/blog/stable_diffusion).\n- If you don't want to rely on the Hugging Face Hub and having to pass a authentication token, you can\ndownload the weights with `git lfs install; git clone https://huggingface.co/runwayml/stable-diffusion-v1-5` and instead pass the local path to the cloned folder to `from_pretrained` as shown below.\n- Stable Diffusion can work with a variety of different samplers as is shown below."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, providing the full URL where the Stable Diffusion demo code can be found."}, {"question": "What transformation does the FNet model use to replace the self-attention layer in a BERT model?\n", "true_answer": "Fourier transform", "source_doc": "huggingface/transformers/blob/main/docs/source/en/model_doc/fnet.md", "generated_answer": "The FNet model replaces the self-attention layer in a BERT model with a Fourier transform, specifically returning only the real parts of the transform. [Document 0]", "retrieved_docs": ["-->\n\n# FNet\n\n## Overview\n\nThe FNet model was proposed in [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) by\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon. The model replaces the self-attention layer in a BERT\nmodel with a fourier transform which returns only the real parts of the transform. The model is significantly faster\nthan the BERT model because it has fewer parameters and is more memory efficient. The model achieves about 92-97%\naccuracy of BERT counterparts on GLUE benchmark, and trains much faster than the BERT model. The abstract from the\npaper is the following:", "*We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the\nself-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text\nclassification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE\nbenchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths,\nour FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena\nbenchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all", "2.  Second, BERT\\'s *bi-directional* self-attention layers have to be\n    changed to *uni-directional* self-attention layers to comply with\n    auto-regressive generation. Because both the bi-directional and the\n    uni-directional self-attention layer are based on the same *key*,\n    *query* and *value* projection weights, the decoder\\'s\n    self-attention layer weights can be initialized with BERT\\'s\n    self-attention layer weights. *E.g.* the query, key and value weight\n    parameters of the decoder\\'s uni-directional self-attention layer\n    are initialized with those of BERT\\'s bi-directional self-attention", "Here's a TLDR explaining how Perceiver works:\n\nThe main problem with the self-attention mechanism of the Transformer is that the time and memory requirements scale\nquadratically with the sequence length. Hence, models like BERT and RoBERTa are limited to a max sequence length of 512\ntokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a set\nof latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements don't\ndepend on the length of the inputs anymore, as one uses a fixed amount of latent variables, like 256 or 512. These are\nrandomly initialized, after which they are trained end-to-end using backpropagation.", "As an example, two years ago, one might have defined BERT's self attention layer as the standard attention layer used by all Transformers models. Logically, a \"standard\" attention function could have been moved into a central `attention.py` file. But then came attention layers that added relative positional embeddings in each attention layer (T5), multiple different forms of chunked attention (Reformer, Longformer, BigBird), and separate attention mechanism for position and word embeddings (DeBERTa), etc... Every time we would have to have asked ourselves whether the \"standard\" attention function should be adapted or whether it would have been better to add a new attention function to `attention.py`. But then how do we name it? `attention_with_positional_embd`, `reformer_attention`, `deberta_attention`?", "techniques have not been adopted by NLP neural network designers. We demonstrate how to replace several operations in\nself-attention layers with grouped convolutions, and we use this technique in a novel network architecture called\nSqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test\nset. The SqueezeBERT code will be released.*", "### feed forward chunking\n\nIn each residual attention block in transformers the self-attention layer is usually followed by 2 feed forward layers.\nThe intermediate embedding size of the feed forward layers is often bigger than the hidden size of the model (e.g., for\n`bert-base-uncased`)."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response accurately identifies the Fourier transform as the replacement for the self-attention layer in the FNet model, which aligns with the reference answer. Additionally, the response provides extra detail by mentioning that only the real parts of the transform are returned, which is a factual statement that adds to the accuracy of the response without contradicting the reference answer."}, {"question": "What type of test should typically accompany a bug fix in Gradio's testing strategy?\n", "true_answer": "Dynamic code test", "source_doc": "gradio-app/gradio/blob/main/test-strategy.md", "generated_answer": "A bug fix in Gradio's testing strategy should typically be accompanied by tests wherever it is reasonably possible. This ensures that the fix is verified and helps prevent regressions. [Document 0]", "retrieved_docs": ["- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)\n- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)\n- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)\n- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)\n- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)\n- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)\n\n## Types of testing", "## Objectives\n\nThe purposes of all testing activities on Gradio fit one of the following objectives:\n\n1. Ensure that the Gradio library functions as we expect it to.\n2. Enable the maintenance team to quickly identify both the presence and source of defects.\n3. Prevent regressions, i.e. if we fix something it should stay fixed.\n4. Improve the quality of the codebase in order to ease maintenance efforts.\n5. Reduce the amount of manual testing required.\n\n## Scope\n\nTesting is always a tradeoff. We can't cover everything unless we want to spend all of our time writing and running tests. We should focus on a few keys areas.\n\nWe should not focus on code coverage but on test coverage following the below criteria:", "- Updated the minimum FastApi used in tests to version 0.87 by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2647](https://github.com/gradio-app/gradio/pull/2647)\n- Fixed bug where interfaces with examples could not be loaded with `gr.Interface.load` by [@freddyaboulton](https://github.com/freddyaboulton) [PR 2640](https://github.com/gradio-app/gradio/pull/2640)", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.1.0-beta.6\n\n### Features", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.2.0-beta.6\n\n### Features", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.3.0-beta.6\n\n### Features", "### Fixes\n\n- [#6046](https://github.com/gradio-app/gradio/pull/6046) [`dbb7de5e0`](https://github.com/gradio-app/gradio/commit/dbb7de5e02c53fee05889d696d764d212cb96c74) - fix tests. Thanks [@pngwn](https://github.com/pngwn)!\n\n## 0.4.0-beta.6\n\n### Features"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided does not specify the type of test that should accompany a bug fix in Gradio's testing strategy. The reference answer indicates that a \"Dynamic code test\" is the correct type of test. Since the response fails to mention \"Dynamic code test\" or any specific type of test, it does not meet the criteria for being correct, accurate, and factual based on the reference answer. Therefore, the response is completely incorrect as per the given score rubric."}, {"question": "How can you force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate?\n", "true_answer": "By passing `fp16=True` to the Accelerator init.", "source_doc": "huggingface/blog/blob/main/accelerate-library.md", "generated_answer": "You can force mixed precision training when initializing the Accelerator in \ud83e\udd17 Accelerate by specifying it during the initialization of the `Accelerator`. This is handled in the `prepare` call, which will take care of setting up mixed precision training for you. [Document 0]", "retrieved_docs": ["First we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). \nThen we can enable gradient checkpointing by calling the model's [`~PreTrainedModel.gradient_checkpointing_enable`] method. \nWhen we initialize the [`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator) \nwe can specify if we want to use mixed precision training and it will take care of it for us in the [`prepare`] call. \nDuring the [`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare)", "</hfoption>\n</hfoptions>\n\n<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment (try enabling `torch.compile` to significantly speedup training):\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "If you prefer to use \ud83e\udd17 Accelerate, find the \ud83e\udd17 Accelerate example [further in this guide](#using-accelerate). \n\n### BF16\n\nIf you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \nbf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \nis `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \nis about the same as fp32 - because both have 8-bits used for the numerical range.\n\nYou can enable BF16 in the \ud83e\udd17 Trainer with:\n\n```python\ntraining_args = TrainingArguments(bf16=True, **default_args)\n```\n\n### TF32", "<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "<Tip>\n\n\ud83e\udd17 Accelerate is a library for helping you train on multiple GPUs/TPUs or with mixed-precision. It'll automatically configure your training setup based on your hardware and environment. Take a look at the \ud83e\udd17 Accelerate [Quick tour](https://huggingface.co/docs/accelerate/quicktour) to learn more.\n\n</Tip>\n\nInitialize an \ud83e\udd17 Accelerate environment:\n\n```bash\naccelerate config\n```\n\nTo setup a default \ud83e\udd17 Accelerate environment without choosing any configurations:\n\n```bash\naccelerate config default\n```\n\nOr if your environment doesn't support an interactive shell like a notebook, you can use:\n\n```bash\nfrom accelerate.utils import write_basic_config\n\nwrite_basic_config()\n```", "<Tip>\n\nNote: when using mixed precision with a small model and a large batch size, there will be some memory savings but with a \nlarge model and a small batch size, the memory use will be larger.\n\n</Tip>\n\nYou can combine the above methods to get a cumulative effect. These techniques are available to you whether you are \ntraining your model with [`Trainer`] or writing a pure PyTorch loop, in which case you can [configure these optimizations \nwith \ud83e\udd17 Accelerate](#using-accelerate)."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "3", "eval_feedback_GPT4": "Feedback: The response correctly identifies that mixed precision training is specified during the initialization of the Accelerator in \ud83e\udd17 Accelerate. However, it does not mention the exact parameter `fp16=True` as stated in the reference answer. The response is somewhat correct but lacks the specific detail provided in the reference answer."}, {"question": "What is the purpose of tokenizers in the NLP pipeline?\n", "true_answer": "To translate text into data that can be processed by the model.", "source_doc": "huggingface/course/blob/main/chapters/en/chapter2/4.mdx", "generated_answer": "The purpose of tokenizers in the NLP pipeline is to translate text into numerical data that can be processed by models, as models can only process numbers. Tokenizers handle the conversion from text to numerical inputs for the neural network and the conversion back to text when needed. They split the input into words, subwords, or symbols (like punctuation) called tokens, map each token to an integer, and add additional inputs that may be useful to the model. The tokenization process involves several steps, including normalization, pre-tokenization, applying the tokenizer model, and post-processing. These steps ensure that the text is appropriately prepared for the model to process. (Sources: Documents 0, 1, 2, 4, 5)", "retrieved_docs": ["<Youtube id=\"VFp38yj8h3A\"/>\n\nTokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we'll explore exactly what happens in the tokenization pipeline. \n\nIn NLP tasks, the data that is generally processed is raw text. Here's an example of such text:\n\n```\nJim Henson was a puppeteer\n```", "Then we'll look at the tokenizer API, which is the other main component of the `pipeline()` function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we'll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level `tokenizer()` function.\n\n<Tip>\n\u26a0\ufe0f In order to benefit from all features available with the Model Hub and \ud83e\udd17 Transformers, we recommend <a href=\"https://huggingface.co/join\">creating an account</a>.\n</Tip>", ". To enable this, the fast tokenizers store additional information at each step of their internal pipeline. That internal pipeline consists of normalization, where we apply some cleaning to the text, like lowercasing or removing the accents;() pre-tokenization, which is where we split the texts into words;() then we apply the model of the tokenizer, which is where the words are splits into tokens,() before finally doing the post-processing, where special tokens are added. From the beginning to the end of the pipeline, the tokenizer keeps track of each span of text that corresponds to each word, then each token. We will see how useful it is when we tackle the following tasks: when doing masked language modeling, one variation that gets state-of-the-art results is to mask all the tokens of a given word instead of randomly chosen tokens. This will require us to use the word IDs we saw", "The core of `tokenizers`, written in Rust.\nProvides an implementation of today's most used tokenizers, with a focus on performance and\nversatility.\n\n## What is a Tokenizer\n\nA Tokenizer works as a pipeline, it processes some raw text as input and outputs an `Encoding`.\nThe various steps of the pipeline are:", "Let's quickly go over each of these.\n\n## Preprocessing with a tokenizer[[preprocessing-with-a-tokenizer]]\n\nLike other neural networks, Transformer models can't process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a *tokenizer*, which will be responsible for:\n\n- Splitting the input into words, subwords, or symbols (like punctuation) that are called *tokens*\n- Mapping each token to an integer\n- Adding additional inputs that may be useful to the model", "The tokenization pipeline\n\nWhen calling `Tokenizer.encode` or\n`Tokenizer.encode_batch`, the input\ntext(s) go through the following pipeline:\n\n-   `normalization`\n-   `pre-tokenization`\n-   `model`\n-   `post-processing`\n\nWe'll see in details what happens during each of those steps in detail,\nas well as when you want to `decode <decoding>` some token ids, and how the \ud83e\udd17 Tokenizers library allows you\nto customize each of those steps to your needs. If you're already\nfamiliar with those steps and want to learn by seeing some code, jump to\n`our BERT from scratch example <example>`.\n\nFor the examples that require a `Tokenizer` we will use the tokenizer we trained in the\n`quicktour`, which you can load with:", "{/if}\n\nIn this section we will take a closer look at the capabilities of the tokenizers in \ud83e\udd17 Transformers. Up to now we have only used them to tokenize inputs or decode IDs back into text, but tokenizers -- especially those backed by the \ud83e\udd17 Tokenizers library -- can do a lot more. To illustrate these additional features, we will explore how to reproduce the results of the `token-classification` (that we called `ner`) and `question-answering` pipelines that we first encountered in [Chapter 1](/course/chapter1).\n\n<Youtube id=\"g8quOxoqhHQ\"/>"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by explaining that tokenizers translate text into numerical data for model processing. The response goes into detail about the role of tokenizers in the NLP pipeline, including the steps involved in tokenization and the conversion of text to numerical inputs, which is a more detailed explanation of the reference answer but still correct and factual."}, {"question": "What is the purpose of the Safety Checker in the Diffusers library?\n", "true_answer": "The Safety Checker checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated to mitigate the risk of generating harmful content.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/conceptual/ethical_guidelines.md", "generated_answer": "The purpose of the Safety Checker in the Diffusers library is to screen generated outputs against known hardcoded harmful and NSFW content. It compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. This component is intended to help users interact with generative models responsibly and ethically by flagging inappropriate content generated during inference. Model creators can choose to incorporate this component into their models if they want. (Sources: Document 0, Document 1, Document 3)", "retrieved_docs": ["- [**Safety Checker**](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py): It checks and compares the class probability of a set of hard-coded harmful concepts in the embedding space against an image after it has been generated. The harmful concepts are intentionally hidden to prevent reverse engineering of the checker.\n\n- **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository\u2019s authors to have more control over its use.", "### Safety checker\n\nDiffusion models like Stable Diffusion can generate harmful content, which is why \ud83e\udde8 Diffusers has a [safety checker](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) to check generated outputs against known hardcoded NSFW content. If you'd like to disable the safety checker for whatever reason, pass `None` to the `safety_checker` argument:\n\n```python\nfrom diffusers import DiffusionPipeline", "\"\"\"\nYou have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide by the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend keeping the safety filter enabled in all public-facing circumstances, disabling it only for use cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n\"\"\"\n```", "To learn more about how we optimize inference with \ud83e\udd17\u00a0Diffusers, check out the [docs](https://huggingface.co/docs/diffusers/optimization/opt_overview)!\n\n## Ethics and safety\n\nGenerative models are cool, but they also have the ability to produce harmful and NSFW content. To help users interact with these models responsibly and ethically, we\u2019ve added a [`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) component that flags inappropriate content generated during inference. Model creators can choose to incorporate this component into their models if they want.", "- **Reproducibility**: we aim to be transparent about the reproducibility of upstream code, models, and datasets when made available through the Diffusers library.\n\n- **Responsibility**: as a community and through teamwork, we hold a collective responsibility to our users by anticipating and mitigating this technology's potential risks and dangers.\n\n\n## Examples of implementations: Safety features and Mechanisms\n\nThe team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.\n\n- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.", "# Diffusers\n\n\ud83e\udd17 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you're looking for a simple inference solution or want to train your own diffusion model, \ud83e\udd17 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on [usability over performance](conceptual/philosophy#usability-over-performance), [simple over easy](conceptual/philosophy#simple-over-easy), and [customizability over abstractions](conceptual/philosophy#tweakable-contributorfriendly-over-abstraction).\n\nThe library has three main components:", "- `\"feature_extractor\"`: a [`~transformers.CLIPImageProcessor`] from \ud83e\udd17 Transformers.\n- `\"safety_checker\"`: a [component](https://github.com/huggingface/diffusers/blob/e55687e1e15407f60f32242027b7bb8170e58266/src/diffusers/pipelines/stable_diffusion/safety_checker.py#L32) for screening against harmful content.\n- `\"scheduler\"`: an instance of [`PNDMScheduler`].\n- `\"text_encoder\"`: a [`~transformers.CLIPTextModel`] from \ud83e\udd17 Transformers.\n- `\"tokenizer\"`: a [`~transformers.CLIPTokenizer`] from \ud83e\udd17 Transformers."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by explaining the purpose of the Safety Checker in the Diffusers library, which is to screen generated outputs for harmful and NSFW content by comparing the class probability of hard-coded harmful concepts in the embedding space against an image after it has been generated. The response also adds useful context by mentioning the ethical considerations and the optional nature of incorporating this component into models, which is in line with the purpose of the Safety Checker as described in the reference answer."}, {"question": "What Python class allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub?\n", "true_answer": "HfApi", "source_doc": "huggingface/huggingface_hub/blob/main/docs/source/en/guides/community.md", "generated_answer": "The Python class that allows you to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub is the `HfApi` class. [Document 0]", "retrieved_docs": ["!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# Interact with Discussions and Pull Requests \n\nThe `huggingface_hub` library provides a Python interface to interact with Pull Requests and Discussions on the Hub.\nVisit [the dedicated documentation page](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)\nfor a deeper view of what Discussions and Pull Requests on the Hub are, and how they work under the hood.\n\n## Retrieve Discussions and Pull Requests from the Hub\n\nThe `HfApi` class allows you to retrieve Discussions and Pull Requests on a given repo:", "[Discussions](https://huggingface.co/gpt2/discussions?type=discussion) allow community members ask and answer questions as well as share their ideas and suggestions directly with the repository owners and the community. Anyone can create and participate in discussions in the community tab of a repository.\n\n## Pull requests\n\n![Pull requests on the Hugging Face Hub](assets/76_community_update/new-pr.png)\n\n[Pull requests](https://huggingface.co/gpt2/discussions?type=pull_request) allow community members open, comment, merge, or close pull requests directly from the website. The easiest way to open a pull request is to use the \"Collaborate\" button in the \"Files and versions\" tab. It will let you do single file contributions very easily.", "The Hugging Face Hub is also a central place for feedback and development in machine learning. Teams use [pull requests and discussions](https://huggingface.co/docs/hub/repositories-pull-requests-discussions) to support peer reviews on models, datasets, and spaces, improve collaboration and accelerate their ML work.\n\n<figure class=\"image table text-center m-0 w-full\">\n  <medium-zoom background=\"rgba(0,0,0,.7)\" alt=\"Pull requests and discussions on a model\" src=\"assets/92_introducing_private_hub/pull-requests-and-discussions.png\"></medium-zoom>\n  <figcaption>Pull requests and discussions on a model</figcaption>\n</figure>", "### The client library, `huggingface_hub`\n\nThis repository hosts the `huggingface_hub`, the client library that interfaces any Python script with the Hugging Face Hub.\nIts implementation lives in `src/huggingface_hub` while the tests are located in `tests/`.\n\nThere are many ways you can contribute to this client library:\n* Fixing outstanding issues with the existing code;\n* Contributing to the examples or to the documentation;\n* Submitting issues related to bugs or desired new features.\n\n## Submitting a new issue or feature request\n\nDo your best to follow these guidelines when submitting an issue or a feature\nrequest. It will make it easier for us to come back to you quickly and with good\nfeedback.\n\n### Did you find a bug?\n\nThe `huggingface_hub` library is robust and reliable thanks to the users who notify us of\nthe problems they encounter. So thank you for reporting an issue.", "You can also use [`HfApi.create_discussion`] (respectively [`HfApi.create_pull_request`]) to create a Discussion (respectively a Pull Request) on a repo.\nOpening a Pull Request this way can be useful if you need to work on changes locally. Pull Requests opened this way will be in `\"draft\"` mode.\n\n```python\n>>> from huggingface_hub import create_discussion, create_pull_request\n\n>>> create_discussion(\n...     repo_id=\"username/repo-name\",\n...     title=\"Hi from the huggingface_hub library!\",\n...     token=\"<insert your access token here>\",\n... )\nDiscussionWithDetails(...)", "## Upload with Python\n\nUsers who prefer to upload a dataset programmatically can use the [huggingface_hub](https://huggingface.co/docs/huggingface_hub/index) library. This library allows users to interact with the Hub from Python. \n\n1. Begin by installing the library:\n\n```bash\npip install huggingface_hub\n```\n\n2. To upload a dataset on the Hub in Python, you need to log in to your Hugging Face account:\n\n```bash\nhuggingface-cli login\n```\n\n3. Use the [`push_to_hub()`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.DatasetDict.push_to_hub) function to help you add, commit, and push a file to your repository:", "```python\n>>> from huggingface_hub import get_repo_discussions\n>>> for discussion in get_repo_discussions(\n...    repo_id=\"bigscience/bloom\",\n...    author=\"ArthurZ\",\n...    discussion_type=\"pull_request\",\n...    discussion_status=\"open\",\n... ):\n...     print(f\"{discussion.num} - {discussion.title} by {discussion.author}, pr: {discussion.is_pull_request}\")\n\n# 19 - Add Flax weights by ArthurZ, pr: True\n```"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It correctly identifies `HfApi` as the Python class used to retrieve Discussions and Pull Requests from a given repository on the Hugging Face Hub, which matches the reference answer."}, {"question": "What is the name of the new library introduced by Hugging Face for hosting scikit-learn models?\n", "true_answer": "Skops", "source_doc": "huggingface/blog/blob/main/skops.md", "generated_answer": "The name of the new library introduced by Hugging Face for hosting scikit-learn models is Skops. [Document 0]", "retrieved_docs": ["--\ntitle: \"Introducing Skops\"\nthumbnail: /blog/assets/94_skops/introducing_skops.png\nauthors:\n- user: merve\n- user: adrin\n- user: BenjaminB\n---\n\n# Introducing Skops\n\n\n##\u00a0Introducing Skops\n\nAt Hugging Face, we are working on tackling various problems in open-source machine learning, including, hosting models securely and openly, enabling reproducibility, explainability and collaboration. We are thrilled to introduce you to our new library: Skops! With Skops, you can host your scikit-learn models on the Hugging Face Hub, create model cards for model documentation and collaborate with others.\n\nLet's go through an end-to-end example: train a model first, and see step-by-step how to leverage Skops for sklearn in production.", "## Using the Hugging Face Client Library\n\nYou can use the [`huggingface_hub`](https://github.com/huggingface/huggingface_hub) library to create, delete, update and retrieve information from repos. You can also download files from repos or integrate them into your library! For example, you can quickly load a Scikit-learn model with a few lines.\n\n```py\nfrom huggingface_hub import hf_hub_download\nimport joblib\n\nREPO_ID = \"YOUR_REPO_ID\"\nFILENAME = \"sklearn_model.joblib\"\n\nmodel = joblib.load(\n    hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n)\n```\n\n## Using Git\n\nSince all models on the Model Hub are Git repositories, you can clone the models locally by running:", "---\n\n## Welcome to the huggingface_hub library\n\nThe `huggingface_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a simple way to do all these things with Python.\n\n## Key features", "Integrate your library with the Hub\n\nThe Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone.\n\nIntegrating the Hub with your library provides many benefits, including:", "## Support for third-party libraries\n\nCentral to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from \ud83e\udd17 Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# \ud83e\udd17 Hub client library\n\nThe `huggingface_hub` library allows you to interact with the [Hugging Face\nHub](https://hf.co), a machine learning platform for creators and collaborators.\nDiscover pre-trained models and datasets for your projects or play with the hundreds of\nmachine learning apps hosted on the Hub. You can also create and share your own models\nand datasets with the community. The `huggingface_hub` library provides a simple way to\ndo all these things with Python.", "## What can you find on the Hugging Face Hub?\n\n### Models \n\nThe Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the Hub via these libraries.\n\n### Datasets\nThe Hugging Face hub hosts over 30,000 datasets. These datasets cover a range of domains and modalities, including text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine learning models.\n\n### Spaces\n\nHugging Face [Spaces](https://huggingface.co/docs/hub/spaces) is a platform that allows you to host machine learning demos and applications. These Spaces range from simple demos allowing you to explore the predictions made by a machine learning model to more involved applications."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the name of the new library introduced by Hugging Face for hosting scikit-learn models as \"Skops,\" which matches the reference answer. The inclusion of \"[Document 0]\" does not detract from the accuracy of the response regarding the name of the library."}, {"question": "What is the purpose of Textual Inversion?\n", "true_answer": "Textual Inversion is a training method for personalizing models by learning new text embeddings from a few example images.", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/api/loaders/textual_inversion.md", "generated_answer": "The purpose of Textual Inversion is to personalize image generation models, such as Stable Diffusion, by learning new concepts from just a few example images. This technique updates the text embeddings to match the example images, allowing for more control over the generated images and tailoring the model towards specific concepts. It is particularly useful for teaching new concepts to a trained model with minimal data and for creating small, shareable trained weights. Textual Inversion can also be used to create negative embeddings to discourage the generation of undesirable features in images. (Sources: Documents 0, 1, 2, 3, 5)", "retrieved_docs": ["# Textual Inversion\n\n[Textual Inversion](https://hf.co/papers/2208.01618) is a training technique for personalizing image generation models with just a few example images of what you want it to learn. This technique works by learning and updating the text embeddings (the new embeddings are tied to a special word you must use in the prompt) to match the example images you provide.", "## Other Methods\n\nThe quest for easy fine-tuning is not new. In addition to Dreambooth, [_textual inversion_](https://huggingface.co/docs/diffusers/main/en/training/text_inversion) is another popular method that attempts to teach new concepts to a trained Stable Diffusion Model. One of the main reasons for using Textual Inversion is that trained weights are also small and easy to share. However, they only work for a single subject (or a small handful of them), whereas LoRA can be used for general-purpose fine-tuning, meaning that it can be adapted to new domains or datasets.", "# Textual inversion\n\n[[open-in-colab]]\n\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).\n\nThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text_inversion) training guide.\n\nImport the necessary libraries:", "## Textual inversion\n\n[Textual inversion](https://textual-inversion.github.io/) is very similar to DreamBooth and it can also personalize a diffusion model to generate certain concepts (styles, objects) from just a few images. This method works by training and finding new embeddings that represent the images you provide with a special word in the prompt. As a result, the diffusion model weights stay the same and the training process produces a relatively tiny (a few KBs) file.\n\nBecause textual inversion creates embeddings, it cannot be used on its own like DreamBooth and requires another model.\n\n```py\nfrom diffusers import AutoPipelineForText2Image\nimport torch", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)", "Textual inversion can also be trained on undesirable things to create *negative embeddings* to discourage a model from generating images with those undesirable things like blurry images or extra fingers on a hand. This can be an easy way to quickly improve your prompt. You'll also load the embeddings with [`~loaders.TextualInversionLoaderMixin.load_textual_inversion`], but this time, you'll need two more parameters:\n\n- `weight_name`: specifies the weight file to load if the file was saved in the \ud83e\udd17 Diffusers format with a specific name or if the file is stored in the A1111 format\n- `token`: specifies the special word to use in the prompt to trigger the embeddings", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is comprehensive and aligns well with the reference answer. It correctly identifies Textual Inversion as a method for personalizing image generation models by learning from a few examples. The response expands on the reference answer by explaining the process of updating text embeddings and the benefits of this technique, such as increased control over image generation and the ability to teach new concepts with minimal data. It also mentions the creation of negative embeddings, which is not included in the reference answer but is accurate and adds to the understanding of Textual Inversion's purpose. The response is factual and provides a clear explanation that is in line with the reference answer, demonstrating a complete understanding of the concept."}, {"question": "What is the recommended multiple of batch size for fp16 data type on an A100 GPU?\n", "true_answer": "64", "source_doc": "huggingface/transformers/blob/main/docs/source/en/perf_train_gpu_one.md", "generated_answer": "The recommended multiple of batch size for fp16 data type on an A100 GPU is 64. [Document 0]", "retrieved_docs": ["[Tensor Core Requirements](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc) \ndefine the multiplier based on the dtype and the hardware. For instance, for fp16 data type a multiple of 8 is recommended, unless \nit's an A100 GPU, in which case use multiples of 64.\n\nFor parameters that are small, consider also [Dimension Quantization Effects](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#dim-quantization). \nThis is where tiling happens and the right multiplier can have a significant speedup.\n\n## Gradient Accumulation", "This is a benchmark sample for the batch size = 1 case. The benchmark was run on a single NVIDIA A100-SXM4-80GB GPU. We used a prompt length of 512, and generated exactly 512 new tokens. The first row is the unquantized `fp16` baseline, while the other rows show memory consumption and performance using different AutoGPTQ kernels.", "*BS* is the batch size.\nThe Gaudi runs were performed in *bfloat16* precision and the A100 runs in *fp16* precision (more information [here](https://huggingface.co/docs/diffusers/optimization/fp16)). All runs were *single-device* runs.\n\n\n### Fine-tuning T5-3B", "5. **Adding all of the above -> 9.51 GB ~10GB -> 1 A100 40GB GPU required** \ud83e\udd2f. The reason for A100 40GB GPU is that the intermediate activations for long sequence lengths of 2048 and batch size of 4 for training lead to higher memory requirements. As we will see below, GPU memory required is 26GB which can be accommodated on A100 40GB GPU. Also, A100 GPUs have better compatibilty with Flash Attention 2.", "As the model needs 352GB in bf16 (bfloat16) weights (`176*2`), the most efficient set-up is 8x80GB A100 GPUs. Also 2x8x40GB A100s or 2x8x48GB A6000 can be used. The main reason for using these GPUs is that at the time of this writing they provide the largest GPU memory, but other GPUs can be used as well. For example, 24x32GB V100s can be used.\n\nUsing a single node will typically deliver a fastest throughput since most of the time intra-node GPU linking hardware is faster than inter-node one, but it's not always the case.\n\nIf you don't have that much hardware, it's still possible to run BLOOM inference on smaller GPUs, by using CPU or NVMe offload, but of course, the generation time will be much slower.", "Table 2: Benchmarking FSDP on GPT-2 XL (1.5B) model\n\nFrom Table 2, we can observe that DDP (w and w/o fp16) isn\u2019t even able to run with batch size of 1 and results in CUDA OOM error. FSDP with Zero-Stage 3 is able to be run on 2 GPUs with batch size of 5 (effective batch size =10 (5 X 2)). FSDP with CPU offload can further increase the max batch size to 14 per GPU when using 2 GPUs. **FSDP with CPU offload enables training GPT-2 1.5B model on a single GPU with a batch size of 10**. This enables ML practitioners with minimal compute resources to train such large models, thereby democratizing large model training.\n\n## Capabilities and limitations of the FSDP Integration", "These tests were run with a batch size of 1 in all cases, using [this script](https://huggingface.co/datasets/pcuenq/gists/blob/main/sayak_lcm_benchmark.py) by [Sayak Paul](https://huggingface.co/sayakpaul).\n\nFor cards with a lot of capacity, such as A100, performance increases significantly when generating multiple images at once, which is usually the case for production workloads.\n\n## LCM LoRAs and Models Released Today"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual as it directly matches the reference answer which states that the recommended multiple of batch size for fp16 data type on an A100 GPU is 64."}, {"question": "How do you run a Gradio Blocks app in reload mode using a Python IDE?\n", "true_answer": "Run `gradio run.py` in the terminal.", "source_doc": "gradio-app/gradio/blob/main/guides/09_other-tutorials/developing-faster-with-reload-mode.md", "generated_answer": "To run a Gradio Blocks app in reload mode using a Python IDE, you should use the `gradio` command instead of `python` when executing your script. For example, if your script is named `app.py`, you would run it in reload mode by typing `gradio app.py` in your terminal. This will automatically reload the app whenever changes are made to the code. [Document 0, Document 2, Document 3]", "retrieved_docs": ["## \ud83d\ude80 Run a Gradio app\n\nYou can get started by creating an `app.py` file in the root:\n\n```\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n   gr.Button()\n    \nif __name__ == \"__main__\":\n   demo.launch()\n```\n\nthen run:\n\n```\ngradio app.py\n```\n\nThis will start the backend server in reload mode, which will watch for changes in the `gradio` folder and reload the app if changes are made. By default, Gradio will launch on port 7860. You can also just use `python app.py`, but this won't automatically trigger updates. \n\n\nIf you're making frontend changes, start the frontend server:", "To make it faster and more convenient to write your code, we've made it easier to \"reload\" your Gradio apps instantly when you are developing in a **Python IDE** (like VS Code, Sublime Text, PyCharm, or so on) or generally running your Python code from the terminal. We've also developed an analogous \"magic command\" that allows you to re-run cells faster if you use **Jupyter Notebooks** (or any similar environment like Colab).\n\nThis short Guide will cover both of these methods, so no matter how you write Python, you'll leave knowing how to build Gradio apps faster.\n\n## Python IDE Reload \ud83d\udd25\n\nIf you are building Gradio Blocks using a Python IDE, your file of code (let's name it `run.py`) might look something like this:\n\n```python\nimport gradio as gr", "###### 2. Reload Mode \ud83d\udc68\u200d\ud83d\udcbb\n\nReload mode helps developers create gradio demos faster by automatically reloading the demo whenever the code changes. It can support development on Python IDEs (VS Code, PyCharm, etc), the terminal, as well as Jupyter notebooks.\n\nIf your demo code is in a script named `app.py`, instead of running `python app.py` you can now run `gradio app.py` and that will launch the demo in reload mode:\n\n```bash\nLaunching in reload mode on: http://127.0.0.1:7860 (Press CTRL+C to quit)\nWatching...\nWARNING: The --reload flag should not be used in production on Windows.\n```", "> [!TIP]\n > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=\"https://www.gradio.app/guides/developing-faster-with-reload-mode\">Hot Reloading Guide</a>.\n\n\n**Understanding the `Interface` Class**", "\u26a0\ufe0f Warning: the `gradio` command does not detect the parameters passed to the `launch()` methods because the `launch()` method is never called in reload mode. For example, setting `auth`, or `show_error` in `launch()` will not be reflected in the app.\n\nThere is one important thing to keep in mind when using the reload mode: Gradio specifically looks for a Gradio Blocks/Interface demo called `demo` in your code. If you have named your demo something else, you will need to pass in the name of your demo as the 2nd parameter in your code. So if your `run.py` file looked like this:\n\n```python\nimport gradio as gr", "```python\nimport gradio as gr\n\nwith gr.Blocks() as my_demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    my_demo.launch()\n```\n\nThen you would launch it in reload mode like this: `gradio run.py my_demo`.\n\nBy default, the Gradio use UTF-8 encoding for scripts. **For reload mode**, If you are using encoding formats other than UTF-8 (such as cp1252), make sure you've done like this:", "```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Greetings from Gradio!\")\n    inp = gr.Textbox(placeholder=\"What is your name?\")\n    out = gr.Textbox()\n\n    inp.change(fn=lambda x: f\"Welcome, {x}!\",\n               inputs=inp,\n               outputs=out)\n\nif __name__ == \"__main__\":\n    demo.launch()\n```\n\nThe problem is that anytime that you want to make a change to your layout, events, or components, you have to close and rerun your app by writing `python run.py`.\n\nInstead of doing this, you can run your code in **reload mode** by changing 1 word: `python` to `gradio`:"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided is incorrect as it suggests using a `gradio` command in place of `python` to run a Gradio Blocks app in reload mode, which is not a recognized command for running Gradio apps. The correct way to run a Gradio app in reload mode is not specified in the response. The reference answer indicates that the correct command should be `gradio run.py`, which is not mentioned in the response. Therefore, the response does not meet the criteria for being correct, accurate, and factual based on the reference answer."}, {"question": "How can you install the Hugging Face Unity API in your Unity project?\n", "true_answer": "To install the Hugging Face Unity API in your Unity project, go to `Window` -> `Package Manager`, click `+` and select `Add Package from git URL`, then enter `https://github.com/huggingface/unity-api.git`.", "source_doc": "huggingface/blog/blob/main/unity-api.md", "generated_answer": "To install the Hugging Face Unity API in your Unity project, follow these steps:\n\n1. Open your Unity project.\n2. Go to `Window` -> `Package Manager`.\n3. Click the `+` button and select `Add Package from git URL`.\n4. Enter the URL `https://github.com/huggingface/unity-api.git`.\n5. Once installed, the Unity API wizard should pop up. If it doesn't, go to `Window` -> `Hugging Face API Wizard`. [Document 0]", "retrieved_docs": ["## Installation\n\n1. Open your Unity project\n2. Go to `Window` -> `Package Manager`\n3. Click `+` and select `Add Package from git URL`\n4. Enter `https://github.com/huggingface/unity-api.git`\n5. Once installed, the Unity API wizard should pop up. If not, go to `Window` -> `Hugging Face API Wizard`\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/packagemanager.gif\">\n</figure>", "--\ntitle: \"How to Install and Use the Hugging Face Unity API\"\nthumbnail: /blog/assets/124_ml-for-games/unity-api-thumbnail.png\nauthors:\n- user: dylanebert\n---\n\n# How to Install and Use the Hugging Face Unity API\n\n<!-- {authors} --> \n\nThe [Hugging Face Unity API](https://github.com/huggingface/unity-api) is an easy-to-use integration of the [Hugging Face Inference API](https://huggingface.co/inference-api), allowing developers to access and use Hugging Face AI models in their Unity projects. In this blog post, we'll walk through the steps to install and use the Hugging Face Unity API.\n\n## Installation", "Use the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.\n\nTo use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.\n\n## Usage Tips\n\n1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.\n2. Address slow response times or performance issues by changing model endpoints to lower resource models.\n\n## Conclusion\n\nThe Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!", "6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n7. Test the API key by clicking `Test API key` in the API Wizard.\n8. Optionally, change the model endpoints to change which model to use. The model endpoint for any model that supports the inference API can be found by going to the model on the Hugging Face website, clicking `Deploy` -> `Inference API`, and copying the url from the `API_URL` field.\n9. Configure advanced settings if desired. For up-to-date information, visit the project repository at `https://github.com/huggingface/unity-api`\n10. To see examples of how to use the API, click `Install Examples`. You can now close the API Wizard.", "## Step 6: Optionally, Download the Hugging Face Unity WebGL Template\n\nYou can enhance your game's appearance in a Space by downloading the Hugging Face Unity WebGL template, available [here](https://github.com/huggingface/Unity-WebGL-template-for-Hugging-Face-Spaces). Just download the repository and drop it in your project files.\n\nThen, in the Player Settings panel, switch the WebGL template to Hugging Face. To do so, in Player Settings, click \"Resolution and Presentation\", then select the Hugging Face WebGL template.\n\n<figure class=\"image text-center\">\n  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/6.png\">\n</figure>", "- **[Compilation of AI tools for Game Dev](https://github.com/simoninithomas/awesome-ai-tools-for-game-dev)**\n- How to install the Unity Hugging Face API:\u00a0**https://huggingface.co/blog/unity-api**\n- AI Speech Recognition in Unity:\u00a0**https://huggingface.co/blog/unity-asr**\n- Making ML-powered web games with Transformers.js:\u00a0**https://huggingface.co/blog/ml-web-games**\n- Building a smart Robot AI using Hugging Face\u00a0\ud83e\udd17\u00a0and Unity:\u00a0**https://thomassimonini.substack.com/p/building-a-smart-robot-ai-using-hugging**", "Hugging Face Spaces are an easy way to build, host, and share demos. While they are typically used for Machine Learning demos, \nthey can also host playable Unity games. Here are some examples:\n- [Huggy](https://huggingface.co/spaces/ThomasSimonini/Huggy)\n- [Farming Game](https://huggingface.co/spaces/dylanebert/FarmingGame) \n- [Unity API Demo](https://huggingface.co/spaces/dylanebert/UnityDemo)\n\nHere's how you can host your own Unity game in a Space.\n\n## Step 1: Create a Space using the Static HTML template\n\nFirst, navigate to [Hugging Face Spaces](https://huggingface.co/new-space) to create a space."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer, detailing the steps to install the Hugging Face Unity API in a Unity project. The response includes the same essential steps as the reference answer, such as navigating to the Package Manager, using the `+` button to add a package from a git URL, and entering the correct URL for the Hugging Face Unity API repository. Additionally, the response provides an extra step to access the Unity API wizard, which is a useful piece of information for users, though not strictly necessary for the installation process. This additional information does not detract from the accuracy of the installation instructions. Therefore, the response meets the criteria for a score of 5 on the rubric."}, {"question": "What is the pretraining objective of the Wav2Vec2 context network?\n", "true_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task where the model has to predict the true quantized speech representation of the masked prediction from a set of false ones.", "source_doc": "huggingface/transformers/blob/main/docs/source/en/tasks_explained.md", "generated_answer": "The pretraining objective of the Wav2Vec2 context network is a contrastive task. The model is trained to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit. [Document 0]", "retrieved_docs": ["4. The pretraining objective of the context network is a *contrastive task*. The model has to predict the true quantized speech representation of the masked prediction from a set of false ones, encouraging the model to find the most similar context vector and quantized speech unit (the target label).\n\nNow that wav2vec2 is pretrained, you can finetune it on your data for audio classification or automatic speech recognition!\n\n### Audio classification", "Wav2Vec2 is a pretrained model for Automatic Speech Recognition (ASR)\nand was released in [September\n2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Alexei Baevski, Michael Auli, and Alex Conneau.\n\nUsing a novel contrastive pretraining objective, Wav2Vec2 learns\npowerful speech representations from more than 50.000 hours of unlabeled\nspeech. Similar, to [BERT\\'s masked language\nmodeling](http://jalammar.github.io/illustrated-bert/), the model learns\ncontextualized speech representations by randomly masking feature\nvectors before passing them to a transformer network.", "[Wav2Vec2](https://huggingface.co/Graphcore/wav2vec2-base-ipu) is a pretrained self-supervised model for automatic speech recognition. Using a novel contrastive pretraining objective, Wav2Vec2 learns powerful speech representations from large amounts of unlabelled speech data, followed by fine-tuning on a small amount of transcribed speech data, outperforming the best semi-supervised methods while being conceptually simpler.\n\n## Hugging Face Optimum Graphcore: building on a solid partnership\n\nGraphcore joined the [Hugging Face Hardware Partner Program](https://huggingface.co/hardware) in 2021 as a founding member, with both companies sharing the common goal of lowering the barriers for innovators seeking to harness the power of machine intelligence.", "The [pretrained Wav2Vec2 checkpoint](https://huggingface.co/facebook/wav2vec2-base) maps the speech signal to a\nsequence of context representations as illustrated in the figure above.\nA fine-tuned Wav2Vec2 checkpoint needs to map this sequence of context\nrepresentations to its corresponding transcription so that a linear\nlayer has to be added on top of the transformer block (shown in yellow).\nThis linear layer is used to classifies each context representation to a\ntoken class analogous how, *e.g.*, after pretraining a linear layer is\nadded on top of BERT\\'s embeddings for further classification - *cf.*\nwith *\\\"BERT\\\"* section of this [blog post](https://huggingface.co/blog/warm-starting-encoder-decoder).", "### Pretraining Wav2Vec2\n\nThe `run_pretrain.py` script allows one to pretrain a Wav2Vec2 model from scratch using Wav2Vec2's contrastive loss objective (see official [paper](https://arxiv.org/abs/2006.11477) for more information). \nIt is recommended to pre-train Wav2Vec2 with Trainer + Deepspeed (please refer to [this guide](https://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration) for more information).\n\nHere is an example of how you can use DeepSpeed ZeRO-2 to pretrain a small Wav2Vec2 model:", "![wav2vec2\\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/wav2vec2.png)\n\nFor the first time, it has been shown that pretraining, followed by\nfine-tuning on very little labeled speech data achieves competitive\nresults to state-of-the-art ASR systems. Using as little as 10 minutes\nof labeled data, Wav2Vec2 yields a word error rate (WER) of less than 5%\non the clean test set of\n[LibriSpeech](https://huggingface.co/datasets/librispeech_asr) - *cf.*\nwith Table 9 of the [paper](https://arxiv.org/pdf/2006.11477.pdf).", "**Wav2Vec2** is a popular pre-trained model for speech recognition.\nReleased in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)\nby Meta AI Research, the novel architecture catalyzed progress in\nself-supervised pretraining for speech recognition, *e.g.* [*G. Ng et\nal.*, 2021](https://arxiv.org/pdf/2104.03416.pdf), [*Chen et al*,\n2021](https://arxiv.org/abs/2110.13900), [*Hsu et al.*,\n2021](https://arxiv.org/abs/2106.07447) and [*Babu et al.*,"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns with the reference answer by stating that the pretraining objective of the Wav2Vec2 context network is a contrastive task and further explains that the model is trained to distinguish the true quantized speech representation from false ones by predicting the masked prediction. This additional detail supports the accuracy and factual nature of the response without deviating from the correct information."}, {"question": "What is the default checkpoint used by the sentiment analysis pipeline in the Transformers library?\n", "true_answer": "distilbert base uncased finetuned sst2 english", "source_doc": "huggingface/course/blob/main/subtitles/en/raw/chapter2/02_inside-pipeline-pt.md", "generated_answer": "The default checkpoint used by the sentiment analysis pipeline in the Transformers library is `distilbert-base-uncased-finetuned-sst-2-english`. [Document 0]", "retrieved_docs": ["Since the default checkpoint of the `sentiment-analysis` pipeline is `distilbert-base-uncased-finetuned-sst-2-english` (you can see its model card [here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), we run the following:\n\n```python\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n```\n\nOnce we have the tokenizer, we can directly pass our sentences to it and we'll get back a dictionary that's ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.", ". They can be words, parts of words or punctuation symbols. Then the tokenizer will had some special tokens (if the model expect them). Here the model uses expects a CLS token at the beginning and a SEP token at the end of the sentence to classify. Lastly, the tokenizer matches each token to its unique ID in the vocabulary of the pretrained model. To load such a tokenizer, the Transformers library provides the AutoTokenizer API. The most important method of this class is from_pretrained, which will download and cache the configuration and the vocabulary associated to a given checkpoint. Here, the checkpoint used by default for the sentiment analysis pipeline is distilbert base uncased finetuned sst2 english. We instantiate a tokenizer associated with that checkpoint, then feed it the two sentences. Since those two sentences are not of the same size, we will need to pad the shortest one to be able to build an array", "Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"sentiment-analysis\")\n```\n\nThe [`pipeline`] downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:\n\n```py\n>>> classifier(\"We are very happy to show you the \ud83e\udd17 Transformers library.\")\n[{'label': 'POSITIVE', 'score': 0.9998}]\n```", "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the `classifier` object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n\nThere are three main steps involved when you pass some text to a pipeline:\n\n1. The text is preprocessed into a format the model can understand.\n2. The preprocessed inputs are passed to the model.\n3. The predictions of the model are post-processed, so you can make sense of them.\n\n\nSome of the currently [available pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) are:", "he pipeline function. The pipeline function is the most high-level API of the Transformers library. It regroups together all the steps to go from raw texts to usable predictions. The model used is at the core of a pipeline, but the pipeline also include all the necessary pre-processing (since the model does not expect texts, but numbers) as well as some post-processing to make the output of the model human-readable. Let's look at a first example with the sentiment analysis pipeline. This pipeline performs text classification on a given input, and determines if it's positive or negative. Here, it attributed the positive label on the given text, with a confidence of 95%. You can pass multiple texts to the same pipeline, which will be processed and passed through the model together, as a batch. The output is a list of individual results, in the same order as the input texts", "```python\npip install -q transformers\nfrom transformers import pipeline\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\ndata = [\"I love you\", \"I hate you\"]\nsentiment_pipeline(data)\n```\n\nThis code snippet uses the [pipeline class](https://huggingface.co/docs/transformers/main_classes/pipelines) to make predictions from models available in the Hub. It uses the [default model for sentiment analysis](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you) to analyze the list of texts `data` and it outputs the following results:", "## Conclusion\n\nBy using Transformers' sentiment analysis pipeline, I saved a non-negligible amount of time. Instead of training/fine-tuning a model, I could find one ready to be used in production and start the deployment in my system. I might fine-tune it in the future, but as shown on my test, the accuracy is already amazing!\nI would have liked a \"pure TensorFlow\" model, or at least a way to load it in TensorFlow without Transformers dependencies to use the AI platform. It would also be great to have a lite version."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer, including the model name and its specifications. The use of backticks to indicate code or a specific term is appropriate and does not detract from the accuracy of the response."}, {"question": "What is the purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\"?\n", "true_answer": "To show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi.", "source_doc": "huggingface/optimum/blob/main/docs/source/notebooks.md", "generated_answer": "The purpose of the notebook \"How to use DeepSpeed to train models with billions of parameters on Habana Gaudi\" is to demonstrate how to use DeepSpeed to pre-train or fine-tune the 1.6 billion-parameter GPT2-XL model for causal language modeling on Habana Gaudi. [Document 0]", "retrieved_docs": ["| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https://github.com/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022.ipynb) | Show how to use DeepSpeed to pre-train/fine-tune the 1.6B-parameter GPT2-XL for causal language modeling on Habana Gaudi. |  [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/optimum-habana/blob/main/notebooks/AI_HW_Summit_2022", "## Conclusion\n\nWhen dealing with images, we presented two solutions to speed up your training workflows: allocating more resources to the dataloader, and decoding and augmenting images directly on accelerator devices rather than on CPU.\nWe showed that it leads to dramatic speedups when training a SOTA vision-language model like BridgeTower: **Habana Gaudi2 with Optimum Habana is about x1.4 faster than Nvidia H100 and x2.5 faster than Nvidia A100 80GB with Transformers!**\nAnd this is super easy to use as you just need to provide a few additional training arguments.\n\nTo go further, we are looking forward to using HPU graphs for training models even faster and to presenting how to use DeepSpeed ZeRO-3 on Gaudi2 to accelerate the training of your LLMs. Stay tuned!", "### Related Topics\n\n- [Faster Training and Inference: Habana Gaudi-2 vs Nvidia A100 80GB](https://huggingface.co/blog/habana-gaudi-2-benchmark)\n- [Leverage DeepSpeed to Train Faster and Cheaper Large Scale Transformer Models with Hugging Face and Habana Labs Gaudi](https://developer.habana.ai/events/leverage-deepspeed-to-train-faster-and-cheaper-large-scale-transformer-models-with-hugging-face-and-habana-labs-gaudi/)\n\n---", "!--\u26a0\ufe0f Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\nrendered properly in your Markdown viewer.\n-->\n\n# DeepSpeed\n\n[DeepSpeed](https://www.deepspeed.ai/) is a library designed for speed and scale for distributed training of large models with billions of parameters. At its core is the Zero Redundancy Optimizer (ZeRO) that shards optimizer states (ZeRO-1), gradients (ZeRO-2), and parameters (ZeRO-3) across data parallel processes. This drastically reduces memory usage, allowing you to scale your training to billion parameter models. To unlock even more memory efficiency, ZeRO-Offload reduces GPU compute and memory by leveraging CPU resources during optimization.", "In this article, you will learn how to use [Habana\u00ae Gaudi\u00ae2](https://habana.ai/training/gaudi2/) to accelerate model training and inference, and train bigger models with \ud83e\udd17 [Optimum Habana](https://huggingface.co/docs/optimum/habana/index). Then, we present several benchmarks including BERT pre-training, Stable Diffusion inference and T5-3B fine-tuning, to assess the performance differences between first generation Gaudi, Gaudi2 and Nvidia A100 80GB. Spoiler alert - Gaudi2 is about twice faster than Nvidia A100 80GB for both training and inference!", "In the fall of 2019 Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase and Yuxiong He published a paper:\n[ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054), which contains a plethora of ingenious new ideas on how one could make their hardware do much more than what it was thought possible before. A short time later [DeepSpeed](https://github.com/microsoft/deepspeed) has been released and it gave to the world the open source implementation of most of the ideas in that paper (a few ideas are still in works) and in parallel a team from Facebook released [FairScale](https://github.com/facebookresearch/fairscale/) which also implemented some of the core ideas from the ZeRO paper.", "## Habana Gaudi2\n\n[Gaudi2](https://habana.ai/training/gaudi2/) is the second-generation AI hardware accelerator designed by Habana Labs. A single server contains 8 accelerator devices (called Habana Processing Units, or HPUs) with 96GB of memory each, which provides room to make very large models fit in. However, hosting the model is not very interesting if the computation is slow. Fortunately, Gaudi2 shines on that aspect: it differs from GPUs in that its architecture enables the accelerator to perform General Matrix Multiplication (GeMM) and other operations in parallel, which speeds up deep learning workflows. These features make Gaudi2 a great candidate for LLM training and inference."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer in stating the purpose of the notebook, which is to demonstrate the use of DeepSpeed for training the GPT2-XL model with 1.6 billion parameters for causal language modeling on Habana Gaudi."}, {"question": "What command line module does PyTorch provide to run a script on multiple GPUs?\n", "true_answer": "torchrun", "source_doc": "huggingface/blog/blob/main/pytorch-ddp-accelerate-transformers.md", "generated_answer": "PyTorch provides the `torchrun` command line module to run a script on multiple GPUs. [Document 0]", "retrieved_docs": ["The optimizer needs to be declared based on the model *on the specific device* (so `ddp_model` and not `model`) for all of the gradients to properly be calculated.\n\nLastly, to run the script PyTorch has a convenient `torchrun` command line module that can help. Just pass in the number of nodes it should use as well as the script to run and you are set:\n\n```bash\ntorchrun --nproc_per_node=2 --nnodes=1 example_script.py\n```\n\nThe above will run the training script on two GPUs that live on a single machine and this is the barebones for performing only distributed training with PyTorch.\n\nNow let's talk about Accelerate, a library aimed to make this process more seameless and also help with a few best practices\n\n## \ud83e\udd17 Accelerate", "However, how would you then get this script to run on say two GPUs or on multiple machines if these resources are available, which could improve training speed through *distributed* training? Just doing `python myscript.py` will only ever run the script using a single GPU. This is where `torch.distributed` comes into play\n\n## PyTorch Distributed Data Parallelism\n\nAs the name implies, `torch.distributed` is meant to work on *distributed* setups. This can include multi-node, where you have a number of machines each with a single GPU, or multi-gpu where a single system has multiple GPUs, or some combination of both.\n\nTo convert our above code to work within a distributed setup, a few setup configurations must first be defined, detailed in the [Getting Started with DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)", "### Script to compare pre-training with PyTorch on 8 GPU V100's\n\nFor comparison you can run the same pre-training with PyTorch on GPU. Note that we have to make use of `gradient_accumulation` \nbecause the maximum batch size that fits on a single V100 GPU is 32 instead of 128.\nHaving created the tokenzier and configuration in `norwegian-roberta-base`, we create the following symbolic links:\n\n```bash\nln -s ~/transformers/examples/pytorch/language-modeling/run_mlm.py ./\n```\n\n, set some environment variables:\n\n```bash\nexport NUM_GPUS=8\nexport TOKENIZERS_PARALLELISM=0\nexport MODEL_DIR=\"./norwegian-roberta-base\"\nmkdir -p ${MODEL_DIR}\n```\n\n, and can start training as follows:", "-->\n\n# Debugging\n\nTraining on multiple GPUs can be a tricky endeavor whether you're running into installation issues or communication problems between your GPUs. This debugging guide covers some issues you may run into and how to resolve them.\n\n## DeepSpeed CUDA installation\n\nIf you're using DeepSpeed, you've probably already installed it with the following command.\n\n```bash\npip install deepspeed\n```\n\nDeepSpeed compiles CUDA C++ code and it can be a potential source of errors when building PyTorch extensions that require CUDA. These errors depend on how CUDA is installed on your system, and this section focuses on PyTorch built with *CUDA 10.2*.\n\n<Tip>\n\nFor any other installation issues, please [open an issue](https://github.com/microsoft/DeepSpeed/issues) with the DeepSpeed team.", "- Native PyTorch DDP through the `pytorch.distributed` module\n- Utilizing \ud83e\udd17 Accelerate's light wrapper around `pytorch.distributed` that also helps ensure the code can be run on a single GPU and TPUs with zero code changes and miminimal code changes to the original code\n- Utilizing \ud83e\udd17 Transformer's high-level Trainer API which abstracts all the boilerplate code and supports various devices and distributed scenarios\n\n## What is \"Distributed\" training and why does it matter?\n\nTake some very basic PyTorch training code below, which sets up and trains a model on MNIST based on the [official MNIST example](https://github.com/pytorch/examples/blob/main/mnist/main.py)", "# Distributed inference with multiple GPUs\n\nOn distributed setups, you can run inference across multiple GPUs with \ud83e\udd17 [Accelerate](https://huggingface.co/docs/accelerate/index) or [PyTorch Distributed](https://pytorch.org/tutorials/beginner/dist_overview.html), which is useful for generating with multiple prompts in parallel.\n\nThis guide will show you how to use \ud83e\udd17 Accelerate and PyTorch Distributed for distributed inference.\n\n## \ud83e\udd17 Accelerate\n\n\ud83e\udd17 [Accelerate](https://huggingface.co/docs/accelerate/index) is a library designed to make it easy to train or run inference across distributed setups. It simplifies the process of setting up the distributed environment, allowing you to focus on your PyTorch code.", "with distributed_state.split_between_processes([\"a dog\", \"a cat\"]) as prompt:\n    result = pipeline(prompt).images[0]\n    result.save(f\"result_{distributed_state.process_index}.png\")\n```\n\nUse the `--num_processes` argument to specify the number of GPUs to use, and call `accelerate launch` to run the script:\n\n```bash\naccelerate launch run_distributed.py --num_processes=2\n```\n\n<Tip>\n\nTo learn more, take a look at the [Distributed Inference with \ud83e\udd17 Accelerate](https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference-with-accelerate) guide.\n\n</Tip>\n\n## PyTorch Distributed"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that `torchrun` is the command line module used by PyTorch to run a script on multiple GPUs."}, {"question": "What is the most popular vision transformer model on the Hugging Face Model Hub for image classification?\n", "true_answer": "google/vit-base-patch16-224", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/image-classification-with-vision-transformers.md", "generated_answer": "The context does not provide specific information about the most popular vision transformer model on the Hugging Face Model Hub for image classification.", "retrieved_docs": ["## Benchmarking Hugging Face Models on AWS Inferentia 2\n\nWe evaluated some of the most popular NLP models from the [Hugging Face Hub](https://huggingface.co/models) including BERT, RoBERTa, DistilBERT, and vision models like Vision Transformers.", "Today, we are thrilled to announce that Hugging Face expands its collaboration with Microsoft to bring open-source models from the Hugging Face Hub to Azure Machine Learning. Together we built a new Hugging Face Hub Model Catalog available directly within Azure Machine Learning Studio, filled with thousands of the most popular Transformers models from the [Hugging Face Hub](https://huggingface.co/models). With this new integration, you can now deploy Hugging Face models in just a few clicks on managed endpoints, running onto secure and scalable Azure infrastructure.\n\n![Hugging Face Model Catalog](assets/75_hugging_face_endpoints_on_azure/02.jpg \"Hugging Face Model Catalog\")", "In the last five years, Transformer models [[1](https://arxiv.org/abs/1706.03762)] have become the _de facto_ standard for many machine learning (ML) tasks, such as natural language processing (NLP), computer vision (CV),  speech, and more. Today, many data scientists and ML engineers rely on popular transformer architectures like BERT [[2](https://arxiv.org/abs/1810.04805)], RoBERTa [[3](https://arxiv.org/abs/1907.11692)], the Vision Transformer [[4](https://arxiv.org/abs/2010.11929)], or any of the 130,000+ pre-trained models available on the [Hugging Face](https://huggingface", "## Enabling the community: One task at a time \ud83d\udc41\n\nThe Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:\n\n- Image classification\n- Image segmentation\n- (Zero-shot) object detection\n- Video classification\n- Depth estimation\n- Image-to-image synthesis\n- Unconditional image generation\n- Zero-shot image classification\n\nEach of these tasks comes with at least 10 model checkpoints on the Hub for you to explore. Furthermore, we support [tasks](https://huggingface.co/tasks) that lie at the intersection of vision and language such as:\n\n- Image-to-text (image captioning, OCR)\n- Text-to-image\n- Document question-answering\n- Visual question-answering", "In the past few months, the Hugging Face team and external contributors\nadded a variety of vision models in TensorFlow to Transformers. This\nlist is growing comprehensively and already includes state-of-the-art\npre-trained models like [Vision Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/vit),\n[Masked Autoencoders](https://huggingface.co/docs/transformers/model_doc/vit_mae),\n[RegNet](https://huggingface.co/docs/transformers/main/en/model_doc/regnet),\n[ConvNeXt](https://huggingface.co/docs/transformers/model_doc/convnext),\nand many others!", "This model was contributed by [nielsr](https://huggingface.co/nielsr). The original code (written in JAX) can be\nfound [here](https://github.com/google-research/vision_transformer).\n\n## Resources\n\nA list of official Hugging Face and community (indicated by \ud83c\udf0e) resources to help you get started with ViT Hybrid.\n\n<PipelineTag pipeline=\"image-classification\"/>", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "1", "eval_feedback_GPT4": "Feedback: The response provided does not answer the question at all. It does not mention any vision transformer model, let alone the most popular one on the Hugging Face Model Hub for image classification. According to the score rubric, the response should be factual and accurate, providing the name of the most popular vision transformer model. Since the response fails to provide any relevant information, it is completely incorrect and not factual in the context of the question asked."}, {"question": "What is the command to upload an ESPnet model to a Hugging Face repository?\n", "true_answer": "./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo", "source_doc": "huggingface/hub-docs/blob/main/docs/hub/espnet.md", "generated_answer": "The command to upload an ESPnet model to a Hugging Face repository is:\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n(Source: Document 0)", "retrieved_docs": ["## Sharing your models\n\n`ESPnet` outputs a `zip` file that can be uploaded to Hugging Face easily. For a full guide on sharing  models, we recommend checking out the [official guide](https://github.com/espnet/espnet_model_zoo#register-your-model)).\n\nThe `run.sh` script allows to upload a given model to a Hugging Face repository.\n\n```bash\n./run.sh --stage 15 --skip_upload_hf false --hf_repo username/model_repo\n```\n\n## Additional resources", "Using ESPnet at Hugging Face\n\n`espnet` is an end-to-end toolkit for speech processing, including automatic speech recognition, text to speech, speech enhancement, dirarization and other tasks.\n\n## Exploring ESPnet in the Hub\n\nYou can find hundreds of `espnet` models by filtering at the left of the [models page](https://huggingface.co/models?library=espnet&sort=downloads). \n\nAll models on the Hub come up with useful features:\n1. An automatically generated model card with a description, a training configuration, licenses and more.\n2. Metadata tags that help for discoverability and contain information such as license, language and datasets.\n3. An interactive widget you can use to play out with the model directly in the browser.\n4. An Inference API that allows to make inference requests.", "If you want to see how to load a specific model, you can click `Use in ESPnet` and you will be given a working snippet that you can load it! \n\n<div class=\"flex justify-center\">\n<img class=\"block dark:hidden\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet.png\"/>\n<img class=\"hidden dark:block\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/libraries-espnet_snippet-dark.png\"/>\n</div>\n\n## Sharing your models", "```\nmlagents-load-from-hf --repo-id=\"ThomasSimonini/MLAgents-Pyramids\" --local-dir=\"./downloads\"\n```\n\nYou need to define two parameters:\n- `--repo-id`: the name of the Hugging Face repo you want to download.\n- `--local-dir`: the path to download the model.\n\n## Visualize an agent playing\n\nYou can easily watch any model playing directly in your browser:\n\n1. Go to your model repo.\n2. In the `Watch Your Agent Play` section, click on the link.\n3. In the demo, on step 1, choose your model repository, which is the model id.\n4. In step 2, choose what model you want to replay.\n\n## Sharing your models\n\nYou can easily upload your models using `mlagents-push-to-hf`:", "## Share your model\n\nAll scripts can upload your final model to the [Model Hub](https://huggingface.co/models). Make sure you are logged into Hugging Face before you begin:\n\n```bash\nhuggingface-cli login\n```\n\nThen add the `push_to_hub` argument to the script. This argument will create a repository with your Hugging Face username and the folder name specified in `output_dir`.\n\nTo give your repository a specific name, use the `push_to_hub_model_id` argument to add it. The repository will be automatically listed under your namespace.\n\nThe following example shows how to upload a model with a specific repository name:", "```py\n>>> tokenizer.push_to_hub(\"my-awesome-model\")\n```\n\nOr perhaps you'd like to add the TensorFlow version of your fine-tuned PyTorch model:\n\n```py\n>>> tf_model.push_to_hub(\"my-awesome-model\")\n```\n\nNow when you navigate to your Hugging Face profile, you should see your newly created model repository. Clicking on the **Files** tab will display all the files you've uploaded to the repository.\n\nFor more details on how to create and upload files to a repository, refer to the Hub documentation [here](https://huggingface.co/docs/hub/how-to-upstream).\n\n## Upload with the web interface", "1. **Log in**\n\nTo begin with, you should check that you are correctly logged in and that you have `git-lfs` installed so that your fine-tuned model can automatically be uploaded.\n\nRun:\n\n```bash\nhuggingface-cli login\n```\n\nto login. It is recommended to login with your access token that can be found under your hugging face profile (icon in the top right corner on [hf.co](http://hf.co/), then Settings -> Access Tokens -> User Access Tokens -> New Token (if haven't generated one already)\n\nYou can then copy-paste this token to log in locally.\n\n2. **Create your model repository**\n\nFirst, let's make sure that `git-lfs` is correctly installed. To so, simply run:\n\n```bash\ngit-lfs -v\n```"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, including the command structure and the parameters used. The inclusion of the source does not affect the accuracy of the command provided."}, {"question": "What file should be added to a model repository to install custom Python dependencies for Inference Endpoints?\n", "true_answer": "requirements.txt", "source_doc": "huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/custom_dependencies.mdx", "generated_answer": "A `requirements.txt` file should be added to a model repository to install custom Python dependencies for Inference Endpoints. (Document 0)", "retrieved_docs": ["To add custom dependencies, add a `requirements.txt` [file](https://huggingface.co/philschmid/distilbert-onnx-banking77/blob/main/requirements.txt) with the Python dependencies you want to install in your model repository on the Hugging Face Hub. When your Endpoint and Image artifacts are created, Inference Endpoints checks if the model repository contains a `requirements.txt ` file and installs the dependencies listed within.\n\n```bash\noptimum[onnxruntime]==1.2.3\nmkl-include\nmkl\n```\n\nCheck out the `requirements.txt` files in the following model repositories for examples:", "Add custom Dependencies\n\nInference Endpoints\u2019 base image includes all required libraries to run inference on \ud83e\udd17 Transformers models, but it also supports custom dependencies. This is useful if you want to:\n\n* [customize your inference pipeline](/docs/inference-endpoints/guides/custom_handler) and need additional Python dependencies\n* run a model which requires special dependencies like the newest or a fixed version of a library (for example, `tapas` (`torch-scatter`)).", "Inference Endpoints Version\n\nHugging Face Inference Endpoints comes with a default serving container which is used for all [supported Transformers and Sentence-Transformers tasks](/docs/inference-endpoints/supported_tasks) and for [custom inference handler](/docs/inference-endpoints/guides/custom_handler) and implement batching.\nBelow you will find information about the installed packages and versions used.\n\nYou can always upgrade installed packages and a custom packages by adding a `requirements.txt` file to your model repository. Read more in [Add custom Dependencies](/docs/inference-endpoints/guides/custom_dependencies).\n\n## Installed packages & version", "```bash\nmodel.tar.gz/\n|- pytorch_model.bin\n|- ....\n|- code/\n  |- inference.py\n  |- requirements.txt \n```\n\nThe `inference.py` file contains your custom inference module, and the `requirements.txt` file contains additional dependencies that should be added. The custom module can override the following methods:", "After that we can install the required dependencies.\n\n\n```bash\npip install \"sagemaker>=2.48.0\" --upgrade\n```\n\n\nTo deploy a model on SageMaker, we need to create a `sagemaker` Session and provide an IAM role with the right permission. The `get_execution_role` method is provided by the SageMaker SDK as an optional convenience. You can also specify the role by writing the specific role ARN you want your endpoint to use. This IAM role will be later attached to the Endpoint, e.g. download the model from Amazon S3. \n\n\n```python\nimport sagemaker\n\nsess = sagemaker.Session()\nrole = sagemaker.get_execution_role()\n```\n\n---\n\n## **Deploy a trained Hugging Face Transformer model to SageMaker for inference**", "## Conclusion\n\nIn this blog post, we have shown how to deploy MusicGen using Inference Endpoints with a custom inference handler. The same technique can be used for any other model in the Hub that does not have an associated pipeline. All you have to do is override the `Endpoint Handler` class in `handler.py`, and add `requirements.txt` to reflect your project's dependencies. \n\n### Read More\n- [Inference Endpoints documentation covering Custom Handler](https://huggingface.co/docs/inference-endpoints/guides/custom_handler)", "### Download the Model Checkpoints\n\nTo run inference in Python, use one of the versions stored in the `packages` folders because the `compiled` ones are only compatible with Swift. You may choose whether you want to use `original` or `split_einsum` attention.\n\nThis is how you'd download the `original` attention variant from the Hub to a directory called `models`:\n\n```Python\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\n\nrepo_id = \"apple/coreml-stable-diffusion-v1-4\"\nvariant = \"original/packages\""], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer by stating that a `requirements.txt` file is needed in a model repository for installing custom Python dependencies for Inference Endpoints."}, {"question": "How many images are needed to teach new concepts to Stable Diffusion using Textual Inversion?\n", "true_answer": "3-5 images", "source_doc": "huggingface/diffusers/blob/main/examples/research_projects/intel_opts/textual_inversion_dfq/README.md", "generated_answer": "To teach new concepts to Stable Diffusion using Textual Inversion, 3-5 images are needed. [Document 0, Document 1, Document 3, Document 4]", "retrieved_docs": ["## Textual Inversion\n\nTextual Inversion lets you personalize a Stable Diffusion model on your own images with just 3-5 samples. With this tool, you can train a model on a concept, and then share the concept with the rest of the community! \n\n![image info](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/diffusers-2nd-month/textual_inversion.jpeg)\n\nIn just a couple of days, the community shared over 200 concepts! Check them out!", "Distillation for quantization on Textual Inversion models to personalize text2image\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images._By using just 3-5 images new concepts can be taught to Stable Diffusion and the model personalized on your own images_\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\nWe have enabled distillation for quantization in `textual_inversion.py` to do quantization aware training as well as distillation on the model generated by Textual Inversion method.\n\n## Installing the dependencies\n\nBefore running the scripts, make sure to install the library's training dependencies:\n\n```bash\npip install -r requirements.txt\n```\n\n## Prepare Datasets", "# Textual inversion\n\n[[open-in-colab]]\n\nThe [`StableDiffusionPipeline`] supports textual inversion, a technique that enables a model like Stable Diffusion to learn a new concept from just a few sample images. This gives you more control over the generated images and allows you to tailor the model towards specific concepts. You can get started quickly with a collection of community created concepts in the [Stable Diffusion Conceptualizer](https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer).\n\nThis guide will show you how to run inference with textual inversion using a pre-learned concept from the Stable Diffusion Conceptualizer. If you're interested in teaching a model new concepts with textual inversion, take a look at the [Textual Inversion](../training/text_inversion) training guide.\n\nImport the necessary libraries:", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Training with Intel Extension for PyTorch\n\nIntel Extension for PyTorch provides the optimizations for faster training and inference on CPUs. You can leverage the training example \"textual_inversion.py\". Follow the [instructions](https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion) to get the model and [dataset](https://huggingface.co/sd-concepts-library/dicoo2) before running the script.", "# Textual Inversion fine-tuning example\n\n[Textual inversion](https://arxiv.org/abs/2208.01618) is a method to personalize text2image models like stable diffusion on your own images using just 3-5 examples.\nThe `textual_inversion.py` script shows how to implement the training procedure and adapt it for stable diffusion.\n\n## Running on Colab\n\nColab for training\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb)", "* [Organization](https://huggingface.co/sd-concepts-library) with the concepts. \n* [Navigator Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion_textual_inversion_library_navigator.ipynb): Browse visually and use over 150 concepts created by the community.\n* [Training Colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/sd_textual_inversion_training.ipynb): Teach Stable Diffusion a new concept and share it with the rest of the community.", "## Other Methods\n\nThe quest for easy fine-tuning is not new. In addition to Dreambooth, [_textual inversion_](https://huggingface.co/docs/diffusers/main/en/training/text_inversion) is another popular method that attempts to teach new concepts to a trained Stable Diffusion Model. One of the main reasons for using Textual Inversion is that trained weights are also small and easy to share. However, they only work for a single subject (or a small handful of them), whereas LoRA can be used for general-purpose fine-tuning, meaning that it can be adapted to new domains or datasets."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly by stating that 3-5 images are needed to teach new concepts to Stable Diffusion using Textual Inversion. The inclusion of document references does not detract from the accuracy of the answer."}, {"question": "What is the maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0?\n", "true_answer": "10GB", "source_doc": "huggingface/transformers/blob/main/docs/source/en/big_models.md", "generated_answer": "The maximum size of a model checkpoint before it is automatically sharded in Transformers version 4.18.0 is 10GB. [Document 0]", "retrieved_docs": ["## Sharded checkpoints\n\nSince version 4.18.0, model checkpoints that end up taking more than 10GB of space are automatically sharded in smaller pieces. In terms of having one single checkpoint when you do `model.save_pretrained(save_dir)`, you will end up with several partial checkpoints (each of which being of size < 10GB) and an index that maps parameter names to the files they are stored in.\n\nYou can control the maximum size before sharding with the `max_shard_size` parameter, so for the sake of an example, we'll use a normal-size models with a small shard size: let's take a traditional BERT model.\n\n```py\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(\"bert-base-cased\")\n```", "This option can be activated with `low_cpu_mem_usage=True`. The model is first created on the Meta device (with empty weights) and the state dict is then loaded inside it (shard by shard in the case of a sharded checkpoint). This way the maximum RAM used is the full size of the model only.\n\n```py\nfrom transformers import AutoModelForSeq2SeqLM\n\nt0pp = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", low_cpu_mem_usage=True)\n```", "TODO: Add more explantion.\n\nBefore training, let's prepare our model first. To be able to shard the model, the sharded dimention needs to be a multiple of devices it'll be sharded on. But GPTNeo's vocab size is 50257, so we need to resize the embeddings accordingly. \n\n```python\nfrom transformers import FlaxGPTNeoForCausalLM, GPTNeoConfig \nmodel = FlaxGPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")", "### Large model loading\n\nIn Transformers 4.20.0, the [`~PreTrainedModel.from_pretrained`] method has been reworked to accommodate large models using [Accelerate](https://huggingface.co/docs/accelerate/big_modeling). This requires Accelerate >= 0.9.0 and PyTorch >= 1.9.0. Instead of creating the full model, then loading the pretrained weights inside it (which takes twice the size of the model in RAM, one for the randomly initialized model, one for the weights), there is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded.", "## Closing thoughts\n\n- While I couldn't port the model ensemble as `transformers` doesn't support it, on the plus side the download size of the final `facebook/wmt19-*` models  is 1.1GB and not 13GB as in the original. For some reason the original includes the optimizer state saved in the model - so it adds almost 9GB (4x2.2GB) of dead weight for those who just want to download the model to use it as is to translate text.", "Before you begin, make sure you have all the necessary libraries installed. \n\n```bash\npip install -q bitsandbytes sentencepiece accelerate transformers\n```\n\n<Tip>\nTo run the following examples with a non-quantized version of the model checkpoint you will need at least 20GB of GPU memory.\n</Tip>\n\n## Loading the model\n\nLet's start by loading the model's 9 billion parameters checkpoint: \n\n```py\n>>> checkpoint = \"HuggingFaceM4/idefics-9b\"\n```\n\nJust like for other Transformers models, you need to load a processor and the model itself from the checkpoint. \nThe IDEFICS processor wraps a [`LlamaTokenizer`] and IDEFICS image processor into a single processor to take care of \npreparing text and image inputs for the model.\n\n```py\n>>> import torch", "The original checkpoints can be found [here](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-ul2-checkpoints).\n\n\n## Running on low resource devices\n\nThe model is pretty heavy (~40GB in half precision) so if you just want to run the model, make sure you load your model in 8bit, and use `device_map=\"auto\"` to make sure  you don't have any OOM issue!\n\n```python\n>>> from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual as it aligns perfectly with the reference answer."}, {"question": "What is the purpose of Weights and Biases (W&B) for data scientists and machine learning scientists?\n", "true_answer": "To track their machine learning experiments at every stage, from training to production.", "source_doc": "gradio-app/gradio/blob/main/guides/06_integrating-other-frameworks/Gradio-and-Wandb-Integration.md", "generated_answer": "Weights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. It enables them to aggregate any metric over samples and display them in panels on a customizable and searchable dashboard. [Document 0]", "retrieved_docs": ["Gradio and W&B Integration\n\nRelated spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN\nTags: WANDB, SPACES\nContributed by Gradio team\n\n## Introduction\n\nIn this Guide, we'll walk you through:\n\n- Introduction of Gradio, and Hugging Face Spaces, and Wandb\n- How to setup a Gradio demo using the Wandb integration for JoJoGAN\n- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face\n\n\n## What is Wandb?\n\nWeights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:", "Similar to the dataset curation/selection step, documenting and measuring bias-related phenomena in models can help both ML developers who are selecting a model to use as-is or to finetune and ML developers who want to train their own models. For the latter, measures of bias-related phenomena in the model can help them learn from what has worked or what hasn\u2019t for other models and serve as a signal to guide their own development choices.", "_This blog post from the [Ethics and Society regulars @\ud83e\udd17](https://huggingface.co/blog/ethics-soc-1) shares some of the lessons we have learned along with tools we have developed to support ourselves and others in our community\u2019s efforts to better address bias in Machine Learning. The first part is a broader reflection on bias and its context. If you\u2019ve already read it and are coming back specifically for the tools, feel free to jump to the [datasets](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias) or [models](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)\nsection!_", "**Meg:** Most recently I\u2019ve really loved what [Abeba Birhane](https://abebabirhane.github.io) has been doing on [values that are encoded in machine learning](https://arxiv.org/abs/2106.15590). My own team at Google had been working on [data genealogies](https://journals.sagepub.com/doi/full/10.1177/20539517211035955), bringing critical analysis on how ML data is handled which they have a few papers on - for example, [Data and its (dis)contents: A survey of dataset development and use in machine learning research](https://arxiv.org/abs/2012.05345)", "Also, as we have mentioned in a [previous newsletter](https://huggingface.co/blog/ethics-soc-2#addressing-bias-throughout-the-ml-development-cycle), there is no one single way to develop machine learning systems, and any of the steps in the development and deployment process can present opportunities to tackle bias, from who is included at the start, to defining the task, to curating the dataset, training the model, and more. This also applies to multimodal models and the ways in which they are ultimately deployed or productionized in society, since the consequences of bias in multimodal models will depend on their downstream use. For instance, if a model is used in a human-in-the-loop setting for graphic design (such as those created by [RunwayML](https://runwayml", "--\ntitle: \"Let's talk about biases in machine learning! Ethics and Society Newsletter #2\" \nthumbnail: /blog/assets/122_ethics_soc_2/thumbnail-solstice.png\nauthors:\n- user: yjernite\n---\n\n# Machine Learning in development: Let's talk about bias!\n\n\n_Bias in ML is ubiquitous, and Bias in ML is complex; so complex in fact that no single technical intervention is likely to meaningfully address the problems it engenders. ML models, as sociotechnical systems, amplify social trends that may exacerbate inequities and harmful biases in ways that depend on their deployment context and are constantly evolving._\n\n_This means that developing ML systems with care requires vigilance and responding to feedback from those deployment contexts, which in turn we can facilitate by sharing lessons across contexts and developing tools to analyze signs of bias at every level of ML development._", "Measuring biases can be done by evaluating models on datasets such as [Wino Bias](https://huggingface.co/datasets/wino_bias) or [MD Gender Bias](https://huggingface.co/datasets/md_gender_bias), and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis) to try to identify which subsets of the evaluation dataset a model performs poorly on.\n\nWe are currently working on additional measurements that can be used to quantify different dimensions of bias in both models and datasets -- stay tuned for more documentation on this topic!"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It aligns well with the reference answer by mentioning the tracking of machine learning experiments at every stage, which is the core purpose of Weights and Biases. Additionally, the response expands on the reference answer by detailing the functionality of aggregating metrics and the use of a customizable dashboard, which are factual statements about W&B's features."}, {"question": "What is the name of the open-source library created by Hugging Face to simplify Transformer acceleration?\n", "true_answer": "Optimum", "source_doc": "huggingface/blog/blob/main/intel.md", "generated_answer": "The name of the open-source library created by Hugging Face to simplify Transformer acceleration is Optimum. [Document 0]", "retrieved_docs": ["The next logical step was to expand on this work and share it with the ML community. Enter the [Optimum Intel](https://github.com/huggingface/optimum-intel) open source library! Let\u2019s take a deeper look at it.\n\n## Get Peak Transformers Performance with Optimum Intel\n[Optimum](https://github.com/huggingface/optimum) is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance.", "To target these needs, Hugging Face built two open-sourced libraries: __Accelerate__ and __Optimum__. While [\ud83e\udd17 Accelerate](https://huggingface.co/docs/accelerate/index) focuses on out-of-the-box distributed training, [\ud83e\udd17 Optimum](https://huggingface.co/docs/optimum/index), as an extension of transformers, accelerates model training and inference by leveraging the maximum efficiency of users\u2019 targeted hardware. Optimum integrated machine learning accelerators like ONNX Runtime and specialized hardware like [Intel's Habana Gaudi](https://huggingface.co/blog/habana-gaudi-2-benchmark), so users can benefit from considerable speedup in both training and inference. Besides, Optimum seamlessly integrates other Hugging Face\u2019s tools while inheriting the same ease of use as Transformers", "## Optimum Library\n\nHugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.", "## Optimising Transformers for Production\n\nTransformers have completely transformed (pun intended) the field of AI. Models such as BERT are widely used by Graphcore customers in a huge array of applications, across NLP and beyond. These multi-talented models can perform feature extraction, text generation, sentiment analysis, translation and many more functions.\n\nAlready, Hugging Face plays host to hundreds of Transformers, from the French-language CamemBERT to ViT which applies lessons learned in NLP to computer vision. The Transformers library is downloaded an average of 2 million times every month and demand is growing.\n\nWith a user base of more than 50,000 developers \u2013 Hugging Face has seen the fastest ever adoption of an open-source project.\n\nNow, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's most advanced AI hardware.", "Today, [Habana\u00ae Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that they\u2019re joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habana\u2019s [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost.", "--\ntitle: \"Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\"\nthumbnail: /blog/assets/77_graphcore-update/graphcore_update.png\nauthors:\n- user: sallydoherty\n  guest: true\n---\n\n# Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\n\n\n[Graphcore](https://huggingface.co/hardware/graphcore/) and Hugging Face have significantly expanded the range of Machine Learning modalities and tasks available in [Hugging Face Optimum](https://github.com/huggingface/optimum), an open-source library for Transformers performance optimization. Developers now have convenient access to a wide range of off-the-shelf Hugging Face Transformer models, optimised to deliver the best possible performance on Graphcore\u2019s IPU.", "elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem: how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on your own dataset and share the result with the community. The second will dive deeper into our libraries and teach you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the spring of 2022. The first chapter requires no technical knowledge and is a good introduction to learn what Transformers models can do and how they could be of use to you or your company"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual based on the reference answer. The respondent correctly identified \"Optimum\" as the open-source library created by Hugging Face to simplify Transformer acceleration."}, {"question": "What parameter is used to ensure that elements in a row have the same height in Gradio?\n", "true_answer": "equal_height", "source_doc": "gradio-app/gradio/blob/main/guides/cn/03_building-with-blocks/02_controlling-layout.md", "generated_answer": "The parameter used to ensure that elements in a row have the same height in Gradio is `equal_height`, which should be passed to the `.style()` method of `gr.Row()`. [Document 1]", "retrieved_docs": ["$code_rows_and_columns\n$demo_rows_and_columns\n\nSee how the first column has two Textboxes arranged vertically. The second column has an Image and Button arranged vertically. Notice how the relative widths of the two columns is set by the `scale` parameter. The column with twice the `scale` value takes up twice the width.\n\nLearn more about Columns in the [docs](https://gradio.app/docs/#column).\n\n# Dimensions\n\nYou can control the height and width of various components, where the parameters are available. These parameters accept either a number (interpreted as pixels) or a string. Using a string allows the direct application of any CSS unit to the encapsulating Block element, catering to more specifc design requirements. When omitted, Gradio uses default dimensions suited for most use cases.\n\nBelow is an example illustrating the use of viewport width (vw):", "- Added a demo to show how to set up variable numbers of outputs in Gradio by [@abidlabs](https://github.com/abidlabs) in [PR 3127](https://github.com/gradio-app/gradio/pull/3127)\n- Updated docs to reflect that the `equal_height` parameter should be passed to the `.style()` method of `gr.Row()` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 3125](https://github.com/gradio-app/gradio/pull/3125)", "```python\nwith gr.Blocks() as demo:\n    with gr.Row(equal_height=True):\n        textbox = gr.Textbox()\n        btn2 = gr.Button(\"Button 2\")\n```\n\nThe widths of elements in a Row can be controlled via a combination of `scale` and `min_width` arguments that are present in every Component.\n\n- `scale` is an integer that defines how an element will take up space in a Row. If scale is set to `0`, and element will not expand to take up space. If scale is set to `1` or greater, the element well expand. Multiple elements in a row will expand proportional to their scale. Below, `btn1` will expand twice as much as `btn2`, while `btn0` will not expand at all:", "- css tweaks by [@pngwn](https://github.com/pngwn) in [PR 1235](https://github.com/gradio-app/gradio/pull/1235)\n- ensure defaults height match for media inputs by [@pngwn](https://github.com/pngwn) in [PR 1236](https://github.com/gradio-app/gradio/pull/1236)\n- Default Label label value by [@radames](https://github.com/radames) in [PR 1239](https://github.com/gradio-app/gradio/pull/1239)", "Usually, the height of Spaces is automatically adjusted when using the Gradio library interface. However, if you provide your own frontend in the Gradio SDK and the content height is larger than the viewport, you'll need to add an [iFrame Resizer script](https://cdnjs.com/libraries/iframe-resizer), so the content is scrollable in the iframe:", "- [#5221](https://github.com/gradio-app/gradio/pull/5221) [`f344592a`](https://github.com/gradio-app/gradio/commit/f344592aeb1658013235ded154107f72d86f24e7) - Allows setting a height to `gr.File` and improves the UI of the component. Thanks [@abidlabs](https://github.com/abidlabs)!", "- [#5221](https://github.com/gradio-app/gradio/pull/5221) [`f344592a`](https://github.com/gradio-app/gradio/commit/f344592aeb1658013235ded154107f72d86f24e7) - Allows setting a height to `gr.File` and improves the UI of the component.  Thanks [@abidlabs](https://github.com/abidlabs)!"], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response correctly identifies the parameter `equal_height` as the one used to ensure that elements in a row have the same height in Gradio, which aligns with the reference answer. Additionally, the response provides extra information on how to apply this parameter by using the `.style()` method of `gr.Row()`, which is accurate and enhances the factual content of the answer."}, {"question": "What is the command to install the latest version of Optimum with OpenVINO support?\n", "true_answer": "pip install --upgrade-strategy eager optimum[\"openvino\"]", "source_doc": "huggingface/diffusers/blob/main/docs/source/en/optimization/open_vino.md", "generated_answer": "The command to install the latest version of Optimum with OpenVINO support is:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis ensures that `optimum-intel` is using the latest version. [Document 0]", "retrieved_docs": ["# OpenVINO\n\n\ud83e\udd17 [Optimum](https://github.com/huggingface/optimum-intel) provides Stable Diffusion pipelines compatible with OpenVINO to perform inference on a variety of Intel processors (see the [full list](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) of supported devices).\n\nYou'll need to install \ud83e\udd17 Optimum Intel with the `--upgrade-strategy eager` option to ensure [`optimum-intel`](https://github.com/huggingface/optimum-intel) is using the latest version:\n\n```bash\npip install --upgrade-strategy eager optimum[\"openvino\"]\n```\n\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelines with OpenVINO.", "```\npip install optimum[openvino]\n```\n\nStarting from the code above, we only need to replace `StableDiffusionPipeline` with `OVStableDiffusionPipeline`. To load a PyTorch model and convert it to the OpenVINO format on-the-fly, you can set `export=True` when loading your model.\n\n```python\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n...\nov_pipe = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nlatency = elapsed_time(ov_pipe, prompt)\nprint(latency)\n\n# Don't forget to save the exported model\nov_pipe.save_pretrained(\"./openvino\")\n```", "Today, we are very happy to announce that we added Intel [OpenVINO](https://docs.openvino.ai/latest/index.html) to [Optimum Intel](https://github.com/huggingface/optimum-intel). You can now easily perform inference with OpenVINO Runtime on a variety of Intel processors  ([see](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html) the full list of supported devices) using Transformers models which can be hosted either on the Hugging Face hub or locally. You can also quantize your model with the OpenVINO Neural Network Compression Framework ([NNCF](https://github.com/openvinotoolkit/nncf)), and reduce its size and prediction latency in near minutes. \u200b", "```bash\npip install optimum\n```\n\nOr install from source:\n\n```bash\npip install git+https://github.com/huggingface/optimum.git\n```\n\nThis command installs the current main dev version of Optimum, which could include latest developments(new features, bug fixes). However, the\nmain version might not be very stable. If you run into any problem, please open an [issue](https://github.com/huggingface/optimum/issues) so\nthat we can fix it as soon as possible.\n\n## ORTTrainer", "Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change!\n\nNow, let's accelerate!\n\n## Optimum Intel and OpenVINO\n\n[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code.\n\nOptimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. \n\nOptimum Intel and OpenVINO can be installed as follows:\n\n```\npip install optimum[openvino]\n```", "```\npip install optimum[openvino]\n```\n\nHere, we load the model, optimize it for a static shape, and save it:\n\n```\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n\nmodel_id = \"./textual_inversion_output\"\n\nov_pipe = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nov_pipe.reshape(batch_size=5, height=512, width=512, num_images_per_prompt=1)\nov_pipe.save_pretrained(\"./textual_inversion_output_ov\")\n```\n\nThen, we load the optimized model, generate five different images and save them:\n\n```\nfrom optimum.intel.openvino import OVStableDiffusionPipeline", "## Now it's your turn\n\u200b\nAs you can see, it's pretty easy to accelerate your models with \ud83e\udd17 Optimum Intel and OpenVINO. If you'd like to get started, please visit the [Optimum Intel](https://github.com/huggingface/optimum-intel) repository, and don't forget to give it a star \u2b50. You'll also find additional examples [there](https://huggingface.co/docs/optimum/intel/optimization_ov). If you'd like to dive deeper into OpenVINO, the Intel [documentation](https://docs.openvino.ai/latest/index.html) has you covered."], "test_settings": "chunk:200_embeddings:thenlper~gte-small_rerank:True_reader-model:mistralai", "eval_score_GPT4": "5", "eval_feedback_GPT4": "Feedback: The response provided is completely correct, accurate, and factual. It matches the reference answer exactly, including the command and the upgrade strategy. The additional explanation about ensuring that `optimum-intel` is using the latest version is not part of the command itself but does not detract from the accuracy of the command provided."}]